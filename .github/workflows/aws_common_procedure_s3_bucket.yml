---
name: Tests - Integration - AWS S3 bucket for Terraform backend
on:
    workflow_dispatch:
    pull_request: {}
    schedule:
        - cron: 0 0 * * 4
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: true
env:
    IS_SCHEDULE: ${{ contains(github.ref, 'refs/heads/schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}
    AWS_PROFILE: infraex
    AWS_REGION: eu-west-2
jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip
    s3-bucket-verification:
        name: Verify S3 related doc scripts
        runs-on: ubuntu-latest
        needs: triage
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5
            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}
            - name: Generate random S3 bucket name
              working-directory: aws/common/procedure/s3-bucket
              run: |
                  set -euo pipefail
                  export BUCKET_NAME="camunda-s3-tf-docs-$(date +%s)-$(uuidgen | head -c 8)"
                  echo "BUCKET_NAME=$BUCKET_NAME" | tee -a "$GITHUB_ENV"

                  sed -i "s/^export S3_TF_BUCKET_NAME=\".*\"/export S3_TF_BUCKET_NAME=\"$BUCKET_NAME\"/" ./s3-bucket-creation.sh
            - name: Execute S3 scripts
              working-directory: aws/common/procedure/s3-bucket
              run: |
                  set -euo pipefail

                  # Creation and versioning of S3 Bucket
                  echo "Creating S3 bucket ${BUCKET_NAME} for Terraform state management"
                  ./s3-bucket-creation.sh

                  # previous step export is encapsulated to its execution
                  export S3_TF_BUCKET_NAME="${BUCKET_NAME}"

                  echo "Enabling versioning for S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-versioning.sh

                  echo "Enabling private bucket policy for S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-private.sh

                  echo "Verifying S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-verify.sh

                  # Terraform init part
                  cat <<EOF > config.tf
                  terraform {
                    backend "s3" {}
                  }
                  EOF

                  echo "Initializing Terraform with S3 backend"
                  ./s3-bucket-tf-init.sh

                  rm -rf config.tf
            - name: Delete S3 bucket
              if: always()
              run: |
                  set -euo pipefail

                  # Delete all versions
                  aws s3api list-object-versions --bucket "${{ env.BUCKET_NAME }}" \
                    --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
                    --output=json \
                  | jq -c '.Objects // [] | .[]' \
                  | while read -r obj; do
                      key=$(echo "$obj" | jq -r '.Key')
                      versionId=$(echo "$obj" | jq -r '.VersionId')
                      echo "Deleting version: $key ($versionId)"
                      aws s3api delete-object --bucket "${{ env.BUCKET_NAME }}" --key "$key" --version-id "$versionId"
                  done

                  # Delete delete markers
                  aws s3api list-object-versions --bucket "${{ env.BUCKET_NAME }}" \
                    --query='{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' \
                    --output=json \
                  | jq -c '.Objects // [] | .[]' \
                  | while read -r obj; do
                      key=$(echo "$obj" | jq -r '.Key')
                      versionId=$(echo "$obj" | jq -r '.VersionId')
                      echo "Deleting delete marker: $key ($versionId)"
                      aws s3api delete-object --bucket "${{ env.BUCKET_NAME }}" --key "$key" --version-id "$versionId"
                  done

                  aws s3 rb s3://${{ env.BUCKET_NAME }} --force
    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - s3-bucket-verification
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label
    report-failure:
        name: Report failure
        runs-on: ubuntu-latest
        if: failure()
        needs:
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@e9a9f33ab193348a82a79bd9250fdf12f708390a
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
