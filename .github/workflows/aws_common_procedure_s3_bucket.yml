---
name: Tests - Integration - AWS S3 bucket for Terraform backend

# https://docs.camunda.io/docs/self-managed/setup/deploy/amazon/amazon-eks/eks-terraform/#create-an-s3-bucket-for-terraform-state-management


permissions:
    contents: read
    pull-requests: write # allow comments, labels (used by internal-apply-skip-label)
on:
    workflow_dispatch:
    pull_request:
        paths:
            - .github/workflows/aws_common_procedure_s3_bucket.yml
            - .tool-versions
            - aws/common/procedure/s3-bucket/**
            - .github/actions/aws-configure-cli/**
            - .github/actions/internal-apply-skip-label/**
    schedule:
        - cron: 0 0 * * 4 # every Thursday at midnight

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: true

env:
    IS_SCHEDULE: ${{ (contains(github.head_ref, 'schedules/') || github.event_name == 'schedule') && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    AWS_PROFILE: infraex
    AWS_REGION: eu-west-2

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    s3-bucket-verification:
        name: Verify S3 related doc scripts
        runs-on: ubuntu-latest
        needs: triage

        steps:
            - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@193a21e1e56c9a65517a822224ac3b4ffa4d6ae4 # 1.5.9

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Generate random S3 bucket name
              working-directory: aws/common/procedure/s3-bucket
              run: |
                  set -euo pipefail
                  export BUCKET_NAME="camunda-s3-tf-docs-$(date +%s)-$(cat /proc/sys/kernel/random/uuid | head -c 8)"
                  echo "BUCKET_NAME=$BUCKET_NAME" | tee -a "$GITHUB_ENV"

                  sed -i "s/^export S3_TF_BUCKET_NAME=\".*\"/export S3_TF_BUCKET_NAME=\"$BUCKET_NAME\"/" ./s3-bucket-creation.sh

            - name: Execute S3 scripts
              working-directory: aws/common/procedure/s3-bucket
              run: |
                  set -euo pipefail

                  # Creation and versioning of S3 Bucket
                  echo "Creating S3 bucket ${BUCKET_NAME} for Terraform state management"
                  ./s3-bucket-creation.sh

                  # previous step export is encapsulated to its execution
                  export S3_TF_BUCKET_NAME="${BUCKET_NAME}"

                  echo "Enabling versioning for S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-versioning.sh

                  echo "Enabling private bucket policy for S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-private.sh

                  echo "Verifying S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-verify.sh

                  # Terraform init part
                  cat <<EOF > config.tf
                  terraform {
                    backend "s3" {}
                  }
                  EOF

                  echo "Initializing Terraform with S3 backend"
                  ./s3-bucket-tf-init.sh

                  rm -rf config.tf

            - name: Delete S3 bucket
              if: always()
              run: |
                  set -euo pipefail

                  # Delete all versions
                  aws s3api list-object-versions --bucket "${{ env.BUCKET_NAME }}" \
                    --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
                    --output=json \
                  | jq -c '.Objects // [] | .[]' \
                  | while read -r obj; do
                      key=$(echo "$obj" | jq -r '.Key')
                      versionId=$(echo "$obj" | jq -r '.VersionId')
                      echo "Deleting version: $key ($versionId)"
                      aws s3api delete-object --bucket "${{ env.BUCKET_NAME }}" --key "$key" --version-id "$versionId"
                  done

                  # Delete delete markers
                  aws s3api list-object-versions --bucket "${{ env.BUCKET_NAME }}" \
                    --query='{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' \
                    --output=json \
                  | jq -c '.Objects // [] | .[]' \
                  | while read -r obj; do
                      key=$(echo "$obj" | jq -r '.Key')
                      versionId=$(echo "$obj" | jq -r '.VersionId')
                      echo "Deleting delete marker: $key ($versionId)"
                      aws s3api delete-object --bucket "${{ env.BUCKET_NAME }}" --key "$key" --version-id "$versionId"
                  done

                  aws s3 rb s3://${{ env.BUCKET_NAME }} --force

    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - s3-bucket-verification
        steps:
            - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6

            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label

    report-failure:
        name: Report failure
        runs-on: ubuntu-latest
        if: failure()
        needs:
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@193a21e1e56c9a65517a822224ac3b4ffa4d6ae4 # 1.5.9
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
