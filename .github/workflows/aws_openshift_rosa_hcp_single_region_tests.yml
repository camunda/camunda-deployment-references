---
name: Tests - Integration - AWS OpenShift ROSA HCP Single Region

# description: This workflow perform integration tests against ROSA HCP platform

on:
    schedule:
        - cron: 0 3 * * 1 # Runs at 3 AM on Monday
    pull_request:
        paths:
            - .github/workflows/aws_openshift_rosa_hcp_single_region_tests.yml
            - .github/workflows-config/aws-openshift-rosa-hcp-single-region/test_matrix.yml
            - .tool-versions
            - generic/kubernetes/single-region/**
            - generic/openshift/single-region/**
            - aws/openshift/rosa-hcp-single-region/**
            - '!aws/openshift/rosa-hcp-single-region/test/golden/**'
            - .github/actions/aws-openshift-rosa-hcp-single-region-create/**
            - .github/actions/aws-openshift-rosa-hcp-single-region-cleanup/**
            - .github/actions/aws-configure-cli/**
            - .github/actions/internal-helm-chart-tests/**
            - .github/actions/internal-apply-skip-label/**
            - .github/actions/internal-generic-encrypt-export/**
            - .github/actions/internal-generic-decrypt-import/**
            - .github/actions/internal-tests-matrix/**

    workflow_dispatch:
        inputs:
            cluster_name:
                description: Cluster name.
                required: false
                type: string
            delete_clusters:
                description: Whether to delete the clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true

            ref-arch:
                description: |
                    Reference architecture to use, can only deploy one at a time.
                    Use a different trigger with unique names for each ref-arch.
                    Valid values are `rosa-hcp-single-region`.
                    Only for workflow_dispatch.
                required: false
                type: string
                default: rosa-hcp-single-region

# limit to a single execution per actor of this workflow
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    # we don't cancel the previous run, so it can finish it and let clusters in a proper state
    cancel-in-progress: false

env:
    IS_SCHEDULE: ${{ contains(github.head_ref, 'schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    AWS_PROFILE: infex
    AWS_REGION: eu-west-2
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BUCKET_REGION: eu-central-1
    S3_BACKEND_BUCKET_PREFIX: aws/openshift/rosa-hcp-single-region/ # keep it synced with the name of the module for simplicity

    CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}

    # TEST VARIABLES

    # Vars with "CI_" prefix are used in the CI workflow only.
    CI_MATRIX_FILE: .github/workflows-config/aws-openshift-rosa-hcp-single-region/test_matrix.yml

    # Docker Hub auth to avoid image pull rate limit.
    # Vars with "TEST_" prefix are used in the test runner tool (Task).
    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: main   # git reference used to clone the camunda/camunda-platform-helm repository to perform the tests
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo   # where to clone it

    # Components that are not enabled by default in the doc, but enabled in our tests to have a better coverage
    WEBMODELER_ENABLED: 'true'
    CONSOLE_ENABLED: 'true'

    ROSA_CLI_VERSION: latest

jobs:
    clusters-info:
        name: Define Matrix
        runs-on: ubuntu-latest
        outputs:
            platform-matrix: ${{ steps.matrix.outputs.platform_matrix }}
        steps:
            - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@c585602ff49e32c4c3b2dc4be1d7e7a0017961db # 1.3.9

            - name: Define tests matrix
              uses: ./.github/actions/internal-tests-matrix
              id: matrix
              with:
                  ci_matrix_file: ${{ env.CI_MATRIX_FILE }}
                  cluster_name: ${{ inputs.cluster_name }}
                  ref_arch: ${{ inputs.ref-arch }}
                  is_schedule: ${{ env.IS_SCHEDULE }}
                  is_renovate_pr: ${{ env.IS_RENOVATE_PR }}
    prepare-clusters:
        name: Prepare clusters
        needs:
            - clusters-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@c585602ff49e32c4c3b2dc4be1d7e7a0017961db # 1.3.9

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY;
                      secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;
                      secret/data/products/infrastructure-experience/ci/common CI_OPENSHIFT_MAIN_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_OPENSHIFT_MAIN_USERNAME;

            - name: Add profile credentials to ~/.aws/credentials
              shell: bash
              run: |
                  aws configure set aws_access_key_id ${{ steps.secrets.outputs.AWS_ACCESS_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set aws_secret_access_key ${{ steps.secrets.outputs.AWS_SECRET_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set region ${{ env.AWS_REGION }} --profile ${{ env.AWS_PROFILE }}

            - name: Set current target branch
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"

            # Also remove the versioning
            - name: Create ROSA cluster and login
              uses: ./.github/actions/aws-openshift-rosa-hcp-single-region-create
              id: create_cluster
              # Do not interrupt tests; otherwise, the Terraform state may become inconsistent.
              if: always() && success()
              with:
                  rh-token: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
                  cluster-name: ${{ matrix.distro.clusterName }}-${{matrix.scenario.shortName }}
                  admin-username: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_USERNAME }}
                  admin-password: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_PASSWORD }}
                  aws-region: ${{ env.AWS_REGION }}
                  s3-backend-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  s3-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  s3-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/
                  openshift-version: ${{ matrix.distro.version }}
                  tf-modules-revision: ${{ github.ref }}

                  tags: >
                      {
                        "ci-run-id": "${{ github.run_id }}",
                        "ci-run-number": "${{ github.run_number }}",
                        "ci-workflow": "${{ github.workflow }}",
                        "ci-actor": "${{ github.actor }}",
                        "ci-ref": "${{ github.ref }}",
                        "ci-commit": "${{ github.sha }}",
                        "ci-repo": "${{ github.repository }}",
                        "ci-event": "${{ github.event_name }}",
                        "map-migrated": "migARUADZHVWZ"
                      }

            - name: Dump kubeconfig before encryption
              run: |
                  kubectl config view --raw > "${{ runner.temp }}/kubeconfig.yaml"

            - name: Export kubeconfig and encrypt it # this is required to pass matrix outputs securely using artifacts
              id: export_kube_config
              uses: ./.github/actions/internal-generic-encrypt-export
              with:
                  file_path: ${{ runner.temp }}/kubeconfig.yaml
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}

            ## Write for matrix outputs workaround
            - uses: cloudposse/github-action-matrix-outputs-write@ed06cf3a6bf23b8dce36d1cf0d63123885bb8375 # v1
              id: out
              with:
                  matrix-step-name: ${{ github.job }}
                  matrix-key: ${{ matrix.distro.name }}-${{ matrix.scenario.name }}
                  outputs: |-
                      kubeconfig_encrypted: ${{ steps.export_kube_config.outputs.encrypted_file_base64 }}

    access-info:
        name: Read kube configs from matrix
        runs-on: ubuntu-latest
        needs: prepare-clusters
        outputs:
            config: ${{ steps.read-workflow.outputs.result }}
        steps:
            - uses: cloudposse/github-action-matrix-outputs-read@33cac12fa9282a7230a418d859b93fdbc4f27b5a # v1
              id: read-workflow
              with:
                  matrix-step-name: prepare-clusters

    integration-tests:
        name: Run integration tests - ${{ matrix.distro.name }}
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - access-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        env:
            TEST_NAMESPACE: camunda   # This namespace is hard-coded in the documentation
            # https://github.com/camunda/camunda-platform-helm/blob/test/integration/scenarios/chart-full-setup/Taskfile.yaml#L12C15-L12C32
            TEST_CLUSTER_TYPE: openshift
        steps:
            - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

            - name: Install asdf tools with cache for the project
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@c585602ff49e32c4c3b2dc4be1d7e7a0017961db # 1.3.9

            - name: Install CLI tools from OpenShift Mirror
              uses: redhat-actions/openshift-tools-installer@144527c7d98999f2652264c048c7a9bd103f8a82 # v1
              with:
                  oc: ${{ matrix.distro.version }}

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_USER;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_ID;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_SECRET;
            - name: ðŸ” Retrieve kubeconfig from outputs
              uses: ./.github/actions/internal-generic-decrypt-import
              with:
                  output_path: ${{ runner.temp }}/kubeconfig
                  encrypted_file_base64: >
                      ${{ fromJson(needs.access-info.outputs.config).kubeconfig_encrypted[
                        format(
                          '{0}-{1}',
                          matrix.distro.name,
                          matrix.scenario.name
                        )
                      ] }}
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}

            - name: ðŸ” Login into the cluster
              timeout-minutes: 2
              run: |
                  mkdir -p "$HOME/.kube"
                  mv "${{ runner.temp }}/kubeconfig" "$HOME/.kube/config"

            - name: ðŸ“ Get a copy of the reference architecture
              run: |
                  # run it as specified in the doc
                  set -euxo pipefail # tolerate, nothing.

                  ./aws/openshift/rosa-hcp-single-region/procedure/get-your-copy.sh
                  tree

            - name: ðŸ—ï¸ Prepare a fresh namespace for the tests
              # we need to retry due as the cluster has just been created and the OIDC provider may not be available yet
              uses: nick-fields/retry@ce71cc2ab81d554ebbe88c79ab5975992d79ba08 # v3
              with:
                  timeout_minutes: 10
                  max_attempts: 40
                  shell: bash
                  retry_wait_seconds: 15
                  command: |
                      set -o errexit # this is required https://github.com/nick-fields/retry/issues/133
                      set -o pipefail

                      cd ./aws/openshift/rosa-hcp-single-region/
                      source ./procedure/gather-cluster-login-id.sh
                      cd -

                      echo "CLUSTER_NAME=$CLUSTER_NAME"
                      echo "CLUSTER_API_URL=$CLUSTER_API_URL"

                      # Delete the namespace to ensure a fresh start
                      if kubectl get namespace "$TEST_NAMESPACE" &>/dev/null; then
                        kubectl delete namespace "$TEST_NAMESPACE" --wait
                        while kubectl get namespace "$TEST_NAMESPACE" &>/dev/null; do
                          echo "Namespace $TEST_NAMESPACE still being deleted, waiting..."
                          sleep 5
                        done
                      fi

                      oc new-project "$TEST_NAMESPACE" \
                        --description="Integration project of $TEST_NAMESPACE" \
                        --display-name="$TEST_NAMESPACE"

                      echo "Sleeping 30s"
                      sleep 30

            - name: ðŸ› ï¸ Assemble deployment values of generic/openshift/single-region
              run: |
                  set -o errexit
                  set -euxo pipefail # tolerate, nothing.

                  # As this action can be retried due to OpenShift API error, it must
                  # be stateless (all commmands can be rerun without issue)

                  echo "Construct the values.yml file"

                  cp -f generic/openshift/single-region/helm-values/base.yml ./values.yml

                  source ./generic/openshift/single-region/procedure/setup-application-domain.sh
                  echo "CAMUNDA_DOMAIN=$DOMAIN_NAME" | tee -a "$GITHUB_ENV"

                  source ./generic/openshift/single-region/procedure/get-ingress-http2-status.sh

                  ./generic/openshift/single-region/procedure/enable-ingress-http2.sh

                  # Enable Routes
                  for file in zeebe-gateway-route.yml operate-route.yml tasklist-route.yml connectors-route.yml domain.yml; do
                      yq ". *d load(\"generic/openshift/single-region/helm-values/$file\")" values.yml > values-result.yml
                      cat values-result.yml && mv values-result.yml values.yml
                  done

                  # Enable SCC
                  yq '. *d load("generic/openshift/single-region/helm-values/scc.yml")' values.yml > values-result.yml
                  cat values-result.yml && mv values-result.yml values.yml

                  if [ "$WEBMODELER_ENABLED" == "true" ]; then
                    echo "Enabling WebModeler"
                    yq -i '.webModeler.enabled = true' values.yml
                    yq -i '.postgresql.enabled = true' values.yml
                  fi

                  if [ "$CONSOLE_ENABLED" == "true" ]; then
                    echo "Enabling Console"
                    yq -i '.console.enabled = true' values.yml
                  fi

                  # Add integration tests values
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    for file in registry.yml identity.yml; do
                      yq ". *d load(\"generic/kubernetes/single-region/tests/helm-values/$file\")" values.yml > values-result.yml
                      cat values-result.yml && mv values-result.yml values.yml
                    done
                  fi

                  ./generic/openshift/single-region/procedure/assemble-envsubst-values.sh


            - name: ðŸ Install Camunda 8 using the generic/openshift helm chart procedure
              run: |
                  set -euxo pipefail # tolerate, nothing.

                  source generic/openshift/single-region/procedure/chart-env.sh
                  source generic/openshift/single-region/procedure/generate-passwords.sh

                  ./generic/openshift/single-region/procedure/create-identity-secret.sh

                  # Generate tests objects
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    # Create the pull secrets described in generic/kubernetes/single-region/tests/helm-values/registry.yml
                    kubectl create secret docker-registry index-docker-io \
                        --docker-server=index.docker.io \
                        --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                        --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                        --namespace="$TEST_NAMESPACE"

                    kubectl create secret generic identity-secret-for-components-integration \
                        --from-literal=identity-admin-client-id="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_ID }}" \
                        --from-literal=identity-admin-client-secret="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_SECRET }}" \
                        --namespace="$TEST_NAMESPACE"
                  fi

                  ./generic/openshift/single-region/procedure/install-chart.sh

            - name: ðŸ‘€â³ Wait for the deployment to be healthy using generic/kubernetes/single-region
              timeout-minutes: 10
              run: |
                  ./generic/kubernetes/single-region/procedure/check-deployment-ready.sh

            - name: ðŸ§™â€â™‚ï¸ Clone camunda/camunda-platform-helm
              if: env.TESTS_ENABLED == 'true'
              uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
              with:
                  repository: camunda/camunda-platform-helm
                  ref: ${{ env.TESTS_CAMUNDA_HELM_CHART_REPO_REF }}
                  path: ${{ env.TESTS_CAMUNDA_HELM_CHART_REPO_PATH }}
                  fetch-depth: 0

            - name: ðŸ§ª TESTS - Set variables
              if: env.TESTS_ENABLED == 'true'
              run: |
                  set -euxo pipefail # tolerate, nothing.

                  CAMUNDA_VERSION=$(cat .camunda-version)

                  export TEST_CHART_DIR_STATIC="$TESTS_CAMUNDA_HELM_CHART_REPO_PATH/charts/camunda-platform-$CAMUNDA_VERSION"
                  echo "TEST_CHART_DIR_STATIC=$TEST_CHART_DIR_STATIC" | tee -a "$GITHUB_ENV"

                  TEST_INGRESS_HOST="$CAMUNDA_DOMAIN"
                  echo "TEST_INGRESS_HOST=$TEST_INGRESS_HOST" | tee -a "$GITHUB_ENV"

                  # shellcheck disable=SC2002
                  TEST_CHART_VERSION=$(cat "$TEST_CHART_DIR_STATIC/Chart.yaml" | yq '.version')
                  echo "TEST_CHART_VERSION=$TEST_CHART_VERSION" | tee -a "$GITHUB_ENV"

                  # setup docker registry secret for tests
                  echo "TEST_CREATE_DOCKER_LOGIN_SECRET=true" | tee -a "$GITHUB_ENV"
                  echo "TEST_DOCKER_USERNAME_CAMUNDA_CLOUD=${{ steps.secrets.outputs.DOCKERHUB_USERNAME }}" | tee -a "$GITHUB_ENV"
                  echo "TEST_DOCKER_PASSWORD_CAMUNDA_CLOUD=${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" | tee -a "$GITHUB_ENV"

                  CI_TASKS_BASE_DIR="$TESTS_CAMUNDA_HELM_CHART_REPO_PATH/test/integration/scenarios/"
                  echo "CI_TASKS_BASE_DIR=$CI_TASKS_BASE_DIR" | tee -a "$GITHUB_ENV"
                  export TEST_CHART_DIR="../../../../charts/camunda-platform-$CAMUNDA_VERSION"
                  echo "TEST_CHART_DIR=$TEST_CHART_DIR" | tee -a "$GITHUB_ENV"
                  export TEST_VALUES_BASE_DIR="$TESTS_CAMUNDA_HELM_CHART_REPO_PATH/test/integration/scenarios"
                  echo "TEST_VALUES_BASE_DIR=$TEST_VALUES_BASE_DIR" | tee -a "$GITHUB_ENV"

                  # replace integration with the camunda release name as part of adaptation required to run the tests in our environment
                  find "$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/" -type f -print0 | xargs -0 sed -i 's/integration/camunda/g'

                  echo "Configure Venom tests"
                  # (adapted from https://github.com/camunda/camunda-platform-helm/blob/test/integration/scenarios/chart-full-setup/Taskfile.yaml#L56)
                  export VARIABLES_ENV_FILE="$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/variables.env"
                  # Enable the ingress tests
                  # shellcheck disable=SC2129
                  echo "VENOM_VAR_TEST_INGRESS_HOST=$TEST_INGRESS_HOST" >> "$VARIABLES_ENV_FILE"
                  echo "VENOM_VAR_TEST_CLIENT_ID=${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_ID }}" >> "$VARIABLES_ENV_FILE"
                  echo "VENOM_VAR_TEST_CLIENT_SECRET=${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_SECRET }}" >> "$VARIABLES_ENV_FILE"
                  echo "VENOM_EXTRA_ARGS=--var-from-file=./vars/variables-ingress-combined.yaml" >> "$VARIABLES_ENV_FILE"
                  ZEEBE_VERSION=$(yq '.zeebe.image.tag' "$TEST_CHART_DIR_STATIC/values.yaml")
                  # shellcheck disable=SC2129
                  echo "ZEEBE_VERSION=$ZEEBE_VERSION" >> "$VARIABLES_ENV_FILE"
                  # In case the Zeebe version has not been released officially yet.
                  echo "ZEEBE_VERSION_FALLBACK=8.5.6" >> "$VARIABLES_ENV_FILE"

                  # Some variables are not working correctly, patching it with yq directly
                  echo "VENOM_VAR_SKIP_TEST_INGRESS=true" >> "$VARIABLES_ENV_FILE"

                  echo "Patch the test files..."

                  # TODO: [BUG] remove the patchs when https://github.com/camunda/camunda-platform-helm/issues/3081 is fixed
                  echo "Patch expression ShoudBeFalse"
                  sed -i "s/ ShouldBeFalse/ ShouldEqual 'false'/g" \
                    "$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/testsuite-core.yaml" \
                    "$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/testsuite-preflight.yaml"

                  echo "Skip test ingress is also broken, fixing it"
                  yq eval '(.testcases[].steps[].skip |= map(select(test("skiptestingress", "i") | not)))' \
                    -i "$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/testsuite-core.yaml"
                  yq eval 'del(.. | select(has("skip") and .skip | length == 0).skip)' \
                    -i "$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/testsuite-core.yaml"

                  if [ "$WEBMODELER_ENABLED" != "true" ]; then
                    echo "Disable Webmodeler in the core tests as it's not enabled"

                    echo "VENOM_VAR_SKIP_TEST_WEBMODELER=false" >> "$VARIABLES_ENV_FILE"
                    yq eval 'del(.. | select(has("component") and .component == "WebModeler"))' \
                      -i "$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/testsuite-core.yaml"
                  fi

                  if [ "$CONSOLE_ENABLED" != "true" ]; then
                    echo "Disable Console in the core tests as it's not enabled"

                    yq eval 'del(.. | select(has("component") and .component == "Console"))' \
                      -i "$TEST_CHART_DIR_STATIC/test/integration/testsuites/vars/files/testsuite-core.yaml"
                  fi

                  echo "Patch the identity secrets to allow venom to access to the platform"
                  sed -i -e 's/integration-test-credentials/identity-secret-for-components/g' \
                         -e 's/identity-operate-client-password/operate-secret/g' \
                         -e 's/identity-optimize-client-password/optimize-secret/g' \
                         -e 's/identity-tasklist-client-password/tasklist-secret/g' \
                         -e 's/identity-zeebe-client-password/zeebe-secret/g' \
                         -e 's/identity-connectors-client-password/connectors-secret/g' \
                         "$TEST_CHART_DIR_STATIC/test/integration/testsuites/core/patches/job.yaml"

                  # remove venom var client secret as we define it in the file
                  yq e 'del(.spec.template.spec.containers[].env[] | select(.name == "VENOM_VAR_TEST_CLIENT_SECRET"))' \
                    -i "$TEST_CHART_DIR_STATIC/test/integration/testsuites/core/patches/job.yaml"

                  # we're overwriting the one of the Helm chart
                  echo "Ensure asdf tool is available in the test suite by using our global one"
                  cp .tool-versions "$TESTS_CAMUNDA_HELM_CHART_REPO_PATH"

            - name: ðŸ§ª TESTS - Run Preflight TestSuite
              if: env.TESTS_ENABLED == 'true'
              timeout-minutes: 10
              run: |
                  task -d "${CI_TASKS_BASE_DIR}/chart-full-setup" test.preflight

            - name: ðŸ§ª TESTS - Run Core TestSuite
              if: env.TESTS_ENABLED == 'true'
              timeout-minutes: 20
              run: |
                  task -d "${CI_TASKS_BASE_DIR}/chart-full-setup" test.core

            - name: ðŸ§ª TESTS - Run additional tests
              if: env.TESTS_ENABLED == 'true'
              timeout-minutes: 20
              run: |
                  set -euxo pipefail # tolerate, nothing.

                  echo "Show zeebe cluster topology using generic/kubernetes/single-region/procedure/check-zeebe-cluster-topology.sh:"
                  export ZEEBE_CLIENT_ID="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_ID }}"
                  export ZEEBE_CLIENT_SECRET="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_SECRET }}"
                  export DOMAIN_NAME="$CAMUNDA_DOMAIN"

                  # Execute the script and capture the output in a variable
                  check_zeebe_topology_output=$(./generic/kubernetes/single-region/procedure/check-zeebe-cluster-topology.sh)

                  echo "$check_zeebe_topology_output" | jq

                  # Checks
                  error_found=false
                  check_zeebe_topology_all_healthy=$(echo "$check_zeebe_topology_output" | jq '[.brokers[].partitions[].health == "healthy"] | all')
                  check_zeebe_topology_cluster_size=$(echo "$check_zeebe_topology_output" | jq '.clusterSize')
                  check_zeebe_topology_partitions_count=$(echo "$check_zeebe_topology_output" | jq '.partitionsCount')

                  if [ "$check_zeebe_topology_all_healthy" = "true" ]; then
                    echo "âœ… All partitions are healthy."
                  else
                    echo "âŒ Not all partitions are healthy"
                    error_found=true
                  fi

                  if [ "$check_zeebe_topology_cluster_size" -eq 3 ]; then
                    echo "âœ… Cluster size is 3."
                  else
                    echo "âŒ Cluster size is not 3."
                    error_found=true
                  fi

                  if [ "$check_zeebe_topology_partitions_count" -eq 3 ]; then
                    echo "âœ… Partitions count is 3."
                  else
                    echo "âŒ Partitions count is not 3."
                    error_found=true
                  fi

                  echo "Comparing golden file of the zeebe topology output..."

                  reference_file="./generic/kubernetes/single-region/procedure/check-zeebe-cluster-topology-output.json"
                  # Save the output to a temporary file
                  temp_output=$(mktemp)
                  echo "$check_zeebe_topology_output" > "$temp_output"

                  # Replace patch version
                  yq e '.brokers[].version |= sub("[.][0-9]+$", ".z") | .gatewayVersion |= sub("[.][0-9]+$", ".z")' -i "$temp_output"
                  yq e '.brokers[].version |= sub("[.][0-9]+$", ".z") | .gatewayVersion |= sub("[.][0-9]+$", ".z")' -i "$reference_file"

                  # Order each file also remove not predictable fields
                  yq e '.brokers |= sort_by(.host) | .brokers[] |= (.partitions |= sort_by(.partitionId) | .partitions[].role = "NOT_PREDICTABLE")' -i "$temp_output"
                  yq e '.brokers |= sort_by(.host) | .brokers[] |= (.partitions |= sort_by(.partitionId) | .partitions[].role = "NOT_PREDICTABLE")' -i "$reference_file"

                  # Compare the two files using diff (in compacted JSON format)
                  diff_output=$(delta <(jq -S . "$temp_output") <(jq -S . "$reference_file"))

                  if [[ -n "$diff_output" ]]; then
                    # If differences are found, print the error and the diff
                    echo "âŒ Error: The golden files of zeebe topology files do not match."
                    echo "Differences found:"
                    echo "$diff_output"

                    # Display the new generated version
                    echo "New version:"
                    cat "$temp_output"

                    error_found=true
                  fi

                  if [ "$error_found" = true ]; then
                    echo "âŒ Some tests failed."
                    exit 1
                  fi
                  echo "âœ… The cluster meets all the expected criteria."

            - name: ðŸ”¬ðŸš¨ Get failed Pods info
              if: failure()
              uses: camunda/camunda-platform-helm/./.github/actions/failed-pods-info@9f499f311306a7eabd2574d7e15d5ab153b65efa # main

    cleanup-clusters:
        name: Cleanup ROSA clusters
        if: always()
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - integration-tests
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        steps:
            - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              if: env.CLEANUP_CLUSTERS == 'true'
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@c585602ff49e32c4c3b2dc4be1d7e7a0017961db # 1.3.9

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY;
                      secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;

            - name: Add profile credentials to ~/.aws/credentials
              shell: bash
              if: env.CLEANUP_CLUSTERS == 'true'
              run: |
                  aws configure set aws_access_key_id ${{ steps.secrets.outputs.AWS_ACCESS_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set aws_secret_access_key ${{ steps.secrets.outputs.AWS_SECRET_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set region ${{ env.AWS_REGION }} --profile ${{ env.AWS_PROFILE }}

            - name: Set current target branch
              if: env.CLEANUP_CLUSTERS == 'true'
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"

            - name: Delete on-demand ROSA HCP Cluster
              uses: ./.github/actions/aws-openshift-rosa-hcp-single-region-cleanup
              if: always() && env.CLEANUP_CLUSTERS == 'true'
              timeout-minutes: 125
              env:
                  RHCS_TOKEN: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
              with:
                  tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  max-age-hours-cluster: 0
                  target: ${{ matrix.distro.clusterName }}-${{matrix.scenario.shortName }}
                  tf-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/

    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - integration-tests
            - cleanup-clusters
        steps:
            - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label

    report-failures:
        name: Report failures
        if: failure()
        runs-on: ubuntu-latest
        needs:
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@e9a9f33ab193348a82a79bd9250fdf12f708390a # 1.2.19
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
