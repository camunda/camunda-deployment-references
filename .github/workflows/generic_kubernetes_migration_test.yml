---
name: Tests - Integration - Generic Migration from Bitnami

permissions:
    contents: read
    pull-requests: write

on:
    schedule:
        - cron: 0 3 * * 2 # Runs at 3 AM on Tuesday (after operator-based tests on Monday)
    pull_request:
        paths:
            - .github/workflows/generic_kubernetes_migration_test.yml
            - .github/workflows-config/generic-kubernetes-migration-from-bitnami/test_matrix.yml
            - .tool-versions
            - generic/kubernetes/migration/**
            - generic/kubernetes/operator-based/**
            - local/kubernetes/kind-single-region/**
            - .github/actions/internal-apply-skip-label/**
            - .github/actions/internal-tests-matrix/**
            - .github/actions/internal-clean-namespace/**

    workflow_dispatch:
        inputs:
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true
            migration_component:
                description: |
                    Which component to test migration for.
                    Valid values are 'full', 'orchestration', 'identity', 'keycloak', 'webmodeler'.
                required: false
                type: string
                default: full

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: false

env:
    IS_SCHEDULE: ${{ (contains(github.head_ref, 'schedules/') || github.event_name == 'schedule') && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    CI_MATRIX_FILE: .github/workflows-config/generic-kubernetes-migration-from-bitnami/test_matrix.yml

    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}

    # renovate: datasource=github-tags depName=camunda/camunda-platform-helm
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: main
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    clusters-info:
        needs:
            - triage
        if: needs.triage.outputs.should_skip == 'false'
        name: Define Matrix
        runs-on: ubuntu-latest
        outputs:
            platform-matrix: ${{ steps.matrix.outputs.platform_matrix }}
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Define tests matrix
              uses: ./.github/actions/internal-tests-matrix
              id: matrix
              with:
                  ci_matrix_file: ${{ env.CI_MATRIX_FILE }}
                  cluster_prefix: kind-migr
                  ref_arch: migration-from-bitnami
                  is_schedule: ${{ env.IS_SCHEDULE }}
                  is_renovate_pr: ${{ env.IS_RENOVATE_PR }}

    migration-tests:
        name: Run migration tests - ${{ matrix.distro.name }} - ${{ matrix.scenario.name }}
        needs:
            - triage
            - clusters-info
        if: needs.triage.outputs.should_skip == 'false'
        runs-on: aws-core-8-default
        timeout-minutes: 60
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        env:
            CLUSTER_NAME: camunda-migration-local
            CAMUNDA_NAMESPACE: camunda
            CAMUNDA_RELEASE_NAME: camunda
            REF_ARCH_PATH: local/kubernetes/kind-single-region
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Install envsubst
              run: |
                  sudo apt-get update -qq
                  sudo apt-get install -y -qq gettext-base

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_USER;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_PASSWORD;

            # =================================================================
            # Create Kind cluster
            # =================================================================
            - name: Create Kind cluster
              working-directory: ${{ env.REF_ARCH_PATH }}
              run: ./procedure/cluster-create.sh

            - name: Create Docker Hub pull secret
              run: |
                  set -euo pipefail
                  kubectl create namespace "${{ env.CAMUNDA_NAMESPACE }}" --dry-run=client -o yaml | kubectl apply -f -
                  kubectl create secret docker-registry index-docker-io \
                      --docker-server=index.docker.io \
                      --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                      --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                      --namespace="${{ env.CAMUNDA_NAMESPACE }}"

            # =================================================================
            # STEP 1: Deploy Camunda with Bitnami sub-charts (SOURCE)
            # =================================================================
            - name: Deploy Camunda with Bitnami sub-charts
              timeout-minutes: 20
              run: |
                  set -euo pipefail

                  echo "Adding Camunda Helm repository..."
                  helm repo add camunda https://helm.camunda.io
                  helm repo update

                  # Pre-create the webModelerPostgresql secret required by the chart.
                  echo "Creating webModelerPostgresql secret..."
                  kubectl create secret generic camunda-postgresql-web-modeler \
                      --namespace="${{ env.CAMUNDA_NAMESPACE }}" \
                      --from-literal=password="webmodeler-password" \
                      --from-literal=postgres-password="$(openssl rand -base64 16)" \
                      || true

                  echo "Installing Camunda Platform with Bitnami sub-charts..."
                  helm install camunda camunda/camunda-platform \
                      --namespace "${{ env.CAMUNDA_NAMESPACE }}" \
                      --values generic/kubernetes/migration/tests/bitnami-values.yml \
                      --timeout 20m \
                      --wait

                  echo "Camunda with Bitnami sub-charts deployed successfully"

            - name: Wait for Bitnami deployment to be ready
              timeout-minutes: 15
              run: |
                  set -euo pipefail

                  echo "Waiting for all pods to be ready..."
                  sleep 60

                  kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}"

                  # Wait for key deployments
                  kubectl rollout status deployment/camunda-zeebe-gateway -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true
                  kubectl rollout status deployment/camunda-operate -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true
                  kubectl rollout status deployment/camunda-identity -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true

                  echo "Bitnami deployment ready"

            # =================================================================
            # STEP 1.5: Generate Test Data for Migration Validation
            # =================================================================
            - name: Generate test data using Camunda 8 Benchmark
              timeout-minutes: 15
              run: |
                  set -euo pipefail

                  echo "Generating test data using camunda-8-benchmark..."

                  kubectl apply -n "${{ env.CAMUNDA_NAMESPACE }}" \
                      -f generic/kubernetes/migration/tests/benchmark-job.yml

                  echo "Waiting for benchmark job to start..."
                  sleep 10

                  echo "Benchmark is running, waiting for data generation (60s)..."
                  sleep 60

                  kubectl get job migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" || true
                  kubectl logs job/migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" --tail=50 || true

                  kubectl delete job migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" --ignore-not-found=true || true

                  kubectl create configmap migration-test-data-marker \
                      --namespace="${{ env.CAMUNDA_NAMESPACE }}" \
                      --from-literal=created_at="$(date -u +'%Y-%m-%dT%H:%M:%SZ')" \
                      --from-literal=generator="camunda-8-benchmark" \
                      --from-literal=process_id="benchmark" \
                      --dry-run=client -o yaml | kubectl apply -f -

                  echo "Test data generation completed"

            - name: Wait for data to propagate to Elasticsearch
              timeout-minutes: 5
              run: |
                  set -euo pipefail

                  echo "Waiting for data to propagate to Elasticsearch..."
                  sleep 60

                  ES_POD=$(kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}" \
                      -l "app=elasticsearch" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                  if [[ -n "$ES_POD" ]]; then
                      echo "Checking Elasticsearch indices..."
                      kubectl exec -n "${{ env.CAMUNDA_NAMESPACE }}" "$ES_POD" -- \
                          curl -s "localhost:9200/_cat/indices?v" | grep -E "(operate|tasklist|optimize|connectors)" || true

                      INSTANCE_COUNT=$(kubectl exec -n "${{ env.CAMUNDA_NAMESPACE }}" "$ES_POD" -- \
                          curl -s "localhost:9200/operate-list-view-*/_count" 2>/dev/null | jq -r '.count // 0' || echo "0")
                      echo "Found ${INSTANCE_COUNT} process instances in Elasticsearch"
                  fi

                  echo "Data propagation complete"

            # =================================================================
            # STEP 2: Run Migration (4 phases)
            # =================================================================
            - name: Set migration environment variables
              run: |
                  set -euo pipefail

                  cat >> "$GITHUB_ENV" <<EOF
                  NAMESPACE=${{ env.CAMUNDA_NAMESPACE }}
                  CAMUNDA_RELEASE_NAME=camunda
                  BACKUP_PVC=migration-backup-pvc
                  BACKUP_STORAGE_SIZE=10Gi
                  MIGRATE_IDENTITY=true
                  MIGRATE_KEYCLOAK=true
                  MIGRATE_WEBMODELER=true
                  MIGRATE_ELASTICSEARCH=true
                  PG_TARGET_MODE=operator
                  ES_TARGET_MODE=operator
                  EOF

            # --- Phase 1: Deploy Targets (no downtime) ---
            - name: Phase 1 - Deploy target infrastructure
              timeout-minutes: 20
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail
                  source ./env.sh
                  yes | ./1-deploy-targets.sh

            # --- Phase 2: Backup (no downtime) ---
            - name: Phase 2 - Initial backup (warm, no downtime)
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail
                  source ./env.sh
                  ./2-backup.sh

            # --- Phase 3: Cutover (downtime window) ---
            - name: Phase 3 - Cutover (freeze, backup, restore, switch)
              timeout-minutes: 20
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail
                  source ./env.sh
                  yes | ./3-cutover.sh

            # --- Phase 4: Validate ---
            - name: Phase 4 - Validate migration
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail
                  source ./env.sh
                  ./4-validate.sh

            # =================================================================
            # Debug & Cleanup
            # =================================================================
            - name: Setup tmate session for debugging
              if: failure()
              uses: mxschmitt/action-tmate@v3
              with:
                  detached: true
              timeout-minutes: 60

            - name: Get failed Pods info
              if: failure()
              uses: ./.github/actions/internal-debug-failed-pods
              with:
                  namespace: ${{ env.CAMUNDA_NAMESPACE }}
                  artifact-suffix: ${{ matrix.distro.name }}-${{ matrix.scenario.name }}

            - name: Cleanup Kind cluster
              if: always()
              run: |
                  set -euo pipefail
                  echo "Cleaning up Kind cluster: ${{ env.CLUSTER_NAME }}"
                  kind delete cluster --name "${{ env.CLUSTER_NAME }}" || true

    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - migration-tests
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label

    report-failures:
        name: Report failures
        if: failure()
        runs-on: ubuntu-latest
        needs:
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
