---
name: Tests - Integration - Generic Migration from Bitnami

permissions:
    contents: read
    pull-requests: write

on:
    schedule:
        - cron: 0 3 * * 2  # Runs at 3 AM on Tuesday (after operator-based tests on Monday)
    pull_request:
        paths:
            - .github/workflows/generic_kubernetes_migration_test.yml
            - .github/workflows-config/generic-kubernetes-migration-from-bitnami/test_matrix.yml
            - .tool-versions
            - generic/kubernetes/migration/**
            - generic/kubernetes/operator-based/**
            - aws/kubernetes/eks-single-region-irsa/**
            - '!aws/kubernetes/eks-single-region-irsa/test/golden/**'
            - .github/actions/aws-kubernetes-eks-single-region-create/**
            - .github/actions/aws-generic-terraform-cleanup/**
            - .github/actions/aws-configure-cli/**
            - .github/actions/internal-apply-skip-label/**
            - .github/actions/internal-generic-encrypt-export/**
            - .github/actions/internal-generic-decrypt-import/**
            - .github/actions/internal-tests-matrix/**
            - .github/actions/internal-clean-namespace/**

    workflow_dispatch:
        inputs:
            cluster_name:
                description: Cluster name.
                required: false
                type: string
            delete_clusters:
                description: Whether to delete the clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true
            migration_component:
                description: |
                    Which component to test migration for.
                    Valid values are 'full', 'orchestration', 'identity', 'keycloak', 'webmodeler'.
                required: false
                type: string
                default: full

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: false

env:
    IS_SCHEDULE: ${{ contains(github.head_ref, 'schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    AWS_PROFILE: infex
    AWS_REGION: eu-west-2
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BUCKET_REGION: eu-central-1
    TF_MODULES_DIRECTORY: ./.tf-modules-workflow/

    S3_BACKEND_BUCKET_PREFIX_EKS_SINGLE_REGION: aws/kubernetes/eks-single-region/

    # TODO : revert this before merge
    CLEANUP_CLUSTERS: false
    # CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}

    CI_MATRIX_FILE: .github/workflows-config/generic-kubernetes-migration-from-bitnami/test_matrix.yml

    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}

    # renovate: datasource=github-tags depName=camunda/camunda-platform-helm
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: main
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    clusters-info:
        needs:
            - triage
        if: needs.triage.outputs.should_skip == 'false'
        name: Define Matrix
        runs-on: ubuntu-latest
        outputs:
            platform-matrix: ${{ steps.matrix.outputs.platform_matrix }}
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Define tests matrix
              uses: ./.github/actions/internal-tests-matrix
              id: matrix
              with:
                  ci_matrix_file: ${{ env.CI_MATRIX_FILE }}
                  # TODO: revert later
                  cluster_name: migr
                  # cluster_name: ${{ inputs.cluster_name }}
                  ref_arch: migration-from-bitnami
                  is_schedule: ${{ env.IS_SCHEDULE }}
                  is_renovate_pr: ${{ env.IS_RENOVATE_PR }}

    prepare-clusters:
        name: Prepare clusters
        needs:
            - clusters-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common CI_ENCRYPTION_KEY;

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Set current target branch
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"

            - name: Create EKS cluster and login
              uses: ./.github/actions/aws-kubernetes-eks-single-region-create
              id: create_eks_cluster
              if: always() && success()
              with:
                  cluster-name: ${{ matrix.distro.clusterName }}-${{ matrix.scenario.shortName }}
                  aws-region: ${{ env.AWS_REGION }}
                  s3-backend-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  s3-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  s3-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX_EKS_SINGLE_REGION }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/
                  tf-modules-revision: ${{ github.ref }}
                  tf-modules-path: ${{ env.TF_MODULES_DIRECTORY }}
                  ref-arch: eks-single-region
                  deploy-aurora: 'false'
                  deploy-opensearch: 'false'
                  tags: >
                      {
                        "ci-run-id": "${{ github.run_id }}",
                        "ci-run-number": "${{ github.run_number }}",
                        "ci-workflow": "${{ github.workflow }}",
                        "ci-actor": "${{ github.actor }}",
                        "ci-ref": "${{ github.ref }}",
                        "ci-commit": "${{ github.sha }}",
                        "ci-repo": "${{ github.repository }}",
                        "ci-event": "${{ github.event_name }}",
                        "map-migrated": "migARUADZHVWZ"
                      }

            - name: Dump kubeconfig before encryption
              run: |
                  set -euo pipefail
                  kubectl config view --raw > "${{ runner.temp }}/kubeconfig.yaml"

            - name: Export kubeconfig and encrypt it
              id: export_kube_config
              uses: ./.github/actions/internal-generic-encrypt-export
              with:
                  file_path: ${{ runner.temp }}/kubeconfig.yaml
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}

            - uses: cloudposse/github-action-matrix-outputs-write@ed06cf3a6bf23b8dce36d1cf0d63123885bb8375 # v1
              if: always()
              id: out
              with:
                  matrix-step-name: ${{ github.job }}
                  matrix-key: ${{ matrix.distro.name }}-${{ matrix.scenario.name }}
                  outputs: |-
                      kubeconfig_encrypted: ${{ steps.export_kube_config.outputs.encrypted_file_base64 }}

    access-info:
        name: Read kube configs from matrix
        runs-on: ubuntu-latest
        needs: prepare-clusters
        outputs:
            config: ${{ steps.read-workflow.outputs.result }}
        steps:
            - uses: cloudposse/github-action-matrix-outputs-read@33cac12fa9282a7230a418d859b93fdbc4f27b5a # v1
              id: read-workflow
              with:
                  matrix-step-name: prepare-clusters

    migration-tests:
        name: Run migration tests - ${{ matrix.distro.name }} - ${{ matrix.scenario.name }}
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - access-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        env:
            CAMUNDA_NAMESPACE: camunda
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

            - name: Install asdf tools with cache for the project
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_USER;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_ENCRYPTION_KEY;

            - name: Configure AWS CLI for EKS access
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Retrieve kubeconfig from outputs
              uses: ./.github/actions/internal-generic-decrypt-import
              with:
                  output_path: ${{ runner.temp }}/kubeconfig
                  encrypted_file_base64: >
                      ${{ fromJson(needs.access-info.outputs.config).kubeconfig_encrypted[
                        format(
                          '{0}-{1}',
                          matrix.distro.name,
                          matrix.scenario.name
                        )
                      ] }}
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}

            - name: üîê Login into the cluster
              timeout-minutes: 2
              run: |
                  set -euo pipefail

                  mkdir -p "$HOME/.kube"
                  mv "${{ runner.temp }}/kubeconfig" "$HOME/.kube/config"

                  kubectl config current-context
                  kubectl get nodes

            - name: üìÅ Get a copy of the migration scripts
              timeout-minutes: 5
              run: |
                  set -euo pipefail
                  ls -la generic/kubernetes/migration/
                  echo "Migration scripts ready"

            - name: üèóÔ∏è Clean and recreate Camunda namespace
              timeout-minutes: 20
              uses: ./.github/actions/internal-clean-namespace
              with:
                  namespace: ${{ env.CAMUNDA_NAMESPACE }}
                  recreate: 'true'

            - name: Wait for cluster stability
              run: sleep 30

            # =========================================================================
            # STEP 1: Deploy Camunda with Bitnami sub-charts (SOURCE)
            # =========================================================================
            - name: üöÄ Deploy Camunda with Bitnami sub-charts
              timeout-minutes: 20
              run: |
                  set -euo pipefail

                  echo "Setting up environment variables..."
                  export CAMUNDA_NAMESPACE="${{ env.CAMUNDA_NAMESPACE }}"
                  export CAMUNDA_RELEASE_NAME="camunda"

                  echo "Adding Camunda Helm repository..."
                  helm repo add camunda https://helm.camunda.io
                  helm repo update

                  echo "Creating Docker registry secret..."
                  kubectl create secret docker-registry index-docker-io \
                      --docker-server=index.docker.io \
                      --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                      --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                      --namespace="${CAMUNDA_NAMESPACE}" || true

                  echo "Creating Bitnami Camunda values file..."
                  cat > /tmp/bitnami-values.yml <<'EOF'
                  # Camunda with Bitnami sub-charts (migration source)
                  global:
                    image:
                      pullSecrets:
                        - name: index-docker-io

                  # Enable Bitnami Elasticsearch (default)
                  elasticsearch:
                    enabled: true
                    replicas: 1
                    minimumMasterNodes: 1
                    resources:
                      requests:
                        cpu: 500m
                        memory: 1Gi
                      limits:
                        cpu: 1
                        memory: 2Gi

                  # Enable Bitnami PostgreSQL for Identity (renamed from postgresql to identityPostgresql in 8.8)
                  identityPostgresql:
                    enabled: true
                    auth:
                      username: identity
                      password: identity-password
                      database: identity

                  identity:
                    enabled: true

                  # Enable Bitnami Keycloak with PostgreSQL
                  identityKeycloak:
                    enabled: true
                    postgresql:
                      enabled: true
                      auth:
                        username: keycloak
                        password: keycloak-password
                        database: keycloak

                  # WebModeler with SMTP configuration required for Camunda 8.8+
                  webModeler:
                    enabled: true
                    restapi:
                      mail:
                        smtpHost: "smtp.example.com"
                        smtpPort: 587
                        smtpUser: "test@example.com"
                        smtpPassword: "test-password"
                        fromAddress: "noreply@example.com"

                  webModelerPostgresql:
                    enabled: true
                    auth:
                      username: webmodeler
                      password: webmodeler-password
                      database: webmodeler
                      # The secret name follows Bitnami PostgreSQL naming: <release>-<nameOverride>
                      existingSecret: "camunda-postgresql-web-modeler"
                      secretKeys:
                        adminPasswordKey: "postgres-password"
                        userPasswordKey: "password"

                  # Orchestration cluster configuration (replaces zeebe, operate, tasklist in 8.8+)
                  orchestration:
                    enabled: true
                    clusterSize: "1"
                    partitionCount: "1"
                    replicationFactor: "1"
                    profiles:
                      broker: true
                      operate: true
                      tasklist: true
                    resources:
                      requests:
                        cpu: 500m
                        memory: 1Gi

                  optimize:
                    enabled: false  # Disable Optimize for faster testing

                  connectors:
                    enabled: false  # Disable Connectors for faster testing
                  EOF

                  echo "Installing Camunda Platform with Bitnami sub-charts..."
                  helm install camunda camunda/camunda-platform \
                      --namespace "${CAMUNDA_NAMESPACE}" \
                      --values /tmp/bitnami-values.yml \
                      --timeout 15m \
                      --wait

                  echo "‚úì Camunda with Bitnami sub-charts deployed successfully"

            - name: ‚è≥ Wait for Bitnami deployment to be ready
              timeout-minutes: 15
              run: |
                  set -euo pipefail

                  echo "Waiting for all pods to be ready..."
                  sleep 60

                  kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}"

                  # Wait for key deployments
                  kubectl rollout status deployment/camunda-zeebe-gateway -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true
                  kubectl rollout status deployment/camunda-operate -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true
                  kubectl rollout status deployment/camunda-identity -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true

                  echo "‚úì Bitnami deployment ready"

            # =========================================================================
            # STEP 1.5: Generate Test Data for Migration Validation
            # =========================================================================
            - name: üìä Generate test data using Camunda 8 Benchmark
              timeout-minutes: 15
              run: |
                  set -euo pipefail

                  echo "Generating test data using camunda-8-benchmark..."

                  # Deploy the benchmark job that will create process instances
                  cat <<EOF | kubectl apply -n "${{ env.CAMUNDA_NAMESPACE }}" -f -
                  apiVersion: batch/v1
                  kind: Job
                  metadata:
                    name: migration-test-data-generator
                    labels:
                      app: migration-test-data
                  spec:
                    ttlSecondsAfterFinished: 3600
                    backoffLimit: 3
                    template:
                      metadata:
                        labels:
                          app: migration-test-data
                      spec:
                        restartPolicy: Never
                        containers:
                          - name: benchmark
                            image: camundacommunityhub/camunda-8-benchmark:main
                            env:
                              - name: CAMUNDA_CLIENT_MODE
                                value: "self-managed"
                              - name: CAMUNDA_CLIENT_ZEEBE_BASE_URL
                                value: "http://camunda-zeebe-gateway:26500"
                              - name: CAMUNDA_CLIENT_ZEEBE_GRPC_ADDRESS
                                value: "http://camunda-zeebe-gateway:26500"
                              - name: CAMUNDA_CLIENT_ZEEBE_PREFER_REST_OVER_GRPC
                                value: "false"
                              - name: BENCHMARK_START_PROCESSES
                                value: "true"
                              - name: BENCHMARK_START_PI_PER_SECOND
                                value: "5"
                              - name: BENCHMARK_AUTO_DEPLOY_PROCESS
                                value: "true"
                              - name: BENCHMARK_BPMN_PROCESS_ID
                                value: "benchmark"
                              - name: BENCHMARK_START_WORKERS
                                value: "true"
                              - name: BENCHMARK_JOB_TYPE
                                value: "benchmark-task"
                              - name: BENCHMARK_TASK_COMPLETION_DELAY
                                value: "100"
                              - name: BENCHMARK_WARMUP_PHASE_DURATION_MILLIS
                                value: "10000"
                              - name: BENCHMARK_START_RATE_ADJUSTMENT_STRATEGY
                                value: "none"
                            resources:
                              requests:
                                cpu: 500m
                                memory: 512Mi
                              limits:
                                cpu: 1
                                memory: 1Gi
                  EOF

                  echo "Waiting for benchmark job to start..."
                  sleep 10

                  # Wait for the job to run for a bit and create some data
                  echo "Benchmark is running, waiting for data generation (60s)..."
                  sleep 60

                  # Check job status
                  kubectl get job migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" || true
                  kubectl logs job/migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" --tail=50 || true

                  # Delete the job (we just needed it to generate some data)
                  kubectl delete job migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" --ignore-not-found=true || true

                  # Create a marker ConfigMap to track test data generation
                  kubectl create configmap migration-test-data-marker \
                      --namespace="${{ env.CAMUNDA_NAMESPACE }}" \
                      --from-literal=created_at="$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
                      --from-literal=generator="camunda-8-benchmark" \
                      --from-literal=process_id="benchmark" \
                      --dry-run=client -o yaml | kubectl apply -f -

                  echo "‚úì Test data generation completed"

            - name: ‚è≥ Wait for data to propagate to Elasticsearch
              timeout-minutes: 5
              run: |
                  set -euo pipefail

                  echo "Waiting for data to propagate to Elasticsearch..."
                  sleep 60

                  # Verify data in Elasticsearch
                  ES_POD=$(kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}" \
                      -l "app=elasticsearch" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                  if [[ -n "$ES_POD" ]]; then
                      echo "Checking Elasticsearch indices..."
                      kubectl exec -n "${{ env.CAMUNDA_NAMESPACE }}" "$ES_POD" -- \
                          curl -s "localhost:9200/_cat/indices?v" | grep -E "(operate|tasklist)" || true

                      # Count process instances
                      INSTANCE_COUNT=$(kubectl exec -n "${{ env.CAMUNDA_NAMESPACE }}" "$ES_POD" -- \
                          curl -s "localhost:9200/operate-list-view-*/_count" 2>/dev/null | jq -r '.count // 0' || echo "0")
                      echo "Found ${INSTANCE_COUNT} process instances in Elasticsearch"
                  fi

                  echo "‚úì Data propagation complete"

            # =========================================================================
            # STEP 2: Run Migration (4 phases)
            # =========================================================================
            - name: üìã Set migration environment variables
              run: |
                  set -euo pipefail

                  # Set environment variables for the new migration scripts (env.sh)
                  cat >> "$GITHUB_ENV" <<EOF
                  NAMESPACE=${{ env.CAMUNDA_NAMESPACE }}
                  CAMUNDA_RELEASE_NAME=camunda
                  BACKUP_PVC=migration-backup-pvc
                  BACKUP_STORAGE_SIZE=50Gi
                  MIGRATE_IDENTITY=true
                  MIGRATE_KEYCLOAK=true
                  MIGRATE_WEBMODELER=true
                  MIGRATE_ELASTICSEARCH=true
                  PG_TARGET_MODE=operator
                  ES_TARGET_MODE=operator
                  EOF

            # ‚îÄ‚îÄ‚îÄ Phase 1: Deploy Targets (no downtime) ‚îÄ‚îÄ‚îÄ
            - name: üîÑ Phase 1 ‚Äî Deploy target infrastructure
              timeout-minutes: 20
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail

                  # Source env.sh to load configuration
                  source ./env.sh

                  # Run Phase 1 non-interactively (pipe 'yes' to confirm prompts)
                  yes | ./1-deploy-targets.sh

            # ‚îÄ‚îÄ‚îÄ Phase 2: Backup (no downtime) ‚îÄ‚îÄ‚îÄ
            - name: üîÑ Phase 2 ‚Äî Initial backup (warm, no downtime)
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail
                  source ./env.sh
                  ./2-backup.sh

            # ‚îÄ‚îÄ‚îÄ Phase 3: Cutover (downtime window) ‚îÄ‚îÄ‚îÄ
            - name: üîÑ Phase 3 ‚Äî Cutover (freeze ‚Üí backup ‚Üí restore ‚Üí switch)
              timeout-minutes: 20
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail
                  source ./env.sh

                  # Pipe 'yes' to the downtime confirmation prompt
                  yes | ./3-cutover.sh

            # ‚îÄ‚îÄ‚îÄ Phase 4: Validate ‚îÄ‚îÄ‚îÄ
            - name: üîÑ Phase 4 ‚Äî Validate migration
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration/
              run: |
                  set -euo pipefail
                  source ./env.sh
                  ./4-validate.sh

            # =========================================================================
            # STEP 3: Verify Final State
            # =========================================================================
            - name: ‚úÖ Verify final deployment state
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  echo "=== Final Pod Status ==="
                  kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}"

                  echo ""
                  echo "=== ECK Elasticsearch Status ==="
                  kubectl get elasticsearch -n "${{ env.CAMUNDA_NAMESPACE }}" || echo "No ECK clusters found"

                  echo ""
                  echo "=== CNPG PostgreSQL Clusters ==="
                  kubectl get clusters.postgresql.cnpg.io -n "${{ env.CAMUNDA_NAMESPACE }}" || echo "No CNPG clusters found"

                  echo ""
                  echo "=== Keycloak Operator Instances ==="
                  kubectl get keycloaks.k8s.keycloak.org -n "${{ env.CAMUNDA_NAMESPACE }}" || echo "No Keycloak instances found"

                  echo ""
                  echo "‚úì Migration tests completed successfully!"

            - name: Validate migrated test data
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  echo "Validating test data after migration..."
                  export CAMUNDA_NAMESPACE="${{ env.CAMUNDA_NAMESPACE }}"
                  export CAMUNDA_RELEASE_NAME="camunda"

                  # Quick validation: check ES indices exist and PG tables present
                  ES_POD=$(kubectl get pods -n "${CAMUNDA_NAMESPACE}" \
                      -l "elasticsearch.k8s.elastic.co/cluster-name" \
                      -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                  if [[ -n "$ES_POD" ]]; then
                      ES_PWD=$(kubectl get secret elasticsearch-es-elastic-user -n "${CAMUNDA_NAMESPACE}" \
                          -o jsonpath='{.data.elastic}' 2>/dev/null | base64 -d || echo "")
                      INDICES=$(kubectl exec -n "${CAMUNDA_NAMESPACE}" "$ES_POD" -- \
                          curl -sf -u "elastic:${ES_PWD}" "localhost:9200/_cat/indices?h=index" 2>/dev/null || echo "")
                      echo "ECK Elasticsearch indices after migration:"
                      echo "$INDICES"

                      if echo "$INDICES" | grep -qE "(operate|tasklist|zeebe)"; then
                          echo "‚úì Camunda indices found in ECK Elasticsearch"
                      else
                          echo "‚ö† No Camunda indices detected ‚Äî data may not have migrated"
                      fi
                  fi

                  # Check CNPG clusters have tables
                  for CLUSTER in pg-identity pg-keycloak pg-webmodeler; do
                      SECRET="${CLUSTER}-secret"
                      if kubectl get secret "$SECRET" -n "${CAMUNDA_NAMESPACE}" &>/dev/null; then
                          PASSWORD=$(kubectl get secret "$SECRET" -n "${CAMUNDA_NAMESPACE}" \
                              -o jsonpath='{.data.password}' | base64 -d)
                          USER=$(kubectl get secret "$SECRET" -n "${CAMUNDA_NAMESPACE}" \
                              -o jsonpath='{.data.username}' | base64 -d)
                          DB=$(kubectl get secret "$SECRET" -n "${CAMUNDA_NAMESPACE}" \
                              -o jsonpath='{.data.dbname}' | base64 -d 2>/dev/null || echo "${CLUSTER#pg-}")

                          TABLE_COUNT=$(kubectl run "pg-check-${CLUSTER}-${RANDOM}" --rm -i --restart=Never \
                              --image=postgres:15-alpine -n "${CAMUNDA_NAMESPACE}" -- \
                              psql "postgresql://${USER}:${PASSWORD}@${CLUSTER}-rw:5432/${DB}" \
                              -tAc "SELECT count(*) FROM information_schema.tables WHERE table_schema='public'" \
                              2>/dev/null || echo "0")
                          echo "${CLUSTER}: ${TABLE_COUNT} tables in public schema"
                      fi
                  done

                  echo "‚úì Test data validation completed"

            - name: Get failed Pods info
              if: failure()
              run: |
                  set -euo pipefail

                  kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" get po
                  kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" describe po "$pod_name";
                    kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" logs "$pod_name" --tail=100;
                  done

    cleanup-clusters:
        name: Cleanup clusters
        if: always()
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - migration-tests
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}

        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              if: env.CLEANUP_CLUSTERS == 'true'
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Set current target branch
              if: env.CLEANUP_CLUSTERS == 'true'
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"

            - name: Delete on-demand EKS Cluster
              uses: ./.github/actions/aws-generic-terraform-cleanup
              if: always() && env.CLEANUP_CLUSTERS == 'true'
              timeout-minutes: 125
              with:
                  tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  max-age-hours: 0
                  target: ${{ matrix.distro.clusterName }}-${{ matrix.scenario.shortName }}
                  tf-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX_EKS_SINGLE_REGION }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/
                  modules-order: vpn,cluster

    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - migration-tests
            - cleanup-clusters
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label

    report-failures:
        name: Report failures
        if: failure()
        runs-on: ubuntu-latest
        needs:
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
