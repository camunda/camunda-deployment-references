---
name: Tests - Integration - AWS Kubernetes EKS Single Region (IRSA)

on:
    schedule:
        - cron: 0 3 * * 2 # Runs at 3 AM on Tuesday
    pull_request:
        paths:
            - .github/workflows/aws_kubernetes_eks-single_region_tests.yml
            - .tool-versions
            - generic/kubernetes/single-region/**
            - aws/kubernetes/eks-single-region/**
            - aws/kubernetes/eks-single-region-irsa/**
            - '!aws/kubernetes/eks-single-region/test/golden/**'
            - '!aws/kubernetes/eks-single-region-irsa/test/golden/**'
            - .github/actions/aws-kubernetes-eks-single-region-create/**

    push:

    workflow_dispatch:
        inputs:
            cluster_name:
                description: Cluster name.
                required: false
                type: string
            delete_clusters:
                description: Whether to delete the clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true

# limit to a single execution per actor of this workflow
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    # in case of renovate we don't cancel the previous run, so it can finish it
    # otherwise weekly renovate PRs with tf docs updates result in broken clusters
    cancel-in-progress: ${{ !contains('renovate[bot]', github.actor) }}

env:
    IS_SCHEDULE: ${{ contains(github.ref, 'refs/heads/schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    AWS_PROFILE: infex
    AWS_REGION: eu-north-1
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BUCKET_REGION: eu-central-1
    S3_BACKEND_BUCKET_PREFIX: aws/kubernetes/eks-single-region-irsa/ # keep it synced with the name of the module for simplicity


    CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}

    # TEST VARIABLES

    # Docker Hub auth to avoid image pull rate limit.
    # Vars with "TEST_" prefix are used in the test runner tool (Task).
    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}
    # renovate: datasource=github-tags depName=camunda/camunda-platform-helm
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: camunda-platform-11.3.0   # git reference used to clone the camunda/camunda-platform-helm repository to perform the tests
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo   # where to clone it

    # Components that are not enabled by default in the doc, but enabled in our tests to have a better coverage
    WEBMODELER_ENABLED: 'false'
    CONSOLE_ENABLED: 'false'
    REF_ARCH: eks-single-region-irsa # using as default as it does two in one.

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    prepare-cluster:
        name: Prepare cluster
        needs:
            - triage
        strategy:
            fail-fast: false
        runs-on: ubuntu-latest
        env:
            TF_MODULES_PATH: ./.action-tf-modules/aws-kubernetes-eks-single-region-create/
        outputs:
            cluster_name: ${{ steps.generate_cluster_name.outputs.CLUSTER_NAME }}
            cert_manager_irsa_arn: ${{ steps.export-terraform-outputs.outputs.CERT_MANAGER_IRSA_ARN }}
            external_dns_irsa_arn: ${{ steps.export-terraform-outputs.outputs.EXTERNAL_DNS_IRSA_ARN }}
            db_keycloak_name: ${{ steps.export-terraform-outputs.outputs.DB_KEYCLOAK_NAME }}
            db_keycloak_username: ${{ steps.export-terraform-outputs.outputs.DB_KEYCLOAK_USERNAME }}
            camunda_keycloak_service_account_name: ${{ steps.export-terraform-outputs.outputs.CAMUNDA_KEYCLOAK_SERVICE_ACCOUNT_NAME }}
            db_keycloak_password: ${{ steps.export-terraform-outputs.outputs.DB_KEYCLOAK_PASSWORD }}
            db_identity_name: ${{ steps.export-terraform-outputs.outputs.DB_IDENTITY_NAME }}
            db_identity_username: ${{ steps.export-terraform-outputs.outputs.DB_IDENTITY_USERNAME }}
            camunda_identity_service_account_name: ${{ steps.export-terraform-outputs.outputs.CAMUNDA_IDENTITY_SERVICE_ACCOUNT_NAME }}
            db_identity_password: ${{ steps.export-terraform-outputs.outputs.DB_IDENTITY_PASSWORD }}
            db_webmodeler_name: ${{ steps.export-terraform-outputs.outputs.DB_WEBMODELER_NAME }}
            db_webmodeler_username: ${{ steps.export-terraform-outputs.outputs.DB_WEBMODELER_USERNAME }}
            camunda_webmodeler_service_account_name: ${{ steps.export-terraform-outputs.outputs.CAMUNDA_WEBMODELER_SERVICE_ACCOUNT_NAME }}
            db_webmodeler_password: ${{ steps.export-terraform-outputs.outputs.DB_WEBMODELER_PASSWORD }}
            db_host: ${{ steps.export-terraform-outputs.outputs.DB_HOST }}
            db_role_keycloak_name: ${{ steps.export-terraform-outputs.outputs.DB_ROLE_KEYCLOAK_NAME }}
            db_role_keycloak_arn: ${{ steps.export-terraform-outputs.outputs.DB_ROLE_KEYCLOAK_ARN }}
            db_role_identity_name: ${{ steps.export-terraform-outputs.outputs.DB_ROLE_IDENTITY_NAME }}
            db_role_identity_arn: ${{ steps.export-terraform-outputs.outputs.DB_ROLE_IDENTITY_ARN }}
            db_role_webmodeler_name: ${{ steps.export-terraform-outputs.outputs.DB_ROLE_WEBMODELER_NAME }}
            db_role_webmodeler_arn: ${{ steps.export-terraform-outputs.outputs.DB_ROLE_WEBMODELER_ARN }}
            opensearch_host: ${{ steps.export-terraform-outputs.outputs.OPENSEARCH_HOST }}
            opensearch_role_name: ${{ steps.export-terraform-outputs.outputs.OPENSEARCH_ROLE_NAME }}
            opensearch_role_arn: ${{ steps.export-terraform-outputs.outputs.OPENSEARCH_ROLE_ARN }}
            camunda_zeebe_service_account_name: ${{ steps.export-terraform-outputs.outputs.CAMUNDA_ZEEBE_SERVICE_ACCOUNT_NAME }}
            camunda_operate_service_account_name: ${{ steps.export-terraform-outputs.outputs.CAMUNDA_OPERATE_SERVICE_ACCOUNT_NAME }}
            camunda_tasklist_service_account_name: ${{ steps.export-terraform-outputs.outputs.CAMUNDA_TASKLIST_SERVICE_ACCOUNT_NAME }}
            camunda_optimize_service_account_name: ${{ steps.export-terraform-outputs.outputs.CAMUNDA_OPTIMIZE_SERVICE_ACCOUNT_NAME }}
            aurora_endpoint: ${{ steps.export-terraform-outputs.outputs.AURORA_ENDPOINT }}
            aurora_port: ${{ steps.export-terraform-outputs.outputs.AURORA_PORT }}
            aurora_username: ${{ steps.export-terraform-outputs.outputs.AURORA_USERNAME }}
            aurora_password: ${{ steps.export-terraform-outputs.outputs.AURORA_PASSWORD }}
            opensearch_master_username: ${{ steps.export-terraform-outputs.outputs.OPENSEARCH_MASTER_USERNAME }}
            opensearch_master_password: ${{ steps.export-terraform-outputs.outputs.OPENSEARCH_MASTER_PASSWORD }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@7709c609789c5e27b757a85817483caadbb5939a # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY;

            - name: Add profile credentials to ~/.aws/credentials
              shell: bash
              run: |
                  aws configure set aws_access_key_id ${{ steps.secrets.outputs.AWS_ACCESS_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set aws_secret_access_key ${{ steps.secrets.outputs.AWS_SECRET_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set region ${{ env.AWS_REGION }} --profile ${{ env.AWS_PROFILE }}

            - name: Set current Camunda version
              id: camunda-version
              run: |
                  set -euo pipefail
                  CAMUNDA_VERSION=$(cat .camunda-version)
                  echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"

            - name: Generate random cluster_name
              id: generate_cluster_name
              run: |
                  set -euo pipefail
                  PR_NUMBER=0
                  if [[ "${{ github.event_name }}" == "pull_request" ]]; then
                      PR_NUMBER=${{ github.event.pull_request.number}}
                  fi
                  # limiting factor are OpenSearch domains of maximum 28 characters
                  # echo "CLUSTER_NAME=eks-sr-$PR_NUMBER-$(head /dev/urandom | tr -dc 'a-z0-9' | head -c 5)" | tee -a "$GITHUB_ENV" "$GITHUB_OUTPUT"
                  # TODO: remove temporary hardcoded cluster name
                  echo "CLUSTER_NAME=eks-sr-0-h4bfl" | tee -a "$GITHUB_ENV" "$GITHUB_OUTPUT"

            - name: Create K8S cluster and login
              uses: ./.github/actions/aws-kubernetes-eks-single-region-create
              id: create_cluster
              # Do not interrupt tests; otherwise, the Terraform state may become inconsistent.
              if: always() && success()
              with:
                  cluster-name: ${{ env.CLUSTER_NAME }}
                  aws-region: ${{ env.AWS_REGION }}
                  s3-backend-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  s3-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  s3-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX }}${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}/
                  tf-modules-revision: ${{ github.ref }}
                  tf-modules-path: ${{ env.TF_MODULES_PATH }}
                  ref-arch: ${{ env.REF_ARCH }}

            - name: Export relevant Terraform outputs
              working-directory: ${{ env.TF_MODULES_PATH }}/aws/kubernetes/${{ env.REF_ARCH }}/
              id: export-terraform-outputs
              run: |
                  source ${{ github.workspace }}/aws/kubernetes/${{ env.REF_ARCH }}/procedure/export-helm-values.sh
                  source ${{ github.workspace }}/aws/kubernetes/${{ env.REF_ARCH }}/procedure/vars-create-db.sh

                  if [[ "${{ matrix.scenario }}" =~ "irsa" ]]; then
                      source ${{ github.workspace }}/aws/kubernetes/${{ env.REF_ARCH }}/procedure/vars-create-os.sh
                  fi

                  # Export as outputs
                  echo "CERT_MANAGER_IRSA_ARN=$CERT_MANAGER_IRSA_ARN" | tee -a "$GITHUB_OUTPUT"
                  echo "EXTERNAL_DNS_IRSA_ARN=$EXTERNAL_DNS_IRSA_ARN" | tee -a "$GITHUB_OUTPUT"

                  # PostgreSQL
                  echo "DB_KEYCLOAK_NAME=$DB_KEYCLOAK_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_KEYCLOAK_USERNAME=$DB_KEYCLOAK_USERNAME" | tee -a "$GITHUB_OUTPUT"
                  echo "CAMUNDA_KEYCLOAK_SERVICE_ACCOUNT_NAME=$CAMUNDA_KEYCLOAK_SERVICE_ACCOUNT_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_KEYCLOAK_PASSWORD=$DB_KEYCLOAK_PASSWORD" | tee -a "$GITHUB_OUTPUT"


                  echo "DB_IDENTITY_NAME=$DB_IDENTITY_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_IDENTITY_USERNAME=$DB_IDENTITY_USERNAME" | tee -a "$GITHUB_OUTPUT"
                  echo "CAMUNDA_IDENTITY_SERVICE_ACCOUNT_NAME=$CAMUNDA_IDENTITY_SERVICE_ACCOUNT_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_IDENTITY_PASSWORD=$DB_IDENTITY_PASSWORD" | tee -a "$GITHUB_OUTPUT"

                  echo "DB_WEBMODELER_NAME=$DB_WEBMODELER_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_WEBMODELER_USERNAME=$DB_WEBMODELER_USERNAME" | tee -a "$GITHUB_OUTPUT"
                  echo "CAMUNDA_WEBMODELER_SERVICE_ACCOUNT_NAME=$CAMUNDA_WEBMODELER_SERVICE_ACCOUNT_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_WEBMODELER_PASSWORD=$DB_WEBMODELER_PASSWORD" | tee -a "$GITHUB_OUTPUT"

                  echo "DB_HOST=$DB_HOST" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_ROLE_KEYCLOAK_NAME=$DB_ROLE_KEYCLOAK_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_ROLE_KEYCLOAK_ARN=$DB_ROLE_KEYCLOAK_ARN" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_ROLE_IDENTITY_NAME=$DB_ROLE_IDENTITY_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_ROLE_IDENTITY_ARN=$DB_ROLE_IDENTITY_ARN" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_ROLE_WEBMODELER_NAME=$DB_ROLE_WEBMODELER_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "DB_ROLE_WEBMODELER_ARN=$DB_ROLE_WEBMODELER_ARN" | tee -a "$GITHUB_OUTPUT"

                  # OpenSearch
                  echo "OPENSEARCH_HOST=$OPENSEARCH_HOST" | tee -a "$GITHUB_OUTPUT"
                  echo "OPENSEARCH_ROLE_NAME=$OPENSEARCH_ROLE_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "OPENSEARCH_ROLE_ARN=$OPENSEARCH_ROLE_ARN" | tee -a "$GITHUB_OUTPUT"
                  echo "CAMUNDA_ZEEBE_SERVICE_ACCOUNT_NAME=$CAMUNDA_ZEEBE_SERVICE_ACCOUNT_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "CAMUNDA_OPERATE_SERVICE_ACCOUNT_NAME=$CAMUNDA_OPERATE_SERVICE_ACCOUNT_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "CAMUNDA_TASKLIST_SERVICE_ACCOUNT_NAME=$CAMUNDA_TASKLIST_SERVICE_ACCOUNT_NAME" | tee -a "$GITHUB_OUTPUT"
                  echo "CAMUNDA_OPTIMIZE_SERVICE_ACCOUNT_NAME=$CAMUNDA_OPTIMIZE_SERVICE_ACCOUNT_NAME" | tee -a "$GITHUB_OUTPUT"

                  echo "AURORA_ENDPOINT=$AURORA_ENDPOINT" | tee -a "$GITHUB_OUTPUT"
                  echo "AURORA_PORT=$AURORA_PORT" | tee -a "$GITHUB_OUTPUT"
                  echo "AURORA_USERNAME=$AURORA_USERNAME" | tee -a "$GITHUB_OUTPUT"
                  echo "AURORA_PASSWORD=$AURORA_PASSWORD" | tee -a "$GITHUB_OUTPUT"

                  echo "OPENSEARCH_MASTER_USERNAME=$OPENSEARCH_MASTER_USERNAME" | tee -a "$GITHUB_OUTPUT"
                  echo "OPENSEARCH_MASTER_PASSWORD=$OPENSEARCH_MASTER_PASSWORD" | tee -a "$GITHUB_OUTPUT"

    # END OF PREPARE CLUSTER

            # - name: Export kubeconfig and encrypt it # this is required to pass matrix outputs securely using artifacts
            #   id: export_kube_config
            #   run: |
            #       # shellcheck disable=SC2005
            #       kubectl config view --raw > kubeconfig.yaml 2>/dev/null
            #       openssl enc -aes-256-cbc -salt -in kubeconfig.yaml -out encrypted_kubeconfig.enc -pass pass:"${GITHUB_TOKEN}" -pbkdf2
            #       encrypted_kubeconfig_base64=$(base64 -w 0 encrypted_kubeconfig.enc)
            #       echo "kubeconfig_raw=${encrypted_kubeconfig_base64}" >> "$GITHUB_OUTPUT"

            ## Write for matrix outputs workaround
            # - uses: cloudposse/github-action-matrix-outputs-write@ed06cf3a6bf23b8dce36d1cf0d63123885bb8375 # v1
            #   id: out
            #   with:
            #       matrix-step-name: ${{ github.job }}
            #       matrix-key: ${{ matrix.distro.name }}
            #       outputs: |-
            #           kubeconfig_raw: ${{ steps.export_kube_config.outputs.kubeconfig_raw }}

    # access-info:
    #     name: Read kube configs from matrix
    #     runs-on: ubuntu-latest
    #     needs: prepare-clusters
    #     outputs:
    #         kubeconfig: ${{ steps.read-workflow.outputs.result }}
    #     steps:
    #         - uses: cloudposse/github-action-matrix-outputs-read@33cac12fa9282a7230a418d859b93fdbc4f27b5a # v1
    #           id: read-workflow
    #           with:
    #               matrix-step-name: prepare-clusters

    integration-tests:
        name: Run integration tests - ${{ matrix.scenario }} - ${{ matrix.declination }}
        runs-on: ubuntu-latest
        needs:
            - prepare-cluster
        strategy:
            fail-fast: false
            max-parallel: 1
            matrix:
                scenario:
                    # - eks-single-region
                    - eks-single-region-irsa
                declination:
                #     - domain
                    - no-domain
        env:
            TEST_NAMESPACE: camunda   # This namespace is hard-coded in the documentation
            # https://github.com/camunda/camunda-platform-helm/blob/test/integration/scenarios/chart-full-setup/Taskfile.yaml#L12C15-L12C32
            TEST_CLUSTER_TYPE: kubernetes
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

            - name: Install asdf tools with cache for the project
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@7709c609789c5e27b757a85817483caadbb5939a # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_USER;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_ID;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_SECRET;

            - name: Add profile credentials to ~/.aws/credentials
              shell: bash
              run: |
                  aws configure set aws_access_key_id ${{ steps.secrets.outputs.AWS_ACCESS_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set aws_secret_access_key ${{ steps.secrets.outputs.AWS_SECRET_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set region ${{ env.AWS_REGION }} --profile ${{ env.AWS_PROFILE }}

            - name: Export outputs as environment variables
              run: |
                  echo "CLUSTER_NAME=${{ needs.prepare-cluster.outputs.cluster_name }}" | tee -a "$GITHUB_ENV"
                  echo "CERT_MANAGER_IRSA_ARN=${{ needs.prepare-cluster.outputs.cert_manager_irsa_arn }}" | tee -a "$GITHUB_ENV"
                  echo "EXTERNAL_DNS_IRSA_ARN=${{ needs.prepare-cluster.outputs.external_dns_irsa_arn }}" | tee -a "$GITHUB_ENV"
                  echo "DB_KEYCLOAK_NAME=${{ needs.prepare-cluster.outputs.db_keycloak_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_KEYCLOAK_USERNAME=${{ needs.prepare-cluster.outputs.db_keycloak_username }}" | tee -a "$GITHUB_ENV"
                  echo "CAMUNDA_KEYCLOAK_SERVICE_ACCOUNT_NAME=${{ needs.prepare-cluster.outputs.camunda_keycloak_service_account_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_KEYCLOAK_PASSWORD=${{ needs.prepare-cluster.outputs.db_keycloak_password }}" | tee -a "$GITHUB_ENV"
                  echo "DB_IDENTITY_NAME=${{ needs.prepare-cluster.outputs.db_identity_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_IDENTITY_USERNAME=${{ needs.prepare-cluster.outputs.db_identity_username }}" | tee -a "$GITHUB_ENV"
                  echo "CAMUNDA_IDENTITY_SERVICE_ACCOUNT_NAME=${{ needs.prepare-cluster.outputs.camunda_identity_service_account_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_IDENTITY_PASSWORD=${{ needs.prepare-cluster.outputs.db_identity_password }}" | tee -a "$GITHUB_ENV"
                  echo "DB_WEBMODELER_NAME=${{ needs.prepare-cluster.outputs.db_webmodeler_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_WEBMODELER_USERNAME=${{ needs.prepare-cluster.outputs.db_webmodeler_username }}" | tee -a "$GITHUB_ENV"
                  echo "CAMUNDA_WEBMODELER_SERVICE_ACCOUNT_NAME=${{ needs.prepare-cluster.outputs.camunda_webmodeler_service_account_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_WEBMODELER_PASSWORD=${{ needs.prepare-cluster.outputs.db_webmodeler_password }}" | tee -a "$GITHUB_ENV"
                  echo "DB_HOST=${{ needs.prepare-cluster.outputs.db_host }}" | tee -a "$GITHUB_ENV"
                  echo "DB_ROLE_KEYCLOAK_NAME=${{ needs.prepare-cluster.outputs.db_role_keycloak_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_ROLE_KEYCLOAK_ARN=${{ needs.prepare-cluster.outputs.db_role_keycloak_arn }}" | tee -a "$GITHUB_ENV"
                  echo "DB_ROLE_IDENTITY_NAME=${{ needs.prepare-cluster.outputs.db_role_identity_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_ROLE_IDENTITY_ARN=${{ needs.prepare-cluster.outputs.db_role_identity_arn }}" | tee -a "$GITHUB_ENV"
                  echo "DB_ROLE_WEBMODELER_NAME=${{ needs.prepare-cluster.outputs.db_role_webmodeler_name }}" | tee -a "$GITHUB_ENV"
                  echo "DB_ROLE_WEBMODELER_ARN=${{ needs.prepare-cluster.outputs.db_role_webmodeler_arn }}" | tee -a "$GITHUB_ENV"
                  echo "OPENSEARCH_HOST=${{ needs.prepare-cluster.outputs.opensearch_host }}" | tee -a "$GITHUB_ENV"
                  echo "OPENSEARCH_ROLE_NAME=${{ needs.prepare-cluster.outputs.opensearch_role_name }}" | tee -a "$GITHUB_ENV"
                  echo "OPENSEARCH_ROLE_ARN=${{ needs.prepare-cluster.outputs.opensearch_role_arn }}" | tee -a "$GITHUB_ENV"
                  echo "CAMUNDA_ZEEBE_SERVICE_ACCOUNT_NAME=${{ needs.prepare-cluster.outputs.camunda_zeebe_service_account_name }}" | tee -a "$GITHUB_ENV"
                  echo "CAMUNDA_OPERATE_SERVICE_ACCOUNT_NAME=${{ needs.prepare-cluster.outputs.camunda_operate_service_account_name }}" | tee -a "$GITHUB_ENV"
                  echo "CAMUNDA_TASKLIST_SERVICE_ACCOUNT_NAME=${{ needs.prepare-cluster.outputs.camunda_tasklist_service_account_name }}" | tee -a "$GITHUB_ENV"
                  echo "CAMUNDA_OPTIMIZE_SERVICE_ACCOUNT_NAME=${{ needs.prepare-cluster.outputs.camunda_optimize_service_account_name }}" | tee -a "$GITHUB_ENV"
                  echo "AURORA_ENDPOINT=${{ needs.prepare-cluster.outputs.aurora_endpoint }}" | tee -a "$GITHUB_ENV"
                  echo "AURORA_PORT=${{ needs.prepare-cluster.outputs.aurora_port }}" | tee -a "$GITHUB_ENV"
                  echo "AURORA_USERNAME=${{ needs.prepare-cluster.outputs.aurora_username }}" | tee -a "$GITHUB_ENV"
                  echo "AURORA_PASSWORD=${{ needs.prepare-cluster.outputs.aurora_password }}" | tee -a "$GITHUB_ENV"
                  echo "OPENSEARCH_MASTER_USERNAME=${{ needs.prepare-cluster.outputs.opensearch_master_username }}" | tee -a "$GITHUB_ENV"
                  echo "OPENSEARCH_MASTER_PASSWORD=${{ needs.prepare-cluster.outputs.opensearch_master_password }}" | tee -a "$GITHUB_ENV"

            - name: Clean up databases
              run: |
                  # TODO:

            - name: 🔐 Login into the cluster
              timeout-minutes: 10
              run: |
                  set -euo pipefail
                  aws eks --region "${{ env.AWS_REGION }}" update-kubeconfig --name "${{ env.CLUSTER_NAME }}" --alias "${{ env.CLUSTER_NAME }}"

                  kubectl config current-context
                  kubectl get nodes

            - name: 📁 Get a copy of the reference architecture
              timeout-minutes: 10
              run: |
                  # run it as specified in the doc
                  set -euo pipefail

                  ./aws/kubernetes/${{ matrix.scenario }}/procedure/get-your-copy.sh
                  tree

            - name: 🏗️ Prepare a fresh namespace for the tests
              run: |
                  set -euo pipefail

                  # Delete the namespace to ensure a fresh start
                  if kubectl get namespace "$TEST_NAMESPACE" &>/dev/null; then
                  kubectl delete namespace "$TEST_NAMESPACE" --wait
                  while kubectl get namespace "$TEST_NAMESPACE" &>/dev/null; do
                      echo "Namespace $TEST_NAMESPACE still being deleted, waiting..."
                      sleep 5
                  done
                  fi

                  # Create the namespace
                  kubectl create namespace "$TEST_NAMESPACE"

            - name: 🛠️ Assemble deployment values of aws/kubernetes/${{ matrix.scenario }}/${{ matrix.declination }}
              timeout-minutes: 10
              run: |
                  set -o errexit
                  set -euxo pipefail

                  echo "Construct the values.yml file"

                  cp -f aws/kubernetes/${{ matrix.scenario }}/helm-values/values-${{ matrix.declination }}.yml ./values.yml

                  if [[ "${{ matrix.declination }}" == "domain" ]]; then
                    echo "CAMUNDA_DOMAIN=${{ env.CLUSTER_NAME }}.camunda.ie" | tee -a "$GITHUB_ENV"
                  fi

                  if [ "$WEBMODELER_ENABLED" == "true" ]; then
                    echo "Enabling WebModeler"
                    yq -i '.webModeler.enabled = true' values.yml
                    yq -i '.postgresql.enabled = true' values.yml
                  fi

                  if [ "$CONSOLE_ENABLED" == "true" ]; then
                    echo "Enabling Console"
                    yq -i '.console.enabled = true' values.yml
                  fi

                  # Add integration tests values
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    for file in registry.yml identity.yml; do
                      yq ". *d load(\"generic/kubernetes/single-region/tests/helm-values/$file\")" values.yml > values-result.yml
                      cat values-result.yml && mv values-result.yml values.yml
                    done
                  fi

                  ./generic/kubernetes/single-region/procedure/assemble-envsubst-values.sh

            - name: Execute general procedure scripts
              timeout-minutes: 10
              run: |
                  set -euo pipefail
                  ./aws/kubernetes/${{ matrix.scenario }}/procedure/check-env-variables.sh
                  ./aws/kubernetes/${{ matrix.scenario }}/procedure/create-setup-db-secret.sh

            - name: Execute non irsa procedure scripts
              if: ${{ !contains(matrix.scenario, 'irsa') }}
              timeout-minutes: 10
              run: |
                  ./aws/kubernetes/${{ matrix.scenario }}/procedure/create-external-db-secrets.sh

            - name: Execute irsa procedure scripts
              if: contains(matrix.scenario, 'irsa')
              timeout-minutes: 10
              run: |
                  ./aws/kubernetes/${{ matrix.scenario }}/procedure/create-setup-os-secret.sh

            - name: 🏁 Install Camunda 8 using the generic/kubernetes helm chart procedure
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  source generic/kubernetes/single-region/procedure/chart-env.sh
                  source generic/kubernetes/single-region/procedure/generate-passwords.sh

                  ./generic/kubernetes/single-region/procedure/create-identity-secret.sh

                  # Generate tests objects
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    # Create the pull secrets described in generic/kubernetes/single-region/tests/helm-values/registry.yml
                    kubectl create secret docker-registry index-docker-io \
                        --docker-server=index.docker.io \
                        --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                        --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                        --namespace="$TEST_NAMESPACE"

                    kubectl create secret generic identity-secret-for-components-integration \
                        --from-literal=identity-admin-client-id="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_ID }}" \
                        --from-literal=identity-admin-client-secret="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_SECRET }}" \
                        --namespace="$TEST_NAMESPACE"
                  fi

                  ./generic/kubernetes/single-region/procedure/install-chart.sh

            - name: 👀⏳ Wait for the deployment to be healthy using generic/kubernetes/single-region
              timeout-minutes: 10
              run: |
                  ./generic/kubernetes/single-region/procedure/check-deployment-ready.sh

            - name: 🔬🚨 Get failed Pods info
              if: failure()
              run: |
                  set -euo pipefail

                  kubectl -n "$TEST_NAMESPACE" get po
                  kubectl -n "$TEST_NAMESPACE" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "$TEST_NAMESPACE" describe po "$pod_name";
                    kubectl -n "$TEST_NAMESPACE" logs "$pod_name";
                  done

    # cleanup-clusters:
    #     name: Cleanup ROSA clusters
    #     if: always()
    #     runs-on: ubuntu-latest
    #     needs:
    #         - clusters-info
    #         - integration-tests
    #     strategy:
    #         fail-fast: false
    #         matrix:
    #             distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
    #     steps:
    #         - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
    #           if: env.CLEANUP_CLUSTERS == 'true'
    #           with:
    #               fetch-depth: 0

    #         - name: Install asdf tools with cache
    #           if: env.CLEANUP_CLUSTERS == 'true'
    #           uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1

    #         - name: Import Secrets
    #           id: secrets
    #           uses: hashicorp/vault-action@7709c609789c5e27b757a85817483caadbb5939a # v3
    #           if: env.CLEANUP_CLUSTERS == 'true'
    #           with:
    #               url: ${{ secrets.VAULT_ADDR }}
    #               method: approle
    #               roleId: ${{ secrets.VAULT_ROLE_ID }}
    #               secretId: ${{ secrets.VAULT_SECRET_ID }}
    #               exportEnv: false
    #               secrets: |
    #                   secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY;
    #                   secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY;
    #                   secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;

    #         - name: Add profile credentials to ~/.aws/credentials
    #           shell: bash
    #           if: env.CLEANUP_CLUSTERS == 'true'
    #           run: |
    #               aws configure set aws_access_key_id ${{ steps.secrets.outputs.AWS_ACCESS_KEY }} --profile ${{ env.AWS_PROFILE }}
    #               aws configure set aws_secret_access_key ${{ steps.secrets.outputs.AWS_SECRET_KEY }} --profile ${{ env.AWS_PROFILE }}
    #               aws configure set region ${{ env.AWS_REGION }} --profile ${{ env.AWS_PROFILE }}

    #         - name: Set current Camunda version
    #           id: camunda-version
    #           if: env.CLEANUP_CLUSTERS == 'true'
    #           run: |
    #               CAMUNDA_VERSION=$(cat .camunda-version)
    #               echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"

    #         - name: Delete on-demand ROSA HCP Cluster
    #           uses: ./.github/actions/aws-openshift-rosa-hcp-single-region-cleanup
    #           if: always() && env.CLEANUP_CLUSTERS == 'true'
    #           timeout-minutes: 125
    #           env:
    #               RHCS_TOKEN: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
    #           with:
    #               tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
    #               tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
    #               max-age-hours-cluster: 0
    #               target: ${{ matrix.distro.clusterName }}
    #               tf-bucket-key-prefix: ${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}/

    # report:
    #     name: Report failures
    #     if: failure()
    #     runs-on: ubuntu-latest
    #     needs:
    #         - integration-tests
    #         - cleanup-clusters
    #     steps:
    #         - name: Notify in Slack in case of failure
    #           id: slack-notification
    #           if: ${{ env.IS_SCHEDULE == 'true' }}
    #           uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1
    #           with:
    #               vault_addr: ${{ secrets.VAULT_ADDR }}
    #               vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
    #               vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
