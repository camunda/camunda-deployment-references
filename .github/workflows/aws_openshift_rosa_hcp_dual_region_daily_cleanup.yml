---
name: Tests - Daily Cleanup - AWS OpenShift ROSA HCP Dual Region

on:
    workflow_dispatch:
        inputs:
            max_age_hours_cluster:
                description: Maximum age of clusters in hours
                required: true
                default: '20'
    pull_request:
        paths:
            - .github/workflows/aws_openshift_rosa_hcp_dual_region_daily_cleanup.yml
            - .tool-versions
            - aws/openshift/rosa-hcp-dual-region/**
            - '!aws/openshift/rosa-hcp-dual-region/terraform/*/test/golden/**'
            - .github/actions/aws-openshift-rosa-hcp-dual-region-cleanup/**

    schedule:
        - cron: 0 1 * * * # At 01:00 everyday.

# limit to a single execution per actor of this workflow
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    # in case of renovate we don't cancel the previous run, so it can finish it
    # otherwise weekly renovate PRs with tf docs updates result in broken clusters
    cancel-in-progress: ${{ !contains('renovate[bot]', github.actor) }}

env:
    IS_SCHEDULE: ${{ contains(github.ref, 'refs/heads/schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}

    # TODO: revert this
    MAX_AGE_HOURS_CLUSTER: ${{ github.event.inputs.max_age_hours_cluster || '0' }}

    # please keep those variables synced with aws_rosa_hcp_tests.yml
    AWS_PROFILE: infex
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BACKEND_BUCKET_PREFIX: aws/openshift/rosa-hcp-dual-region/ # keep it synced with the name of the module for simplicity
    S3_BUCKET_REGION: eu-central-1
    CLUSTER_1_AWS_REGION: eu-west-2
    CLUSTER_2_AWS_REGION: eu-west-3


jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    cleanup-clusters:
        needs:
            - triage
        if: needs.triage.outputs.should_skip == 'false'
        runs-on: ubuntu-latest
        steps:
            - name: Checkout repository
              uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1

            - name: Use repo .tool-version as global version
              run: cp .tool-versions ~/.tool-versions

            - name: Set current Camunda version
              id: camunda-version
              run: |
                  CAMUNDA_VERSION=$(cat .camunda-version)
                  echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@a1b77a09293a4366e48a5067a86692ac6e94fdc0 # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY;
                      secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;

            # Official action does not support profiles
            - name: Add profile credentials to ~/.aws/credentials
              run: |
                  aws configure set aws_access_key_id ${{ steps.secrets.outputs.AWS_ACCESS_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set aws_secret_access_key ${{ steps.secrets.outputs.AWS_SECRET_KEY }} --profile ${{ env.AWS_PROFILE }}
                  aws configure set region ${{ env.CLUSTER_1_AWS_REGION }} --profile ${{ env.AWS_PROFILE }}

            - name: Delete clusters
              id: delete_clusters
              timeout-minutes: 125
              uses: ./.github/actions/aws-openshift-rosa-hcp-dual-region-cleanup
              env:
                  RHCS_TOKEN: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
              with:
                  tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  max-age-hours-cluster: ${{ env.MAX_AGE_HOURS_CLUSTER }}
                  tf-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX }}${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}/

            # There are cases where the deletion of resources fails due to dependencies.
            - name: Retry delete clusters
              id: retry_delete_clusters
              if: failure() && steps.delete_clusters.outcome == 'failure'
              timeout-minutes: 125
              uses: ./.github/actions/aws-openshift-rosa-hcp-dual-region-cleanup
              env:
                  RHCS_TOKEN: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
                  RETRY_DESTROY: 'true' # trigger cloud nuke
              with:
                  tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  max-age-hours-cluster: 0 # the previous step alters the age and resets it to 0
                  tf-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX }}${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}/

            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ failure() && env.IS_SCHEDULE == 'true' && steps.retry_delete_clusters.outcome == 'failure' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@e9a9f33ab193348a82a79bd9250fdf12f708390a # 1.2.19
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
