---
name: Tests - Integration - AWS S3 bucket for Terraform backend

# https://docs.camunda.io/docs/self-managed/setup/deploy/amazon/amazon-eks/eks-terraform/#create-an-s3-bucket-for-terraform-state-management

on:
    workflow_dispatch:
    pull_request:
        paths:
            - .github/workflows/aws_kubernetes_eks_single_region_tf_s3_bucket.yml
            - .tool-versions
            - aws/kubernetes/eks-single-region/procedure/s3*.sh
            - .github/actions/aws-configure-cli/**
    schedule:
        - cron: 0 0 * * 4 # every Thursday at midnight

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: true

env:
    IS_SCHEDULE: ${{ contains(github.ref, 'refs/heads/schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}

    AWS_PROFILE: infraex
    AWS_REGION: eu-west-2

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    s3-bucket-verification:
        name: Verify S3 related doc scripts
        runs-on: ubuntu-latest
        needs: triage

        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@6dc218bf7ee3812a4b6b13c305bce60d5d1d46e5 # 1.3.1

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Generate random S3 bucket name
              working-directory: aws/kubernetes/eks-single-region/procedure
              run: |
                  set -euo pipefail
                  export BUCKET_NAME="camunda-s3-tf-docs-$(date +%s)-$(uuidgen | head -c 8)"
                  echo "BUCKET_NAME=$BUCKET_NAME" | tee -a "$GITHUB_ENV"

                  sed -i "s/^export S3_TF_BUCKET_NAME=\".*\"/export S3_TF_BUCKET_NAME=\"$BUCKET_NAME\"/" ./s3-bucket-creation.sh

            - name: Execute S3 scripts
              working-directory: aws/kubernetes/eks-single-region/procedure
              run: |
                  set -euo pipefail

                  # Creation and versioning of S3 Bucket
                  echo "Creating S3 bucket ${BUCKET_NAME} for Terraform state management"
                  ./s3-bucket-creation.sh

                  # previous step export is encapsulated to its execution
                  export S3_TF_BUCKET_NAME="${BUCKET_NAME}"

                  echo "Enabling versioning for S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-versioning.sh

                  echo "Enabling private bucket policy for S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-private.sh

                  echo "Verifying S3 bucket ${BUCKET_NAME}"
                  ./s3-bucket-verify.sh

                  # Terraform init part
                  cat <<EOF > config.tf
                  terraform {
                    backend "s3" {}
                  }
                  EOF

                  echo "Initializing Terraform with S3 backend"
                  ./s3-bucket-tf-init.sh

                  rm -rf config.tf

            - name: Delete S3 bucket
              if: always()
              run: |
                  set -euo pipefail

                  # Delete all versions
                  aws s3api list-object-versions --bucket "${{ env.BUCKET_NAME }}" \
                    --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
                    --output=json \
                  | jq -c '.Objects // [] | .[]' \
                  | while read -r obj; do
                      key=$(echo "$obj" | jq -r '.Key')
                      versionId=$(echo "$obj" | jq -r '.VersionId')
                      echo "Deleting version: $key ($versionId)"
                      aws s3api delete-object --bucket "${{ env.BUCKET_NAME }}" --key "$key" --version-id "$versionId"
                  done

                  # Delete delete markers
                  aws s3api list-object-versions --bucket "${{ env.BUCKET_NAME }}" \
                    --query='{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' \
                    --output=json \
                  | jq -c '.Objects // [] | .[]' \
                  | while read -r obj; do
                      key=$(echo "$obj" | jq -r '.Key')
                      versionId=$(echo "$obj" | jq -r '.VersionId')
                      echo "Deleting delete marker: $key ($versionId)"
                      aws s3api delete-object --bucket "${{ env.BUCKET_NAME }}" --key "$key" --version-id "$versionId"
                  done

                  aws s3 rb s3://${{ env.BUCKET_NAME }} --force

    report:
        name: Report failures
        if: failure()
        runs-on: ubuntu-latest
        needs: s3-bucket-verification
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@e9a9f33ab193348a82a79bd9250fdf12f708390a # 1.2.19
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
