---
name: Tests - Integration - AWS OpenShift ROSA HCP Dual Region
permissions:
    contents: read
    pull-requests: write
on:
    schedule:
        - cron: 0 3 * * 2
    pull_request: {}
    workflow_dispatch:
        inputs:
            cluster_name:
                description: Cluster name.
                required: false
                type: string
            delete_clusters:
                description: Whether to delete the clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true
            ref-arch:
                description: |
                    Reference architecture to use, can only deploy one at a time.
                    Use a different trigger with unique names for each ref-arch.
                    Valid values are `rosa-hcp-dual-region`.
                    Only for workflow_dispatch.
                required: false
                type: string
                default: rosa-hcp-dual-region
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: false
env:
    IS_SCHEDULE: ${{ contains(github.head_ref, 'schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}
    AWS_PROFILE: infex
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BUCKET_REGION: eu-central-1
    S3_BACKEND_BUCKET_PREFIX: aws/openshift/rosa-hcp-dual-region/
    TF_MODULES_DIRECTORY: ./.tf-modules-workflow/
    CLUSTER_1_AWS_REGION: eu-west-2
    CLUSTER_2_AWS_REGION: eu-west-3
    CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}
    CI_MATRIX_FILE: .github/workflows-config/aws-openshift-rosa-hcp-dual-region/test_matrix.yml
    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: fix-venom-8-8
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo
    ROSA_CLI_VERSION: latest
jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip
    clusters-info:
        needs:
            - triage
        if: needs.triage.outputs.should_skip == 'false'
        name: Define Matrix
        runs-on: ubuntu-latest
        outputs:
            platform-matrix: ${{ steps.matrix.outputs.platform_matrix }}
        steps:
            - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
              with:
                  fetch-depth: 0
            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@eb9d51b4dc89deeda7fc160166f378725ee0f06a
            - name: Define tests matrix
              uses: ./.github/actions/internal-tests-matrix
              id: matrix
              with:
                  ci_matrix_file: ${{ env.CI_MATRIX_FILE }}
                  cluster_name: ${{ inputs.cluster_name }}
                  ref_arch: ${{ inputs.ref-arch }}
                  is_schedule: ${{ env.IS_SCHEDULE }}
                  is_renovate_pr: ${{ env.IS_RENOVATE_PR }}
    prepare-clusters:
        name: Prepare clusters
        needs:
            - clusters-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0
            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@eb9d51b4dc89deeda7fc160166f378725ee0f06a
            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;
                      secret/data/products/infrastructure-experience/ci/common CI_OPENSHIFT_MAIN_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_OPENSHIFT_MAIN_USERNAME;
                      secret/data/products/infrastructure-experience/ci/common CI_ENCRYPTION_KEY;
            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.CLUSTER_1_AWS_REGION }}
            - name: Set current target branch
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"
            - name: Create ROSA cluster and login
              uses: ./.github/actions/aws-openshift-rosa-hcp-dual-region-create
              id: create_clusters
              if: always() && success()
              with:
                  rh-token: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
                  cluster-name-1: ${{ matrix.distro.clusterName }}-${{matrix.scenario.shortName }}-1
                  cluster-name-2: ${{ matrix.distro.clusterName }}-${{matrix.scenario.shortName }}-2
                  admin-username-cluster-1: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_USERNAME }}
                  admin-username-cluster-2: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_USERNAME }}
                  admin-password-cluster-1: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_PASSWORD }}
                  admin-password-cluster-2: ${{ steps.secrets.outputs.CI_OPENSHIFT_MAIN_PASSWORD }}
                  aws-region-cluster-1: ${{ env.CLUSTER_1_AWS_REGION }}
                  aws-region-cluster-2: ${{ env.CLUSTER_2_AWS_REGION }}
                  s3-backend-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  s3-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  s3-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/
                  openshift-version-cluster-1: ${{ matrix.distro.version }}
                  openshift-version-cluster-2: ${{ matrix.distro.version }}
                  tf-modules-revision: ${{ github.ref }}
                  tf-modules-path: ${{ env.TF_MODULES_DIRECTORY }}
                  cleanup-tf-modules-path: 'false'
                  tags: >
                      {


                        "ci-run-id": "${{ github.run_id }}",
                        "ci-run-number": "${{ github.run_number }}",
                        "ci-workflow": "${{ github.workflow }}",
                        "ci-actor": "${{ github.actor }}",
                        "ci-ref": "${{ github.ref }}",
                        "ci-commit": "${{ github.sha }}",
                        "ci-repo": "${{ github.repository }}",
                        "ci-event": "${{ github.event_name }}",
                        "map-migrated": "migARUADZHVWZ"
                      }

            - name: Dump kubeconfig before encryption
              run: |
                  kubectl config view --raw > "${{ runner.temp }}/kubeconfig.yaml"
            - name: Export kubeconfig and encrypt it
              id: export_kube_config
              uses: ./.github/actions/internal-generic-encrypt-export
              with:
                  file_path: ${{ runner.temp }}/kubeconfig.yaml
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}
            - name: Dump other secrets from the action
              id: dump_secrets
              run: |
                  set -euo pipefail

                  yq eval -n \
                    '.["backup-bucket-s3-aws-access-key"] = "${{ steps.create_clusters.outputs.backup-bucket-s3-aws-access-key }}" |
                    .["backup-bucket-s3-aws-secret-access-key"] = "${{ steps.create_clusters.outputs.backup-bucket-s3-aws-secret-access-key }}" |
                    .["backup-bucket-s3-bucket-name"] = "${{ steps.create_clusters.outputs.backup-bucket-s3-bucket-name }}"' > "${{ runner.temp }}/sensitive_values.yaml"
            - name: Export other secrets from the action
              id: export_secrets
              uses: ./.github/actions/internal-generic-encrypt-export
              with:
                  file_path: ${{ runner.temp }}/sensitive_values.yaml
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}
            - uses: cloudposse/github-action-matrix-outputs-write@ed06cf3a6bf23b8dce36d1cf0d63123885bb8375
              id: out
              with:
                  matrix-step-name: ${{ github.job }}
                  matrix-key: ${{ matrix.distro.name }}-${{ matrix.scenario.name }}
                  outputs: |-
                      kubeconfig_encrypted: ${{ steps.export_kube_config.outputs.encrypted_file_base64 }}
                      sensitive_values_raw: ${{ steps.export_secrets.outputs.encrypted_file_base64 }}
            - name: üåê Post-creation steps
              timeout-minutes: 20
              run: |
                  set -euo pipefail

                  # Here we verify the extraction of the env variables as presented in the documentation

                  cd ${{ env.TF_MODULES_DIRECTORY }}/aws/openshift/${{ matrix.scenario.name }}/terraform/clusters/
                  source ../../procedure/gather-cluster-login-id.sh
                  cd -

                  if [[ -z "$CLUSTER_1_NAME" || -z "$CLUSTER_1_API_URL" || -z "$CLUSTER_1_ADMIN_USERNAME" || -z "$CLUSTER_1_ADMIN_PASSWORD" ]]; then
                    echo "‚ùå ERROR: One or more Cluster 1 environment variables are empty."
                    exit 1
                  fi
                  if [[ -z "$CLUSTER_2_NAME" || -z "$CLUSTER_2_API_URL" || -z "$CLUSTER_2_ADMIN_USERNAME" || -z "$CLUSTER_2_ADMIN_PASSWORD" ]]; then
                    echo "‚ùå ERROR: One or more Cluster 2 environment variables are empty."
                    exit 1
                  fi


                  ./aws/openshift/${{ matrix.scenario.name }}/procedure/verify-cluster-nodes.sh
    access-info:
        name: Read kube configs from matrix
        runs-on: ubuntu-latest
        needs: prepare-clusters
        outputs:
            config: ${{ steps.read-workflow.outputs.result }}
        steps:
            - uses: cloudposse/github-action-matrix-outputs-read@33cac12fa9282a7230a418d859b93fdbc4f27b5a
              id: read-workflow
              with:
                  matrix-step-name: prepare-clusters
    integration-tests:
        name: Run integration tests - ${{ matrix.distro.name }} - ${{ matrix.scenario.name }}
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - access-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        env:
            TEST_CLUSTER_TYPE: openshift
        steps:
            - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
            - name: Install asdf tools with cache for the project
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@eb9d51b4dc89deeda7fc160166f378725ee0f06a
            - name: Install CLI tools from OpenShift Mirror
              uses: redhat-actions/openshift-tools-installer@144527c7d98999f2652264c048c7a9bd103f8a82
              with:
                  oc: ${{ matrix.distro.version }}
            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_USER;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_ID;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_SECRET;
                      secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;
                      secret/data/products/infrastructure-experience/ci/common CI_ENCRYPTION_KEY;
            - name: üîê Retrieve kubeconfig from outputs
              uses: ./.github/actions/internal-generic-decrypt-import
              with:
                  output_path: ${{ runner.temp }}/kubeconfig
                  encrypted_file_base64: >
                      ${{ fromJson(needs.access-info.outputs.config).kubeconfig_encrypted[


                        format(
                          '{0}-{1}',
                          matrix.distro.name,
                          matrix.scenario.name
                        )
                      ] }}

                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}
            - name: üîê Login into the clusters
              timeout-minutes: 2
              run: |
                  mkdir -p "$HOME/.kube"
                  mv "${{ runner.temp }}/kubeconfig" "$HOME/.kube/config"

                  oc config current-context
                  # get nodes will be performed in the next step
            - name: üè∑Ô∏è Configure clusters informations
              timeout-minutes: 10
              run: "set -euo pipefail\n\nsed -i -e 's/^export CLUSTER_1_NAME=\".*\"/export CLUSTER_1_NAME=\"${{ matrix.distro.clusterName }}-${{matrix.scenario.shortName\
                  \ }}-1\"/' \\\n       -e 's/^export CLUSTER_2_NAME=\".*\"/export CLUSTER_2_NAME=\"${{ matrix.distro.clusterName }}-${{matrix.scenario.shortName\
                  \ }}-2\"/' \\\n      generic/openshift/dual-region/procedure/set-cluster-names.sh\n\nsource ./generic/openshift/dual-region/procedure/set-cluster-names.sh\n\
                  \necho \"CLUSTER_1_NAME=$CLUSTER_1_NAME\" | tee -a \"$GITHUB_ENV\"\necho \"CLUSTER_2_NAME=$CLUSTER_2_NAME\" | tee -a \"$GITHUB_ENV\"\n\
                  \n# ensure we are logged in\noc --context=\"$CLUSTER_1_NAME\" get nodes\noc --context=\"$CLUSTER_2_NAME\" get nodes\n\necho \"\U0001F4E4\
                  \ Export all env variables in GITHUB_ENV for consistency between steps\"\nenv | while IFS= read -r line; do echo \"$line\" >> \"$GITHUB_ENV\"\
                  ; done\n"
            - name: üìÅ Get a copy of the reference architecture
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  ./aws/openshift/${{ matrix.scenario.name }}/procedure/get-your-copy.sh
                  tree
            - name: Ensure ACM and associated MultiCluster are not installed
              timeout-minutes: 30
              run: "set -euo pipefail\n\necho \"\U0001F9F9 Starting comprehensive ACM cleanup for fresh installation...\"\necho \"================================================================\"\
                  \necho \"\"\n\nMANAGEDCLUSTER_TIMEOUT=60  # Reduced timeout for ManagedCluster deletion\nCLUSTERS=(\"local-cluster\" \"$CLUSTER_2_NAME\"\
                  )\n\n# Step 1: Delete all ManagedClusters (aggressive mode)\necho \"Step 1: Deleting all ManagedClusters...\"\necho \"----------------------------------------\"\
                  \nfor cluster_name in \"${CLUSTERS[@]}\"; do\n  echo \"\U0001F5D1Ô∏è  Deleting ManagedCluster: $cluster_name\"\n\n  if oc --context \"$CLUSTER_1_NAME\"\
                  \ get managedcluster \"$cluster_name\" >/dev/null 2>&1; then\n\n    # Immediately remove finalizers (don't wait for graceful deletion\
                  \ in cleanup scenario)\n    echo \"  Removing finalizers immediately...\"\n    oc --context \"$CLUSTER_1_NAME\" patch managedcluster \"\
                  $cluster_name\" \\\n      --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' || true\n\n    # Now delete with short timeout\n    echo\
                  \ \"  Deleting resource...\"\n    oc --context \"$CLUSTER_1_NAME\" delete managedcluster \"$cluster_name\" \\\n      --timeout=30s --grace-period=0\
                  \ 2>/dev/null || true\n\n    # Verify deletion (short wait)\n    echo \"‚è≥ Verifying deletion of '$cluster_name'...\"\n    SECONDS=0\n\
                  \    while oc --context \"$CLUSTER_1_NAME\" get managedcluster \"$cluster_name\" >/dev/null 2>&1; do\n      if [ $((SECONDS % 10)) -eq\
                  \ 0 ]; then\n        echo \"  Still waiting... (${SECONDS}s elapsed)\"\n      fi\n      sleep 2\n\n      if [ \"$SECONDS\" -ge \"$MANAGEDCLUSTER_TIMEOUT\"\
                  \ ]; then\n        echo \"  ‚ö†Ô∏è  ManagedCluster still present after $MANAGEDCLUSTER_TIMEOUT seconds\"\n        echo \"  Forcing final removal...\"\
                  \n        oc --context \"$CLUSTER_1_NAME\" patch managedcluster \"$cluster_name\" \\\n          --type='json' -p='[{\"op\": \"remove\"\
                  , \"path\": \"/metadata/finalizers\"}]' 2>/dev/null || true\n        oc --context \"$CLUSTER_1_NAME\" delete managedcluster \"$cluster_name\"\
                  \ \\\n          --grace-period=0 --force 2>/dev/null || true\n        sleep 5\n        break\n      fi\n    done\n\n    if ! oc --context\
                  \ \"$CLUSTER_1_NAME\" get managedcluster \"$cluster_name\" >/dev/null 2>&1; then\n      echo \"  ‚úÖ ManagedCluster '$cluster_name' deleted\"\
                  \n    else\n      echo \"  ‚ö†Ô∏è  ManagedCluster '$cluster_name' still exists - continuing anyway\"\n    fi\n  else\n    echo \"  ‚ÑπÔ∏è  ManagedCluster\
                  \ '$cluster_name' not found - skipping\"\n  fi\n  echo \"\"\ndone\n\n# Step 2: Clean up cluster-specific namespaces on hub\necho \"Step\
                  \ 2: Cleaning up cluster namespaces on hub...\"\necho \"-------------------------------------------------\"\nfor cluster_name in \"${CLUSTERS[@]}\"\
                  ; do\n  if oc --context \"$CLUSTER_1_NAME\" get namespace \"$cluster_name\" >/dev/null 2>&1; then\n    echo \"\U0001F5D1Ô∏è  Deleting namespace:\
                  \ $cluster_name\"\n    oc --context \"$CLUSTER_1_NAME\" delete namespace \"$cluster_name\" --timeout=60s || {\n      echo \"  ‚ö†Ô∏è  Normal\
                  \ deletion failed, forcing...\"\n      # Remove finalizers from all resources in the namespace\n      oc --context \"$CLUSTER_1_NAME\"\
                  \ patch namespace \"$cluster_name\" --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' || true\n    }\n  fi\ndone\necho \"\"\n\n#\
                  \ Step 3: Clean up klusterlet resources on managed clusters\necho \"Step 3: Cleaning up klusterlet resources on managed clusters...\"\n\
                  echo \"----------------------------------------------------------------\"\n\n# Clean cluster 1 (hub, also a managed cluster as local-cluster)\n\
                  if oc --context \"$CLUSTER_1_NAME\" get namespace open-cluster-management-agent >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Cleaning\
                  \ klusterlet on cluster 1 (hub/local-cluster)\"\n  oc --context \"$CLUSTER_1_NAME\" -n open-cluster-management-agent delete all --all\
                  \ --timeout=30s || true\n  oc --context \"$CLUSTER_1_NAME\" delete namespace open-cluster-management-agent --timeout=60s || {\n    oc\
                  \ --context \"$CLUSTER_1_NAME\" patch namespace open-cluster-management-agent --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' ||\
                  \ true\n  }\nfi\n\nif oc --context \"$CLUSTER_1_NAME\" get namespace open-cluster-management-agent-addon >/dev/null 2>&1; then\n  echo\
                  \ \"\U0001F5D1Ô∏è  Cleaning klusterlet addon namespace on cluster 1\"\n  oc --context \"$CLUSTER_1_NAME\" delete namespace open-cluster-management-agent-addon\
                  \ --timeout=60s || {\n    oc --context \"$CLUSTER_1_NAME\" patch namespace open-cluster-management-agent-addon --type='merge' -p '{\"\
                  metadata\":{\"finalizers\":[]}}' || true\n  }\nfi\n\n# Clean cluster 2\nif oc --context \"$CLUSTER_2_NAME\" get namespace open-cluster-management-agent\
                  \ >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Cleaning klusterlet on cluster 2\"\n  oc --context \"$CLUSTER_2_NAME\" -n open-cluster-management-agent\
                  \ delete all --all --timeout=30s || true\n  oc --context \"$CLUSTER_2_NAME\" delete namespace open-cluster-management-agent --timeout=60s\
                  \ || {\n    oc --context \"$CLUSTER_2_NAME\" patch namespace open-cluster-management-agent --type='merge' -p '{\"metadata\":{\"finalizers\"\
                  :[]}}' || true\n  }\nfi\n\nif oc --context \"$CLUSTER_2_NAME\" get namespace open-cluster-management-agent-addon >/dev/null 2>&1; then\n\
                  \  echo \"\U0001F5D1Ô∏è  Cleaning klusterlet addon namespace on cluster 2\"\n  oc --context \"$CLUSTER_2_NAME\" delete namespace open-cluster-management-agent-addon\
                  \ --timeout=60s || {\n    oc --context \"$CLUSTER_2_NAME\" patch namespace open-cluster-management-agent-addon --type='merge' -p '{\"\
                  metadata\":{\"finalizers\":[]}}' || true\n  }\nfi\n\n# Delete klusterlet CRs if they exist\nif oc --context \"$CLUSTER_1_NAME\" get klusterlet\
                  \ >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Deleting klusterlet CRs on cluster 1\"\n  oc --context \"$CLUSTER_1_NAME\" delete klusterlet\
                  \ --all --timeout=30s || {\n    for kl in $(oc --context \"$CLUSTER_1_NAME\" get klusterlet -o name); do\n      oc --context \"$CLUSTER_1_NAME\"\
                  \ patch \"$kl\" --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' || true\n    done\n  }\nfi\n\nif oc --context \"$CLUSTER_2_NAME\"\
                  \ get klusterlet >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Deleting klusterlet CRs on cluster 2\"\n  oc --context \"$CLUSTER_2_NAME\"\
                  \ delete klusterlet --all --timeout=30s || {\n    for kl in $(oc --context \"$CLUSTER_2_NAME\" get klusterlet -o name); do\n      oc --context\
                  \ \"$CLUSTER_2_NAME\" patch \"$kl\" --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' || true\n    done\n  }\nfi\necho \"\"\n\n#\
                  \ Step 4: Delete ManagedClusterSet and related resources\necho \"Step 4: Deleting ManagedClusterSet...\"\necho \"-------------------------------------\"\
                  \nif oc --context=\"$CLUSTER_1_NAME\" get managedclusterset camunda-zeebe >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Deleting ManagedClusterSet:\
                  \ camunda-zeebe\"\n  oc --context=\"$CLUSTER_1_NAME\" delete -f ./generic/openshift/dual-region/procedure/acm/managed-cluster-set.yml\
                  \ --timeout=60s || {\n    oc --context=\"$CLUSTER_1_NAME\" patch managedclusterset camunda-zeebe --type='merge' -p '{\"metadata\":{\"\
                  finalizers\":[]}}' || true\n  }\nfi\n\nif oc --context=\"$CLUSTER_1_NAME\" get managedclustersetbinding camunda-zeebe \\\n    -n open-cluster-management\
                  \ >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Deleting ManagedClusterSetBinding: camunda-zeebe\"\n  oc --context=\"$CLUSTER_1_NAME\"\
                  \ delete managedclustersetbinding camunda-zeebe \\\n    -n open-cluster-management --timeout=60s || {\n    oc --context=\"$CLUSTER_1_NAME\"\
                  \ patch managedclustersetbinding camunda-zeebe \\\n      -n open-cluster-management --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}'\
                  \ || true\n  }\nfi\necho \"\"\n\n# Step 5: Delete MultiClusterHub (aggressive cleanup)\necho \"Step 5: Deleting MultiClusterHub...\"\n\
                  echo \"------------------------------------\"\nif oc --context=\"$CLUSTER_1_NAME\" get clusterrole open-cluster-management:cluster-manager-admin\
                  \ >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Deleting cluster role: open-cluster-management:cluster-manager-admin\"\n  oc --context=\"\
                  $CLUSTER_1_NAME\" delete clusterrole open-cluster-management:cluster-manager-admin || true\nfi\n\nif oc --context=\"$CLUSTER_1_NAME\"\
                  \ get crd multiclusterhubs.operator.open-cluster-management.io >/dev/null 2>&1; then\n  # Check if MultiClusterHub exists\n  if oc --context=\"\
                  $CLUSTER_1_NAME\" get multiclusterhub -n open-cluster-management >/dev/null 2>&1; then\n    echo \"\U0001F5D1Ô∏è  MultiClusterHub found\
                  \ - forcing immediate deletion\"\n\n    # Immediate finalizer removal (don't wait for graceful deletion)\n    echo \"  Removing finalizers\
                  \ immediately...\"\n    for mch in $(oc --context=\"$CLUSTER_1_NAME\" get multiclusterhub -n open-cluster-management -o name 2>/dev/null);\
                  \ do\n      oc --context=\"$CLUSTER_1_NAME\" patch \"$mch\" -n open-cluster-management --type='merge' -p '{\"metadata\":{\"finalizers\"\
                  :[]}}' || true\n    done\n\n    # Force delete with zero grace period\n    echo \"  Force deleting resource...\"\n    oc --context=\"\
                  $CLUSTER_1_NAME\" delete multiclusterhub --all -n open-cluster-management --grace-period=0 --force 2>/dev/null || true\n\n    # Verify\
                  \ deletion (short timeout since we forced it)\n    echo \"‚è≥ Verifying deletion...\"\n    SECONDS=0\n    while oc --context=\"$CLUSTER_1_NAME\"\
                  \ get multiclusterhub -n open-cluster-management >/dev/null 2>&1; do\n      if [ $((SECONDS % 10)) -eq 0 ]; then\n        echo \"  Still\
                  \ waiting for deletion... (${SECONDS}s elapsed)\"\n      fi\n      sleep 2\n\n      if [ $SECONDS -ge 60 ]; then\n        echo \"  ‚ö†Ô∏è\
                  \  MultiClusterHub still present after force delete - will clean namespace\"\n        break\n      fi\n    done\n    echo \"  ‚úÖ MultiClusterHub\
                  \ deletion complete\"\n  else\n    echo \"  ‚ÑπÔ∏è  MultiClusterHub not found - already deleted\"\n  fi\nelse\n  echo \"  ‚ÑπÔ∏è  MultiClusterHub\
                  \ CRD not found - skipping\"\nfi\necho \"\"\n\n# Step 6: Delete ACM Operator Subscription and CSV\necho \"Step 6: Deleting ACM Operator...\"\
                  \necho \"---------------------------------\"\nif oc --context=\"$CLUSTER_1_NAME\" get subscription advanced-cluster-management -n open-cluster-management\
                  \ >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Deleting ACM subscription\"\n  oc --context=\"$CLUSTER_1_NAME\" delete subscription advanced-cluster-management\
                  \ -n open-cluster-management --timeout=30s || true\nfi\n\n# Delete ClusterServiceVersion (CSV) for ACM\nif oc --context=\"$CLUSTER_1_NAME\"\
                  \ get csv -n open-cluster-management \\\n    | grep -q advanced-cluster-management; then\n  echo \"\U0001F5D1Ô∏è  Deleting ACM ClusterServiceVersion\"\
                  \n  oc --context=\"$CLUSTER_1_NAME\" delete csv -n open-cluster-management \\\n    -l operators.coreos.com/advanced-cluster-management.open-cluster-management=\
                  \ \\\n    --timeout=30s || true\nfi\necho \"\"\n\n# Step 7: Delete MultiClusterEngine (aggressive cleanup)\necho \"Step 7: Deleting MultiClusterEngine...\"\
                  \necho \"---------------------------------------\"\nif oc --context=\"$CLUSTER_1_NAME\" get crd multiclusterengines.multicluster.openshift.io\
                  \ >/dev/null 2>&1; then\n  # Check if MultiClusterEngine exists\n  if oc --context=\"$CLUSTER_1_NAME\" get multiclusterengine -n open-cluster-management\
                  \ >/dev/null 2>&1; then\n    echo \"\U0001F5D1Ô∏è  MultiClusterEngine found - forcing immediate deletion\"\n\n    # Immediate finalizer\
                  \ removal\n    echo \"  Removing finalizers immediately...\"\n    for mce in $(oc --context=\"$CLUSTER_1_NAME\" get multiclusterengine\
                  \ -n open-cluster-management -o name 2>/dev/null); do\n      oc --context=\"$CLUSTER_1_NAME\" patch \"$mce\" -n open-cluster-management\
                  \ --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' || true\n    done\n\n    # Force delete with zero grace period\n    echo \" \
                  \ Force deleting resource...\"\n    oc --context=\"$CLUSTER_1_NAME\" delete multiclusterengine --all -n open-cluster-management --grace-period=0\
                  \ --force 2>/dev/null || true\n\n    # Verify deletion (short timeout)\n    echo \"‚è≥ Verifying deletion...\"\n    SECONDS=0\n    while\
                  \ oc --context=\"$CLUSTER_1_NAME\" get multiclusterengine -n open-cluster-management >/dev/null 2>&1; do\n      if [ $((SECONDS % 10))\
                  \ -eq 0 ]; then\n        echo \"  Still waiting for deletion... (${SECONDS}s elapsed)\"\n      fi\n      sleep 2\n\n      if [ \"$SECONDS\"\
                  \ -ge 60 ]; then\n        echo \"  ‚ö†Ô∏è  MultiClusterEngine still present after force delete - will clean namespace\"\n        break\n \
                  \     fi\n    done\n    echo \"  ‚úÖ MultiClusterEngine deletion complete\"\n  else\n    echo \"  ‚ÑπÔ∏è  MultiClusterEngine not found - already\
                  \ deleted\"\n  fi\nelse\n  echo \"  ‚ÑπÔ∏è  MultiClusterEngine CRD not found - skipping\"\nfi\necho \"\"\n\n# Step 8: Delete open-cluster-management\
                  \ namespace (with thorough verification)\necho \"Step 8: Deleting open-cluster-management namespace...\"\necho \"------------------------------------------------------\"\
                  \nif oc --context=\"$CLUSTER_1_NAME\" get namespace open-cluster-management >/dev/null 2>&1; then\n  echo \"\U0001F5D1Ô∏è  Initiating deletion\
                  \ of namespace: open-cluster-management\"\n\n  # First, try graceful deletion\n  oc --context=\"$CLUSTER_1_NAME\" delete namespace open-cluster-management\
                  \ --timeout=120s || {\n    echo \"  ‚ö†Ô∏è  Graceful deletion timed out, investigating...\"\n\n    # Check namespace status\n    ns_status=$(oc\
                  \ --context=\"$CLUSTER_1_NAME\" get namespace open-cluster-management -o jsonpath='{.status.phase}' 2>/dev/null || echo \"NotFound\")\n\
                  \    echo \"  Namespace status: $ns_status\"\n\n    if [ \"$ns_status\" = \"Terminating\" ]; then\n      echo \"  Namespace stuck in Terminating\
                  \ state, removing finalizers...\"\n\n      # Remove finalizers from all resources in the namespace\n      echo \"  Removing finalizers\
                  \ from all pods...\"\n      for pod in $(oc --context=\"$CLUSTER_1_NAME\" get pods -n open-cluster-management -o name 2>/dev/null); do\n\
                  \        oc --context=\"$CLUSTER_1_NAME\" patch \"$pod\" -n open-cluster-management --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}'\
                  \ 2>/dev/null || true\n      done\n\n      echo \"  Removing finalizers from all deployments...\"\n      for deploy in $(oc --context=\"\
                  $CLUSTER_1_NAME\" get deployments -n open-cluster-management -o name 2>/dev/null); do\n        oc --context=\"$CLUSTER_1_NAME\" patch\
                  \ \"$deploy\" -n open-cluster-management --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' 2>/dev/null || true\n      done\n\n  \
                  \    # Finally remove namespace finalizers\n      echo \"  Removing namespace finalizers...\"\n      oc --context=\"$CLUSTER_1_NAME\"\
                  \ patch namespace open-cluster-management --type='merge' -p '{\"metadata\":{\"finalizers\":[]}}' || true\n\n      sleep 5\n    fi\n  }\n\
                  \n  # Wait for namespace to be completely gone\n  echo \"‚è≥ Waiting for namespace to be completely deleted...\"\n  SECONDS=0\n  while oc\
                  \ --context=\"$CLUSTER_1_NAME\" get namespace open-cluster-management >/dev/null 2>&1; do\n    if [ $((SECONDS % 15)) -eq 0 ]; then\n\
                  \      ns_phase=$(oc --context=\"$CLUSTER_1_NAME\" get namespace open-cluster-management -o jsonpath='{.status.phase}' 2>/dev/null ||\
                  \ echo \"Deleted\")\n      echo \"  Still waiting... namespace phase: $ns_phase (${SECONDS}s elapsed)\"\n\n      # Show what's preventing\
                  \ deletion\n      if [ $((SECONDS % 30)) -eq 0 ]; then\n        echo \"  Resources still in namespace:\"\n        oc --context=\"$CLUSTER_1_NAME\"\
                  \ api-resources --verbs=list --namespaced -o name 2>/dev/null | \\\n          xargs -n 1 oc --context=\"$CLUSTER_1_NAME\" get -n open-cluster-management\
                  \ --ignore-not-found --show-kind 2>/dev/null | \\\n          head -20 || true\n      fi\n    fi\n    sleep 3\n\n    if [ $SECONDS -ge\
                  \ 180 ]; then\n      echo \"  ‚õî Timeout after 3 minutes - forcing final removal\"\n      # Last resort: remove all finalizers from namespace\n\
                  \      oc --context=\"$CLUSTER_1_NAME\" get namespace open-cluster-management -o json 2>/dev/null | \\\n        jq '.metadata.finalizers\
                  \ = []' | \\\n        oc --context=\"$CLUSTER_1_NAME\" replace --raw \"/api/v1/namespaces/open-cluster-management/finalize\" -f - || true\n\
                  \      sleep 10\n      break\n    fi\n  done\n\n  # Final verification\n  if ! oc --context=\"$CLUSTER_1_NAME\" get namespace open-cluster-management\
                  \ >/dev/null 2>&1; then\n    echo \"  ‚úÖ Namespace completely deleted\"\n  else\n    echo \"  ‚ö†Ô∏è  WARNING: Namespace still exists after\
                  \ cleanup attempts!\"\n    echo \"  This may cause ACM installation to fail. Manual intervention may be required.\"\n    oc --context=\"\
                  $CLUSTER_1_NAME\" get namespace open-cluster-management -o yaml || true\n  fi\nelse\n  echo \"  ‚ÑπÔ∏è  Namespace not found - already deleted\"\
                  \nfi\necho \"\"\n\n# Step 9: Clean up remaining ACM CRDs and cluster-scoped resources\necho \"Step 9: Cleaning up ACM CRDs and cluster\
                  \ resources...\"\necho \"------------------------------------------------------\"\necho \"\U0001F5D1Ô∏è  Removing ACM-related ClusterRoles\
                  \ and ClusterRoleBindings\"\noc --context=\"$CLUSTER_1_NAME\" delete clusterrole -l open-cluster-management.io/aggregate-to-work= --timeout=30s\
                  \ 2>/dev/null || true\noc --context=\"$CLUSTER_1_NAME\" delete clusterrolebinding -l open-cluster-management.io/aggregate-to-work= --timeout=30s\
                  \ 2>/dev/null || true\n\necho \"\U0001F5D1Ô∏è  Removing webhook configurations\"\noc --context=\"$CLUSTER_1_NAME\" delete validatingwebhookconfiguration\
                  \ -l app=multicluster-operators-subscription --timeout=30s 2>/dev/null || true\noc --context=\"$CLUSTER_1_NAME\" delete mutatingwebhookconfiguration\
                  \ -l app=multicluster-operators-subscription --timeout=30s 2>/dev/null || true\necho \"\"\n\necho \"================================================================\"\
                  \necho \"‚úÖ ACM cleanup completed - system ready for fresh installation\"\necho \"================================================================\"\
                  \n"
            - name: üö¢ Configure ACM
              timeout-minutes: 30
              env:
                  RHCS_TOKEN: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
              run: |
                  set -euo pipefail

                  cd ./generic/openshift/dual-region/procedure/acm/

                  echo "Install ACM"
                  if ! ./install-acm.sh 2>&1; then
                    echo "ACM installation failed."
                    exit 1
                  fi

                  ./verify-acm.sh

                  echo "Configure MultiClusterHub"
                  if ! ./install-multi-cluster-hub.sh 2>&1; then
                    echo "MultiClusterHub installation failed."
                    exit 1
                  fi

                  ./verify-multi-cluster-hub.sh

                  echo "Configure ManagedClusterSet"
                  if ! ./install-managed-cluster-set.sh 2>&1; then
                    echo "ManagedClusterSet installation failed."
                    exit 1
                  fi

                  # Initial verification (should be quick)
                  timeout 60 ./verify-managed-cluster-set.sh || echo "Initial verification timeout - continuing with import"

                  echo "Import Clusters in the ManagedClusterSet"
                  if ! ./initiate-cluster-set.sh 2>&1; then
                    echo "Cluster import to ManagedClusterSet failed."
                    echo ""
                    echo "Running diagnostics..."
                    ./debug-managed-cluster.sh local-cluster || true
                    ./debug-managed-cluster.sh "$CLUSTER_2_NAME" || true
                    exit 1
                  fi

                  echo "Verify all clusters are healthy (with timeout and retry logic)"
                  ./verify-managed-cluster-set.sh
            - name: üê† Configure Submariner
              timeout-minutes: 30
              run: |
                  set -euo pipefail

                  echo "Configure Submariner"
                  cd ./generic/openshift/dual-region/procedure/submariner/

                  echo "Running pre-flight checks..."
                  if ! ./preflight-check.sh; then
                    echo "‚ö†Ô∏è  Pre-flight checks failed!"
                    echo "Attempting to wait for clusters to become healthy..."

                    cd ../acm/
                    if ! ./verify-managed-cluster-set.sh; then
                      echo "‚ùå Clusters are not healthy. Cannot proceed with Submariner installation."
                      echo ""
                      echo "Running diagnostics..."
                      ./debug-managed-cluster.sh local-cluster || true
                      ./debug-managed-cluster.sh "$CLUSTER_2_NAME" || true
                      exit 1
                    fi
                    cd ../submariner/

                    echo "Re-running pre-flight checks after waiting for clusters..."
                    if ! ./preflight-check.sh; then
                      echo "‚ùå Pre-flight checks still failing. Aborting Submariner installation."
                      exit 1
                    fi
                  fi

                  echo "Listing available broker nodes..."
                  ./list-nodes-brokers.sh

                  echo "Labeling broker nodes..."
                  ./label-nodes-brokers.sh

                  echo "Verifying node labels..."
                  ./list-nodes-brokers.sh

                  echo "Installing Submariner..."
                  ./install-submariner.sh

                  echo "Verifying Submariner installation..."
                  ./verify-submariner.sh

                  echo "Installing subctl CLI..."
                  source ./install-subctl.sh
                  echo "PATH=$PATH" | tee -a "$GITHUB_ENV"

                  echo "Verifying subctl..."
                  ./verify-subctl.sh
            - name: üèóÔ∏è Retrieve exported Environment values from outputs
              uses: ./.github/actions/internal-generic-decrypt-import
              with:
                  output_path: ${{ runner.temp }}/sensitive.yaml
                  encrypted_file_base64: >
                      ${{ fromJson(needs.access-info.outputs.config).sensitive_values_raw[


                        format(
                          '{0}-{1}',
                          matrix.distro.name,
                          matrix.scenario.name
                        )
                      ] }}

                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}
            - name: üèóÔ∏è Prepare the environment for the deployment
              timeout-minutes: 20
              run: "set -euo pipefail\n\necho \"Load outputs values from creation of the clusters\"\n\n# shellcheck disable=SC2086\nexport BACKUP_BUCKET_S3_AWS_ACCESS_KEY=$(yq\
                  \ eval '.[\"backup-bucket-s3-aws-access-key\"]' \"${{ runner.temp }}/sensitive.yaml\")\n# protect sensitive values\necho \"::add-mask::$BACKUP_BUCKET_S3_AWS_ACCESS_KEY\"\
                  \n# shellcheck disable=SC2086\nexport BACKUP_BUCKET_S3_AWS_SECRET_ACCESS_KEY=$(yq eval '.[\"backup-bucket-s3-aws-secret-access-key\"]'\
                  \ \"${{ runner.temp }}/sensitive.yaml\")\n# protect sensitive values\necho \"::add-mask::$BACKUP_BUCKET_S3_AWS_SECRET_ACCESS_KEY\"\n#\
                  \ shellcheck disable=SC2086\nexport BACKUP_BUCKET_S3_BUCKET_NAME=$(yq eval '.[\"backup-bucket-s3-bucket-name\"]' \"${{ runner.temp }}/sensitive.yaml\"\
                  )\n\necho \"Setup environment values\"\nsed -i \\\n  -e \"s#^export CLUSTER_1_NAME=\\\".*\\\"#export CLUSTER_1_NAME=\\\"${{ matrix.distro.clusterName\
                  \ }}-${{matrix.scenario.shortName }}-1\\\"#\" \\\n  -e \"s#^export CLUSTER_2_NAME=\\\".*\\\"#export CLUSTER_2_NAME=\\\"${{ matrix.distro.clusterName\
                  \ }}-${{matrix.scenario.shortName }}-2\\\"#\" \\\n  -e \"s#^export CLUSTER_1_REGION=\\\".*\\\"#export CLUSTER_1_REGION=\\\"${{ env.CLUSTER_1_AWS_REGION\
                  \ }}\\\"#\" \\\n  -e \"s#^export CLUSTER_2_REGION=\\\".*\\\"#export CLUSTER_2_REGION=\\\"${{ env.CLUSTER_2_AWS_REGION }}\\\"#\" \\\n \
                  \ -e \"s#^export AWS_ACCESS_KEY_ES=\\\".*\\\"#export AWS_ACCESS_KEY_ES=\\\"${BACKUP_BUCKET_S3_AWS_ACCESS_KEY}\\\"#\" \\\n  -e \"s#^export\
                  \ AWS_SECRET_ACCESS_KEY_ES=\\\".*\\\"#export AWS_SECRET_ACCESS_KEY_ES=\\\"${BACKUP_BUCKET_S3_AWS_SECRET_ACCESS_KEY}\\\"#\" \\\n  -e \"\
                  s#^export AWS_ES_BUCKET_NAME=\\\".*\\\"#export AWS_ES_BUCKET_NAME=\\\"${BACKUP_BUCKET_S3_BUCKET_NAME}\\\"#\" \\\n  -e \"s#^export AWS_ES_BUCKET_REGION=\\\
                  \".*\\\"#export AWS_ES_BUCKET_REGION=\\\"${{ env.CLUSTER_1_AWS_REGION }}\\\"#\" \\\n  generic/openshift/dual-region/procedure/chart-env.sh\n\
                  \n# the chart env should be loaded by the client at the very first step of his installation\nsource .github/scripts/gha-functions.sh\n\
                  export_new_env_vars ./generic/openshift/dual-region/procedure/chart-env.sh\n\necho \"Delete the namespaces to ensure a fresh start\"\n\
                  if kubectl --context \"$CLUSTER_1_NAME\" get namespace \"$CAMUNDA_NAMESPACE_1\" &>/dev/null; then\n  kubectl --context \"$CLUSTER_1_NAME\"\
                  \ delete namespace \"$CAMUNDA_NAMESPACE_1\" --wait\n  while kubectl --context \"$CLUSTER_1_NAME\" get namespace \"$CAMUNDA_NAMESPACE_1\"\
                  \ &>/dev/null; do\n    echo \"Namespace $CAMUNDA_NAMESPACE_1 still being deleted, waiting...\"\n    sleep 5\n  done\nfi\n\nif kubectl\
                  \ --context \"$CLUSTER_2_NAME\" get namespace \"$CAMUNDA_NAMESPACE_2\" &>/dev/null; then\n  kubectl --context \"$CLUSTER_2_NAME\" delete\
                  \ namespace \"$CAMUNDA_NAMESPACE_2\" --wait\n  while kubectl --context \"$CLUSTER_2_NAME\" get namespace \"$CAMUNDA_NAMESPACE_2\" &>/dev/null;\
                  \ do\n    echo \"Namespace $CAMUNDA_NAMESPACE_2 still being deleted, waiting...\"\n    sleep 5\n  done\nfi\n\necho \"Setup namespaces\
                  \ and initial secrets\"\nsource ./generic/openshift/dual-region/procedure/setup-namespaces-secrets.sh\n\necho \"Generate dual-region variables\"\
                  \nCLUSTER_1_NAME='local-cluster' ZEEBE_CLUSTER_SIZE=8 source ./generic/openshift/dual-region/procedure/generate-zeebe-helm-values.sh\n\
                  \necho \"\U0001F4E4 Export all env variables in GITHUB_ENV for consistency between steps\"\nenv | while IFS= read -r line; do echo \"\
                  $line\" >> \"$GITHUB_ENV\"; done\n"
            - name: üõ†Ô∏è Assemble deployment values of generic/openshift/dual-region
              timeout-minutes: 10
              run: |
                  set -o errexit
                  set -euo pipefail

                  echo "Construct the values.yml file for each cluster"

                  # TODO: Align with AKS deployment to use values-enterprise.yaml as base
                  # and apply registry-harbor.yml for all deployments (not just tests).
                  # This should be documented in the procedure for production deployments.
                  # See: azure_kubernetes_aks_single_region_tests.yml for reference implementation.
                  cp -f generic/openshift/dual-region/helm-values/values-base.yml ./values.yml

                  echo "Region 1:"
                  yq '. *+ load("generic/openshift/dual-region/helm-values/values-region-1.yml")' values.yml > values-region-1-result.yml
                  cat values-region-1-result.yml && mv values-region-1-result.yml values-region-1.yml

                  echo "Region 2:"
                  yq '. *+ load("generic/openshift/dual-region/helm-values/values-region-2.yml")' values.yml > values-region-2-result.yml
                  cat values-region-2-result.yml && mv values-region-2-result.yml values-region-2.yml

                  # Add integration tests values
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    echo "Enabling test files"
                    for file in registry.yml; do

                      echo "Region 1:"
                      yq ". *+ load(\"generic/kubernetes/dual-region/tests/helm-values/$file\")" values-region-1.yml > values-region-1-result.yml
                      cat values-region-1-result.yml && mv values-region-1-result.yml values-region-1.yml

                      echo "Region 2:"
                      yq ". *+ load(\"generic/kubernetes/dual-region/tests/helm-values/$file\")" values-region-2.yml > values-region-2-result.yml
                      cat values-region-2-result.yml && mv values-region-2-result.yml values-region-2.yml

                    done
                  fi

                  ./generic/openshift/dual-region/procedure/assemble-envsubst-values.sh
            - name: üèÅ Install Camunda 8 using the generic/openshift helm chart procedure
              timeout-minutes: 30
              run: |
                  set -euo pipefail

                  # Generate tests objects
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    # Create the pull secrets described in generic/kubernetes/dual-region/tests/helm-values/registry.yml
                    kubectl create secret docker-registry index-docker-io \
                        --docker-server=index.docker.io \
                        --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                        --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                        --context="$CLUSTER_1_NAME" \
                        --namespace="$CAMUNDA_NAMESPACE_1"

                    kubectl create secret docker-registry index-docker-io \
                        --docker-server=index.docker.io \
                        --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                        --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                        --context="$CLUSTER_2_NAME" \
                        --namespace="$CAMUNDA_NAMESPACE_2"
                  fi

                  ./generic/openshift/dual-region/procedure/install-chart.sh
                  echo "Waiting for the manifests to be applied"
                  sleep 30

                  ./generic/openshift/dual-region/procedure/export-services-submariner.sh
                  ./generic/openshift/dual-region/procedure/verify-exported-services.sh
            - name: üëÄ‚è≥ Wait for the deployment to be healthy using generic/openshift/dual-region
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  ./generic/openshift/dual-region/procedure/check-deployment-ready.sh
            - name: Set current Camunda version
              id: camunda-version
              run: |
                  set -euo pipefail

                  CAMUNDA_VERSION=$(cat .camunda-version)
                  echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"
            - name: üß™ Run Helm Chart tests
              if: env.TESTS_ENABLED == 'true'
              timeout-minutes: 60
              uses: ./.github/actions/internal-camunda-chart-tests
              with:
                  secrets: ${{ toJSON(secrets) }}
                  camunda-version: ${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}
                  tests-camunda-helm-chart-repo-ref: ${{ env.TESTS_CAMUNDA_HELM_CHART_REPO_REF }}
                  tests-camunda-helm-chart-repo-path: ${{ env.TESTS_CAMUNDA_HELM_CHART_REPO_PATH }}
                  test-cluster-type: ${{ env.TEST_CLUSTER_TYPE }}
                  enable-helm-chart-tests: 'false'
                  zeebe-authenticated: 'false'
                  zeebe-topology-check-script: ./generic/kubernetes/dual-region/procedure/check-zeebe-cluster-topology.sh
                  zeebe-topology-golden-file: ./generic/kubernetes/dual-region/procedure/check-zeebe-cluster-topology-output.json
                  camunda-namespace-1: ${{ env.CAMUNDA_NAMESPACE_1 }}
                  cluster-2-name: ${{ env.CLUSTER_2_NAME }}
                  camunda-namespace-2: ${{ env.CAMUNDA_NAMESPACE_2 }}
                  test-release-name: ${{ env.CAMUNDA_RELEASE_NAME }}
                  test-client-id: ${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_ID }}
                  test-client-secret: ${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_SECRET }}
            - name: üî¨üö® Get failed Pods info - Cluster 1
              if: failure()
              run: |
                  set -euo pipefail

                  echo "Debug MCH installation issues"
                  # https://docs.redhat.com/en/documentation/red_hat_advanced_cluster_management_for_kubernetes/2.13/html/release_notes/acm-release-notes#upgrade-stuck
                  oc --context="$CLUSTER_1_NAME" get multiclusterhubs -n open-cluster-management -o yaml

                  kubectl --context="$CLUSTER_1_NAME" -n "$CAMUNDA_NAMESPACE_1" get po
                  kubectl --context="$CLUSTER_1_NAME" -n "$CAMUNDA_NAMESPACE_1" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n### Failed Pod: ${pod_name} ###\n"
                    kubectl --context="$CLUSTER_1_NAME" -n "$CAMUNDA_NAMESPACE_1" describe po "$pod_name"
                    kubectl --context="$CLUSTER_1_NAME" -n "$CAMUNDA_NAMESPACE_1" logs "$pod_name"
                  done
            - name: üî¨üö® Get failed Pods info - Cluster 2
              if: failure()
              run: |
                  set -euo pipefail

                  kubectl --context="$CLUSTER_2_NAME" -n "$CAMUNDA_NAMESPACE_2" get po
                  kubectl -n "$CAMUNDA_NAMESPACE_2" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n### Failed Pod: ${pod_name} ###\n"
                    kubectl --context="$CLUSTER_2_NAME" -n "$CAMUNDA_NAMESPACE_2" describe po "$pod_name"
                    kubectl --context="$CLUSTER_2_NAME" -n "$CAMUNDA_NAMESPACE_2" logs "$pod_name"
                  done
    cleanup-clusters:
        name: Cleanup ROSA clusters
        if: always()
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - integration-tests
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        steps:
            - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  fetch-depth: 0
            - name: Install asdf tools with cache
              if: env.CLEANUP_CLUSTERS == 'true'
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@eb9d51b4dc89deeda7fc160166f378725ee0f06a
            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common RH_OPENSHIFT_TOKEN;
            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.CLUSTER_1_AWS_REGION }}
            - name: Set current target branch
              if: env.CLEANUP_CLUSTERS == 'true'
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"
            - name: Delete on-demand ROSA HCP Cluster
              uses: ./.github/actions/aws-generic-terraform-cleanup
              if: always() && env.CLEANUP_CLUSTERS == 'true'
              timeout-minutes: 125
              env:
                  RHCS_TOKEN: ${{ steps.secrets.outputs.RH_OPENSHIFT_TOKEN }}
              with:
                  tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  max-age-hours: 0
                  target: ${{ matrix.distro.clusterName }}-${{matrix.scenario.shortName }}
                  tf-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/
                  openshift: 'true'
                  delete-ghost-rosa-clusters: 'true'
                  modules-order: backup_bucket,peering,clusters
    integration-tests-retry:
        name: Retry Tests in case of failure after cleanup
        if: failure() && fromJSON(github.run_attempt) < 3
        runs-on: ubuntu-latest
        needs:
            - integration-tests
            - cleanup-clusters
        steps:
            - name: Retrigger job
              uses: camunda/infra-global-github-actions/rerun-failed-run@128beef322b89d51a416832f3e26e6b17451c8a9
              with:
                  error-messages: ''
                  rerun-whole-workflow: 'true'
                  run-id: ${{ github.run_id }}
                  repository: ${{ github.repository }}
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - integration-tests
            - cleanup-clusters
        steps:
            - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label
    report-failure:
        name: Report failures
        if: failure() && (needs.integration-tests-retry.result != 'success' || fromJSON(github.run_attempt) >= 3)
        runs-on: ubuntu-latest
        needs:
            - integration-tests-retry
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@eb9d51b4dc89deeda7fc160166f378725ee0f06a
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
