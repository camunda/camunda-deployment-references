---
name: Tests - Integration - Teleport - AWS Kubernetes EKS Dual Region

permissions:
    contents: read # don't allow commits
    id-token: write # for Teleport authentication
    pull-requests: write # allow comments, labels (used by internal-apply-skip-label)

on:
    schedule:
        - cron: 0 2 * * 1 # Every Monday at 02:00 UTC
    workflow_dispatch:
        inputs:
            delete_clusters:
                description: Whether to delete the Zeebe clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true
    pull_request:
        # For now limit automatic execution to a minimum, can always be done manually via workflow_dispatch for a branch
        paths:
            - .github/workflows/aws_kubernetes_eks_dual_region_teleport_tests.yml
            - aws/kubernetes/eks-dual-region/helm-values/**
            - aws/kubernetes/eks-dual-region/procedure/**
            - aws/kubernetes/eks-dual-region/test/**
            - .tools-versions
            - .github/actions/internal-multi-region-tests/**

# limit to a single execution per actor of this workflow
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    # we don't cancel the previous run, so it can finish it and let clusters in a proper state
    cancel-in-progress: false

env:
    IS_SCHEDULE: ${{ (contains(github.head_ref, 'schedules/') || github.event_name == 'schedule') && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    AWS_PROFILE: infraex
    AWS_REGION: eu-west-2

    LABELS: camunda.cloud/ephemeral=true
    ANNOTATIONS: janitor/ttl=2h

    KUBECONFIG: ./kubeconfig

    CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}
    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}
    BACKUP_BUCKET: tests-c8-multi-region-es-eu-central-1 # static shared bucket for all Teleport tests
    ZEEBE_CLUSTER_SIZE: 8 # default size of Zeebe cluster for tests - used by scripts
    CAMUNDA_RELEASE_NAME: camunda
    CLUSTER_NAME: camunda-ci-eks
    CLUSTER_0_NAME: camunda.teleport.sh-camunda-ci-eks
    CLUSTER_1_NAME: camunda.teleport.sh-camunda-ci-eks
    CLUSTER_1_NAMESPACE: c8-multiregion-test-cluster-1
    CLUSTER_0_NAMESPACE: c8-multiregion-test-cluster-0
    NAMESPACE_PREFIX: infraex-
    TELEPORT: true

    # renovate: datasource=helm depName=camunda-platform versioning=regex:^14(\.(?<minor>\d+))?(\.(?<patch>\d+))?$ registryUrl=https://helm.camunda.io
    CAMUNDA_HELM_CHART_VERSION: 0.0.0-snapshot-alpha
    # TODO: [release-duty] before the release, update this!
    # TODO: [release-duty] adjust renovate comment to bump the major version

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    integration-tests:
        runs-on: ubuntu-latest
        if: needs.triage.outputs.should_skip == 'false'
        needs:
            - triage
        steps:
            - name: Checkout repository
              uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6
            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@193a21e1e56c9a65517a822224ac3b4ffa4d6ae4 # 1.5.9
            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}
            - name: Set up Teleport
              uses: teleport-actions/setup@a820ebbf1bc1a496efca348ad21042d6e8df73a6 # v1
              with:
                  # renovate: datasource=custom.teleport-camunda depName=teleport
                  version: 18.6.8
            - name: Authenticate with Teleport
              uses: teleport-actions/auth-k8s@d45dc8f8d215c23519c9d87095ac5de98796bfdc # v2
              with:
                  proxy: camunda.teleport.sh:443
                  token: infra-ci-prod-github-action-infraex
                  kubernetes-cluster: camunda-ci-eks
                  certificate-ttl: 3h # Set a TTL for the certificate matching the job duration (worst case)
            - name: Write kubeconfig file
              id: write-kubeconfig
              working-directory: ./aws/kubernetes/eks-dual-region/test
              run: |
                  kubectl config view --raw > ./kubeconfig
            - name: Generate values for namespaces with prefix and random suffix
              id: update-namespaces
              run: |
                  set -euxo pipefail

                  RANDOM_ID="$(openssl rand -hex 3)"

                  BACKUP_NAME=${RANDOM_ID}
                  CLUSTER_1_NAMESPACE="${NAMESPACE_PREFIX}${CLUSTER_1_NAMESPACE}-${RANDOM_ID}"
                  CLUSTER_0_NAMESPACE="${NAMESPACE_PREFIX}${CLUSTER_0_NAMESPACE}-${RANDOM_ID}"

                  # Write the updated values to the GitHub Actions environment for subsequent steps.
                  {
                    echo "BACKUP_NAME=${BACKUP_NAME}"
                    echo "CLUSTER_1_NAMESPACE=${CLUSTER_1_NAMESPACE}"
                    echo "CLUSTER_0_NAMESPACE=${CLUSTER_0_NAMESPACE}"
                    echo "CAMUNDA_NAMESPACE_0=${CLUSTER_0_NAMESPACE}"
                    echo "CAMUNDA_NAMESPACE_1=${CLUSTER_1_NAMESPACE}"
                  } | tee -a "$GITHUB_ENV" "$GITHUB_OUTPUT"

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY | S3_BACKUP_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY | S3_BACKUP_SECRET_KEY;

            - name: Create namespaces and secrets
              id: create-namespaces
              env:
                  AWS_SECRET_ACCESS_KEY_ES: ${{ steps.secrets.outputs.S3_BACKUP_SECRET_KEY }}
                  AWS_ACCESS_KEY_ES: ${{ steps.secrets.outputs.S3_BACKUP_ACCESS_KEY }}
              working-directory: ./aws/kubernetes/eks-dual-region/test
              run: |
                  set -euo pipefail

                  go test --count=1 -v -timeout 9m -failfast -run TestClusterPrerequisites

            - name: Label and annotate namespaces and secrets
              working-directory: ./aws/kubernetes/eks-dual-region/test
              run: |
                  set -euo pipefail

                  export LABELS="${{ env.LABELS }}"
                  export ANNOTATIONS="${{ env.ANNOTATIONS }}"

                  apply_labels_and_annotations() {
                    local namespace="$1"

                    # Apply labels to namespace
                    if [ -n "${LABELS:-}" ]; then
                      kubectl label namespace "$namespace" "${LABELS}" --overwrite
                    fi

                    # Apply annotations to namespace
                    if [ -n "${ANNOTATIONS:-}" ]; then
                      for ann in ${ANNOTATIONS}; do
                        kubectl annotate namespace "$namespace" "$ann" --overwrite
                      done
                    fi

                    # Apply labels and annotations to all secrets in the namespace
                    local secrets
                    secrets="$(kubectl get secrets -n "$namespace" -o jsonpath='{.items[*].metadata.name}')"
                    for secret in $secrets; do
                      if [ -n "${LABELS:-}" ]; then
                        kubectl label secret "$secret" -n "$namespace" "${LABELS}" --overwrite
                      fi
                      if [ -n "${ANNOTATIONS:-}" ]; then
                        for ann in ${ANNOTATIONS}; do
                          kubectl annotate secret "$secret" -n "$namespace" "$ann" --overwrite
                        done
                      fi
                    done
                  }

                  # Combine cluster-0 and cluster-1 arrays into one comma-separated list
                  IFS=',' read -r -a ALL_NAMESPACES <<< "${CLUSTER_0_NAMESPACE},${CLUSTER_1_NAMESPACE}"
                  for namespace in "${ALL_NAMESPACES[@]}"; do
                    apply_labels_and_annotations "$namespace"
                  done
            - name: Run Multi-region tests
              if: env.TESTS_ENABLED == 'true'
              timeout-minutes: 60
              uses: ./.github/actions/internal-multi-region-tests
              with:
                  cluster-0-name: ${{ env.CLUSTER_0_NAME }}
                  cluster-1-name: ${{ env.CLUSTER_1_NAME }}
                  cluster-0-namespace: ${{ env.CLUSTER_0_NAMESPACE }}
                  cluster-1-namespace: ${{ env.CLUSTER_1_NAMESPACE }}
                  helm-version: ${{ env.CAMUNDA_HELM_CHART_VERSION }}
                  backup-name: ${{ env.BACKUP_NAME }}
                  backup-bucket: ${{ env.BACKUP_BUCKET }}
                  default-values-yaml: ${{ github.workspace }}/aws/kubernetes/eks-dual-region/helm-values/camunda-values.yml
                  region-0-values-yaml: ${{ github.workspace }}/aws/kubernetes/eks-dual-region/helm-values/region0/camunda-values.yml
                  region-1-values-yaml: ${{ github.workspace }}/aws/kubernetes/eks-dual-region/helm-values/region1/camunda-values.yml
                  skip-cleanup: 'true'

            - name: Delete namespaces
              if: always() && env.CLEANUP_CLUSTERS == 'true'
              working-directory: ./aws/kubernetes/eks-dual-region/test
              run: |
                  set -euo pipefail

                  kubectl delete namespace "${CLUSTER_0_NAMESPACE}" || true
                  kubectl delete namespace "${CLUSTER_1_NAMESPACE}" || true

    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - integration-tests
        steps:
            - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6

            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label

    report-failure:
        name: Report failures
        if: failure()
        runs-on: ubuntu-latest
        needs:
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@193a21e1e56c9a65517a822224ac3b4ffa4d6ae4 # 1.5.9
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
