---
name: Tests - Integration - Generic Migration from Bitnami

permissions:
    contents: read
    pull-requests: write

on:
    schedule:
        - cron: 0 3 * * 2  # Runs at 3 AM on Tuesday (after operator-based tests on Monday)
    pull_request:
        paths:
            - .github/workflows/generic_kubernetes_migration_from_bitnami_test.yml
            - .github/workflows-config/generic-kubernetes-migration-from-bitnami/test_matrix.yml
            - .tool-versions
            - generic/kubernetes/migration-from-bitnami/**
            - aws/kubernetes/eks-single-region-irsa/**
            - '!aws/kubernetes/eks-single-region-irsa/test/golden/**'
            - .github/actions/aws-kubernetes-eks-single-region-create/**
            - .github/actions/aws-generic-terraform-cleanup/**
            - .github/actions/aws-configure-cli/**
            - .github/actions/internal-apply-skip-label/**
            - .github/actions/internal-generic-encrypt-export/**
            - .github/actions/internal-generic-decrypt-import/**
            - .github/actions/internal-tests-matrix/**
            - .github/actions/internal-clean-namespace/**

    workflow_dispatch:
        inputs:
            cluster_name:
                description: Cluster name.
                required: false
                type: string
            delete_clusters:
                description: Whether to delete the clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true
            migration_component:
                description: |
                    Which component to test migration for.
                    Valid values are 'full', 'orchestration', 'identity', 'keycloak', 'webmodeler'.
                required: false
                type: string
                default: full

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: false

env:
    IS_SCHEDULE: ${{ contains(github.head_ref, 'schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    AWS_PROFILE: infex
    AWS_REGION: eu-west-2
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BUCKET_REGION: eu-central-1
    TF_MODULES_DIRECTORY: ./.tf-modules-workflow/

    S3_BACKEND_BUCKET_PREFIX_EKS_SINGLE_REGION: aws/kubernetes/eks-single-region/

    # TODO : revert this before merge
    CLEANUP_CLUSTERS: false
    # CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}

    CI_MATRIX_FILE: .github/workflows-config/generic-kubernetes-migration-from-bitnami/test_matrix.yml

    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}

    # renovate: datasource=github-tags depName=camunda/camunda-platform-helm
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: main
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    clusters-info:
        needs:
            - triage
        if: needs.triage.outputs.should_skip == 'false'
        name: Define Matrix
        runs-on: ubuntu-latest
        outputs:
            platform-matrix: ${{ steps.matrix.outputs.platform_matrix }}
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Define tests matrix
              uses: ./.github/actions/internal-tests-matrix
              id: matrix
              with:
                  ci_matrix_file: ${{ env.CI_MATRIX_FILE }}
                  # TODO: revert later
                  cluster_name: migr
                  # cluster_name: ${{ inputs.cluster_name }}
                  ref_arch: migration-from-bitnami
                  is_schedule: ${{ env.IS_SCHEDULE }}
                  is_renovate_pr: ${{ env.IS_RENOVATE_PR }}

    prepare-clusters:
        name: Prepare clusters
        needs:
            - clusters-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common CI_ENCRYPTION_KEY;

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Set current target branch
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"

            - name: Create EKS cluster and login
              uses: ./.github/actions/aws-kubernetes-eks-single-region-create
              id: create_eks_cluster
              if: always() && success()
              with:
                  cluster-name: ${{ matrix.distro.clusterName }}-${{ matrix.scenario.shortName }}
                  aws-region: ${{ env.AWS_REGION }}
                  s3-backend-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  s3-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  s3-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX_EKS_SINGLE_REGION }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/
                  tf-modules-revision: ${{ github.ref }}
                  tf-modules-path: ${{ env.TF_MODULES_DIRECTORY }}
                  ref-arch: eks-single-region
                  deploy-aurora: 'false'
                  deploy-opensearch: 'false'
                  tags: >
                      {
                        "ci-run-id": "${{ github.run_id }}",
                        "ci-run-number": "${{ github.run_number }}",
                        "ci-workflow": "${{ github.workflow }}",
                        "ci-actor": "${{ github.actor }}",
                        "ci-ref": "${{ github.ref }}",
                        "ci-commit": "${{ github.sha }}",
                        "ci-repo": "${{ github.repository }}",
                        "ci-event": "${{ github.event_name }}",
                        "map-migrated": "migARUADZHVWZ"
                      }

            - name: Dump kubeconfig before encryption
              run: |
                  set -euo pipefail
                  kubectl config view --raw > "${{ runner.temp }}/kubeconfig.yaml"

            - name: Export kubeconfig and encrypt it
              id: export_kube_config
              uses: ./.github/actions/internal-generic-encrypt-export
              with:
                  file_path: ${{ runner.temp }}/kubeconfig.yaml
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}

            - uses: cloudposse/github-action-matrix-outputs-write@ed06cf3a6bf23b8dce36d1cf0d63123885bb8375 # v1
              if: always()
              id: out
              with:
                  matrix-step-name: ${{ github.job }}
                  matrix-key: ${{ matrix.distro.name }}-${{ matrix.scenario.name }}
                  outputs: |-
                      kubeconfig_encrypted: ${{ steps.export_kube_config.outputs.encrypted_file_base64 }}

    access-info:
        name: Read kube configs from matrix
        runs-on: ubuntu-latest
        needs: prepare-clusters
        outputs:
            config: ${{ steps.read-workflow.outputs.result }}
        steps:
            - uses: cloudposse/github-action-matrix-outputs-read@33cac12fa9282a7230a418d859b93fdbc4f27b5a # v1
              id: read-workflow
              with:
                  matrix-step-name: prepare-clusters

    migration-tests:
        name: Run migration tests - ${{ matrix.distro.name }} - ${{ matrix.scenario.name }}
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - access-info
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}
        env:
            CAMUNDA_NAMESPACE: camunda
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

            - name: Install asdf tools with cache for the project
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_USER;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_ENCRYPTION_KEY;

            - name: Configure AWS CLI for EKS access
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Retrieve kubeconfig from outputs
              uses: ./.github/actions/internal-generic-decrypt-import
              with:
                  output_path: ${{ runner.temp }}/kubeconfig
                  encrypted_file_base64: >
                      ${{ fromJson(needs.access-info.outputs.config).kubeconfig_encrypted[
                        format(
                          '{0}-{1}',
                          matrix.distro.name,
                          matrix.scenario.name
                        )
                      ] }}
                  encryption_key: ${{ steps.secrets.outputs.CI_ENCRYPTION_KEY }}

            - name: ðŸ” Login into the cluster
              timeout-minutes: 2
              run: |
                  set -euo pipefail

                  mkdir -p "$HOME/.kube"
                  mv "${{ runner.temp }}/kubeconfig" "$HOME/.kube/config"

                  kubectl config current-context
                  kubectl get nodes

            - name: ðŸ“ Get a copy of the migration scripts
              timeout-minutes: 5
              run: |
                  set -euo pipefail
                  ./generic/kubernetes/migration-from-bitnami/get-your-copy.sh
                  tree generic/kubernetes/migration-from-bitnami/

            - name: ðŸ—ï¸ Clean and recreate Camunda namespace
              timeout-minutes: 20
              uses: ./.github/actions/internal-clean-namespace
              with:
                  namespace: ${{ env.CAMUNDA_NAMESPACE }}
                  recreate: 'true'

            - name: Wait for cluster stability
              run: sleep 30

            # =========================================================================
            # STEP 1: Deploy Camunda with Bitnami sub-charts (SOURCE)
            # =========================================================================
            - name: ðŸš€ Deploy Camunda with Bitnami sub-charts
              timeout-minutes: 20
              run: |
                  set -euo pipefail

                  echo "Setting up environment variables..."
                  export CAMUNDA_NAMESPACE="${{ env.CAMUNDA_NAMESPACE }}"
                  export CAMUNDA_RELEASE_NAME="camunda"

                  echo "Adding Camunda Helm repository..."
                  helm repo add camunda https://helm.camunda.io
                  helm repo update

                  echo "Creating Docker registry secret..."
                  kubectl create secret docker-registry index-docker-io \
                      --docker-server=index.docker.io \
                      --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                      --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                      --namespace="${CAMUNDA_NAMESPACE}" || true

                  echo "Creating Bitnami Camunda values file..."
                  cat > /tmp/bitnami-values.yml <<'EOF'
                  # Camunda with Bitnami sub-charts (migration source)
                  global:
                    image:
                      pullSecrets:
                        - name: index-docker-io

                  # Enable Bitnami Elasticsearch (default)
                  elasticsearch:
                    enabled: true
                    replicas: 1
                    minimumMasterNodes: 1
                    resources:
                      requests:
                        cpu: 500m
                        memory: 1Gi
                      limits:
                        cpu: 1
                        memory: 2Gi

                  # Enable Bitnami PostgreSQL for Identity (renamed from postgresql to identityPostgresql in 8.8)
                  identityPostgresql:
                    enabled: true
                    auth:
                      username: identity
                      password: identity-password
                      database: identity

                  identity:
                    enabled: true

                  # Enable Bitnami Keycloak with PostgreSQL
                  identityKeycloak:
                    enabled: true
                    postgresql:
                      enabled: true
                      auth:
                        username: keycloak
                        password: keycloak-password
                        database: keycloak

                  # WebModeler with SMTP configuration required for Camunda 8.8+
                  webModeler:
                    enabled: true
                    restapi:
                      mail:
                        smtpHost: "smtp.example.com"
                        smtpPort: 587
                        smtpUser: "test@example.com"
                        smtpPassword: "test-password"
                        fromAddress: "noreply@example.com"

                  webModelerPostgresql:
                    enabled: true
                    auth:
                      username: webmodeler
                      password: webmodeler-password
                      database: webmodeler

                  # Orchestration cluster configuration (replaces zeebe, operate, tasklist in 8.8+)
                  orchestration:
                    enabled: true
                    clusterSize: "1"
                    partitionCount: "1"
                    replicationFactor: "1"
                    profiles:
                      broker: true
                      operate: true
                      tasklist: true
                    resources:
                      requests:
                        cpu: 500m
                        memory: 1Gi

                  optimize:
                    enabled: false  # Disable Optimize for faster testing

                  connectors:
                    enabled: false  # Disable Connectors for faster testing
                  EOF

                  echo "Installing Camunda Platform with Bitnami sub-charts..."
                  helm install camunda camunda/camunda-platform \
                      --namespace "${CAMUNDA_NAMESPACE}" \
                      --values /tmp/bitnami-values.yml \
                      --timeout 15m \
                      --wait

                  echo "âœ“ Camunda with Bitnami sub-charts deployed successfully"

            - name: â³ Wait for Bitnami deployment to be ready
              timeout-minutes: 15
              run: |
                  set -euo pipefail

                  echo "Waiting for all pods to be ready..."
                  sleep 60

                  kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}"

                  # Wait for key deployments
                  kubectl rollout status deployment/camunda-zeebe-gateway -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true
                  kubectl rollout status deployment/camunda-operate -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true
                  kubectl rollout status deployment/camunda-identity -n "${{ env.CAMUNDA_NAMESPACE }}" --timeout=300s || true

                  echo "âœ“ Bitnami deployment ready"

            # =========================================================================
            # STEP 1.5: Generate Test Data for Migration Validation
            # =========================================================================
            - name: ðŸ“Š Generate test data using Camunda 8 Benchmark
              timeout-minutes: 15
              run: |
                  set -euo pipefail

                  echo "Generating test data using camunda-8-benchmark..."

                  # Deploy the benchmark job that will create process instances
                  cat <<EOF | kubectl apply -n "${{ env.CAMUNDA_NAMESPACE }}" -f -
                  apiVersion: batch/v1
                  kind: Job
                  metadata:
                    name: migration-test-data-generator
                    labels:
                      app: migration-test-data
                  spec:
                    ttlSecondsAfterFinished: 3600
                    backoffLimit: 3
                    template:
                      metadata:
                        labels:
                          app: migration-test-data
                      spec:
                        restartPolicy: Never
                        containers:
                          - name: benchmark
                            image: camundacommunityhub/camunda-8-benchmark:main
                            env:
                              - name: CAMUNDA_CLIENT_MODE
                                value: "self-managed"
                              - name: CAMUNDA_CLIENT_ZEEBE_BASE_URL
                                value: "http://camunda-zeebe-gateway:26500"
                              - name: CAMUNDA_CLIENT_ZEEBE_GRPC_ADDRESS
                                value: "http://camunda-zeebe-gateway:26500"
                              - name: CAMUNDA_CLIENT_ZEEBE_PREFER_REST_OVER_GRPC
                                value: "false"
                              - name: BENCHMARK_START_PROCESSES
                                value: "true"
                              - name: BENCHMARK_START_PI_PER_SECOND
                                value: "5"
                              - name: BENCHMARK_AUTO_DEPLOY_PROCESS
                                value: "true"
                              - name: BENCHMARK_BPMN_PROCESS_ID
                                value: "benchmark"
                              - name: BENCHMARK_START_WORKERS
                                value: "true"
                              - name: BENCHMARK_JOB_TYPE
                                value: "benchmark-task"
                              - name: BENCHMARK_TASK_COMPLETION_DELAY
                                value: "100"
                              - name: BENCHMARK_WARMUP_PHASE_DURATION_MILLIS
                                value: "10000"
                              - name: BENCHMARK_START_RATE_ADJUSTMENT_STRATEGY
                                value: "none"
                            resources:
                              requests:
                                cpu: 500m
                                memory: 512Mi
                              limits:
                                cpu: 1
                                memory: 1Gi
                  EOF

                  echo "Waiting for benchmark job to start..."
                  sleep 10

                  # Wait for the job to run for a bit and create some data
                  echo "Benchmark is running, waiting for data generation (60s)..."
                  sleep 60

                  # Check job status
                  kubectl get job migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" || true
                  kubectl logs job/migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" --tail=50 || true

                  # Delete the job (we just needed it to generate some data)
                  kubectl delete job migration-test-data-generator -n "${{ env.CAMUNDA_NAMESPACE }}" --ignore-not-found=true || true

                  # Create a marker ConfigMap to track test data generation
                  kubectl create configmap migration-test-data-marker \
                      --namespace="${{ env.CAMUNDA_NAMESPACE }}" \
                      --from-literal=created_at="$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
                      --from-literal=generator="camunda-8-benchmark" \
                      --from-literal=process_id="benchmark" \
                      --dry-run=client -o yaml | kubectl apply -f -

                  echo "âœ“ Test data generation completed"

            - name: â³ Wait for data to propagate to Elasticsearch
              timeout-minutes: 5
              run: |
                  set -euo pipefail

                  echo "Waiting for data to propagate to Elasticsearch..."
                  sleep 60

                  # Verify data in Elasticsearch
                  ES_POD=$(kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}" \
                      -l "app=elasticsearch" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                  if [[ -n "$ES_POD" ]]; then
                      echo "Checking Elasticsearch indices..."
                      kubectl exec -n "${{ env.CAMUNDA_NAMESPACE }}" "$ES_POD" -- \
                          curl -s "localhost:9200/_cat/indices?v" | grep -E "(operate|tasklist)" || true

                      # Count process instances
                      INSTANCE_COUNT=$(kubectl exec -n "${{ env.CAMUNDA_NAMESPACE }}" "$ES_POD" -- \
                          curl -s "localhost:9200/operate-list-view-*/_count" 2>/dev/null | jq -r '.count // 0' || echo "0")
                      echo "Found ${INSTANCE_COUNT} process instances in Elasticsearch"
                  fi

                  echo "âœ“ Data propagation complete"

            # =========================================================================
            # STEP 2: Run Migration Scripts
            # =========================================================================
            - name: ðŸ“‹ Set migration environment variables
              run: |
                  set -euo pipefail

                  # Set environment variables for migration scripts
                  cat >> "$GITHUB_ENV" <<EOF
                  CAMUNDA_NAMESPACE=${{ env.CAMUNDA_NAMESPACE }}
                  CAMUNDA_RELEASE_NAME=camunda
                  BACKUP_PVC_NAME=migration-backup-pvc
                  BACKUP_STORAGE_SIZE=50Gi
                  EOF

            - name: ðŸ” Run prerequisites check
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/
              run: |
                  set -euo pipefail
                  ./1-prerequisites/check-prerequisites.sh

            - name: ðŸ“¦ Create backup PVC
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/
              run: |
                  set -euo pipefail
                  ./1-prerequisites/create-backup-pvc.sh

            # --- Orchestration Migration (ES â†’ ECK) ---
            - name: ðŸ”„ Orchestration - 0. Introspect
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/orchestration/
              run: |
                  set -euo pipefail
                  ./0-introspect.sh

            - name: ðŸ”„ Orchestration - 1. Backup
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/orchestration/
              run: |
                  set -euo pipefail
                  ./1-backup.sh

            - name: ðŸ”„ Orchestration - 2. Deploy Target (ECK) - Non-Interactive
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/orchestration/
              run: |
                  set -euo pipefail

                  # Pre-set environment to skip interactive prompts
                  export ES_TARGET_OPTION=1  # 1 = ECK

                  # Deploy ECK operator
                  ../../2-deploy-operators/deploy-eck.sh

                  # Run deploy-target with non-interactive input
                  echo "1" | ./2-deploy-target.sh || ./2-deploy-target.sh <<< "1"

            - name: ðŸ”„ Orchestration - 3. Freeze
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/orchestration/
              run: |
                  set -euo pipefail
                  ./3-freeze.sh

            - name: ðŸ”„ Orchestration - 4. Restore
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/orchestration/
              run: |
                  set -euo pipefail
                  ./4-restore.sh

            - name: ðŸ”„ Orchestration - 5. Cutover
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/orchestration/
              run: |
                  set -euo pipefail
                  ./5-cutover.sh

            - name: ðŸ”„ Orchestration - 6. Validate
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/orchestration/
              run: |
                  set -euo pipefail
                  ./6-validate.sh

            # --- Identity Migration (PG â†’ CNPG) ---
            - name: ðŸ”„ Identity - 0. Introspect
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/identity/
              run: |
                  set -euo pipefail
                  ./0-introspect.sh

            - name: ðŸ”„ Identity - 1. Backup
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/identity/
              run: |
                  set -euo pipefail
                  ./1-backup.sh

            - name: ðŸ”„ Identity - 2. Deploy Target (CNPG) - Non-Interactive
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/identity/
              run: |
                  set -euo pipefail

                  # Deploy CNPG operator
                  ../../2-deploy-operators/deploy-cnpg.sh

                  # Run deploy-target with CNPG option (1)
                  echo "1" | ./2-deploy-target.sh

            - name: ðŸ”„ Identity - 3. Freeze
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/identity/
              run: |
                  set -euo pipefail
                  ./3-freeze.sh

            - name: ðŸ”„ Identity - 4. Restore
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/identity/
              run: |
                  set -euo pipefail
                  ./4-restore.sh

            - name: ðŸ”„ Identity - 5. Cutover
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/identity/
              run: |
                  set -euo pipefail
                  ./5-cutover.sh

            - name: ðŸ”„ Identity - 6. Validate
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/identity/
              run: |
                  set -euo pipefail
                  ./6-validate.sh

            # --- Keycloak Migration (KC â†’ Keycloak Operator) ---
            - name: ðŸ”„ Keycloak - 0. Introspect
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/keycloak/
              run: |
                  set -euo pipefail
                  ./0-introspect.sh

            - name: ðŸ”„ Keycloak - 1. Backup
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/keycloak/
              run: |
                  set -euo pipefail
                  ./1-backup.sh

            - name: ðŸ”„ Keycloak - 2. Deploy Target - Non-Interactive
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/keycloak/
              run: |
                  set -euo pipefail

                  # Deploy Keycloak operator
                  ../../2-deploy-operators/deploy-keycloak-operator.sh

                  # Run deploy-target:
                  # Option 1 = CNPG for PostgreSQL
                  # Option 2 = no-domain mode
                  printf "1\n2\n" | ./2-deploy-target.sh

            - name: ðŸ”„ Keycloak - 3. Freeze
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/keycloak/
              run: |
                  set -euo pipefail
                  ./3-freeze.sh

            - name: ðŸ”„ Keycloak - 4. Restore
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/keycloak/
              run: |
                  set -euo pipefail
                  ./4-restore.sh

            - name: ðŸ”„ Keycloak - 5. Cutover
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/keycloak/
              run: |
                  set -euo pipefail
                  ./5-cutover.sh

            - name: ðŸ”„ Keycloak - 6. Validate
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/keycloak/
              run: |
                  set -euo pipefail
                  ./6-validate.sh

            # --- WebModeler Migration (PG â†’ CNPG) ---
            - name: ðŸ”„ WebModeler - 0. Introspect
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/webmodeler/
              run: |
                  set -euo pipefail
                  ./0-introspect.sh

            - name: ðŸ”„ WebModeler - 1. Backup
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/webmodeler/
              run: |
                  set -euo pipefail
                  ./1-backup.sh

            - name: ðŸ”„ WebModeler - 2. Deploy Target (CNPG) - Non-Interactive
              timeout-minutes: 15
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/webmodeler/
              run: |
                  set -euo pipefail

                  # Run deploy-target with CNPG option (1)
                  echo "1" | ./2-deploy-target.sh

            - name: ðŸ”„ WebModeler - 3. Freeze
              timeout-minutes: 5
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/webmodeler/
              run: |
                  set -euo pipefail
                  ./3-freeze.sh

            - name: ðŸ”„ WebModeler - 4. Restore
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/webmodeler/
              run: |
                  set -euo pipefail
                  ./4-restore.sh

            - name: ðŸ”„ WebModeler - 5. Cutover
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/webmodeler/
              run: |
                  set -euo pipefail
                  ./5-cutover.sh

            - name: ðŸ”„ WebModeler - 6. Validate
              timeout-minutes: 10
              working-directory: ./generic/kubernetes/migration-from-bitnami/migrations/webmodeler/
              run: |
                  set -euo pipefail
                  ./6-validate.sh

            # =========================================================================
            # STEP 3: Verify Final State
            # =========================================================================
            - name: âœ… Verify final deployment state
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  echo "=== Final Pod Status ==="
                  kubectl get pods -n "${{ env.CAMUNDA_NAMESPACE }}"

                  echo ""
                  echo "=== ECK Elasticsearch Status ==="
                  kubectl get elasticsearch -n "${{ env.CAMUNDA_NAMESPACE }}" || echo "No ECK clusters found"

                  echo ""
                  echo "=== CNPG PostgreSQL Clusters ==="
                  kubectl get clusters.postgresql.cnpg.io -n "${{ env.CAMUNDA_NAMESPACE }}" || echo "No CNPG clusters found"

                  echo ""
                  echo "=== Keycloak Operator Instances ==="
                  kubectl get keycloaks.k8s.keycloak.org -n "${{ env.CAMUNDA_NAMESPACE }}" || echo "No Keycloak instances found"

                  echo ""
                  echo "âœ“ Migration tests completed successfully!"

            - name: Validate migrated test data
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  echo "Validating test data after migration..."
                  export CAMUNDA_NAMESPACE="${{ env.CAMUNDA_NAMESPACE }}"
                  export CAMUNDA_RELEASE_NAME="camunda"

                  # Run the validation script
                  ./generic/kubernetes/migration-from-bitnami/shared/test-data/validate-test-data.sh

                  echo "âœ“ Test data validation completed"

            - name: Get failed Pods info
              if: failure()
              run: |
                  set -euo pipefail

                  kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" get po
                  kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" describe po "$pod_name";
                    kubectl -n "${{ env.CAMUNDA_NAMESPACE }}" logs "$pod_name" --tail=100;
                  done

    cleanup-clusters:
        name: Cleanup clusters
        if: always()
        runs-on: ubuntu-latest
        needs:
            - clusters-info
            - migration-tests
        strategy:
            fail-fast: false
            matrix:
                distro: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).distro }}
                scenario: ${{ fromJson(needs.clusters-info.outputs.platform-matrix).scenario }}

        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              if: env.CLEANUP_CLUSTERS == 'true'
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Set current target branch
              if: env.CLEANUP_CLUSTERS == 'true'
              id: target-branch
              run: |
                  set -euo pipefail
                  TARGET_BRANCH=$(cat .target-branch)
                  echo "TARGET_BRANCH=$TARGET_BRANCH" | tee -a "$GITHUB_OUTPUT"

            - name: Delete on-demand EKS Cluster
              uses: ./.github/actions/aws-generic-terraform-cleanup
              if: always() && env.CLEANUP_CLUSTERS == 'true'
              timeout-minutes: 125
              with:
                  tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  max-age-hours: 0
                  target: ${{ matrix.distro.clusterName }}-${{ matrix.scenario.shortName }}
                  tf-bucket-key-prefix: ${{ env.S3_BACKEND_BUCKET_PREFIX_EKS_SINGLE_REGION }}${{ steps.target-branch.outputs.TARGET_BRANCH }}/
                  modules-order: vpn,cluster

    report-success:
        name: Report success
        runs-on: ubuntu-latest
        needs:
            - migration-tests
            - cleanup-clusters
        steps:
            - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

            - name: Prevent other runs for renovate
              if: ${{ env.IS_RENOVATE_PR == 'true' }}
              env:
                  GH_TOKEN: ${{ github.token }}
              uses: ./.github/actions/internal-apply-skip-label

    report-failures:
        name: Report failures
        if: failure()
        runs-on: ubuntu-latest
        needs:
            - report-success
        steps:
            - name: Notify in Slack in case of failure
              id: slack-notification
              if: ${{ env.IS_SCHEDULE == 'true' }}
              uses: camunda/infraex-common-config/.github/actions/report-failure-on-slack@b52f5bb65b5bb9e0b0458c46b8671bf1fe9a907f # 1.5.7
              with:
                  vault_addr: ${{ secrets.VAULT_ADDR }}
                  vault_role_id: ${{ secrets.VAULT_ROLE_ID }}
                  vault_secret_id: ${{ secrets.VAULT_SECRET_ID }}
