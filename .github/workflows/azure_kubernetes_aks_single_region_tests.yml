---
name: Tests - Integration - Azure Kubernetes AKS Single Region

permissions:
    id-token: write
    contents: read

on:
    schedule:
        - cron: 0 3 * * 3 # Runs at 3 AM on Wednesdays
    pull_request:
        paths:
            - .github/workflows/azure_kubernetes_aks_single_region_tests.yml
            - .tool-versions
            - generic/kubernetes/single-region/**
            - azure/kubernetes/aks-single-region/**
            - azure/modules/**
            - generic/kubernetes/single-region/procedure/**
            - '!azure/kubernetes/aks-single-region/test/golden/**'
            - .github/actions/azure-kubernetes-aks-single-region-create/**
            - .github/actions/azure-kubernetes-aks-single-region-cleanup/**

    workflow_dispatch:
        inputs:
            delete_clusters:
                description: Whether to delete the clusters.
                type: boolean
                default: true
            enable_tests:
                description: Whether to enable the tests.
                type: boolean
                default: true
            cluster_name:
                description: Name of the cluster to deploy.
                type: string
                required: false
            resource-group-name:
                description: Name of the resource group.
                type: string
                required: false
            resource-prefix:
                description: Prefix for the resource names.
                type: string
                required: false

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: ${{ !contains('renovate[bot]', github.actor) }}

env:
    IS_SCHEDULE: ${{ contains(github.head_ref, 'schedules/') || github.event_name == 'schedule' && 'true' || 'false' }}
    IS_RENOVATE_PR: ${{ github.event_name == 'pull_request' && github.event.pull_request.user.login == 'renovate[bot]' }}

    # terraform statefile is to be stored in an AWS S3 bucket to benefit from existing automated cleanup capabilities
    # AWS itself is not used to deploy resources, only Azure
    AWS_PROFILE: infraex
    AWS_REGION: eu-central-1
    S3_BACKEND_BUCKET: tests-ra-aws-rosa-hcp-tf-state-eu-central-1
    S3_BUCKET_REGION: eu-central-1

    CLEANUP_CLUSTERS: ${{ github.event.inputs.delete_clusters || 'true' }}

    # TEST VARIABLES

    # Docker Hub auth to avoid image pull rate limit.
    # Vars with "TEST_" prefix are used in the test runner tool (Task).
    TESTS_ENABLED: ${{ github.event.inputs.enable_tests || 'true' }}
    # TODO: [release-duty] before the release, update repo ref!
    # renovate: datasource=github-tags depName=camunda/camunda-platform-helm
    TESTS_CAMUNDA_HELM_CHART_REPO_REF: main   # git reference used to clone the camunda/camunda-platform-helm repository to perform the tests
    TESTS_CAMUNDA_HELM_CHART_REPO_PATH: ./.camunda_helm_repo   # where to clone it

    # Optional components that are not enabled by default in the doc
    WEBMODELER_ENABLED: 'true'
    CONSOLE_ENABLED: 'true'

jobs:
    triage:
        runs-on: ubuntu-latest
        outputs:
            should_skip: ${{ steps.skip_check.outputs.should_skip }}
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
            - name: Check labels
              id: skip_check
              uses: ./.github/actions/internal-triage-skip

    prepare-clusters:
        name: Prepare clusters
        outputs:
            POSTGRES_FQDN: ${{ steps.db_env.outputs.POSTGRES_FQDN }}
            POSTGRES_PORT: ${{ steps.db_env.outputs.POSTGRES_PORT }}
            POSTGRES_ADMIN_USERNAME: ${{ steps.db_env.outputs.POSTGRES_ADMIN_USERNAME }}
            POSTGRES_ADMIN_PASSWORD: ${{ steps.db_env.outputs.POSTGRES_ADMIN_PASSWORD }}
            DB_KEYCLOAK_NAME: ${{ steps.db_env.outputs.DB_KEYCLOAK_NAME }}
            DB_KEYCLOAK_USERNAME: ${{ steps.db_env.outputs.DB_KEYCLOAK_USERNAME }}
            DB_KEYCLOAK_PASSWORD: ${{ steps.db_env.outputs.DB_KEYCLOAK_PASSWORD }}
            DB_IDENTITY_NAME: ${{ steps.db_env.outputs.DB_IDENTITY_NAME }}
            DB_IDENTITY_USERNAME: ${{ steps.db_env.outputs.DB_IDENTITY_USERNAME }}
            DB_IDENTITY_PASSWORD: ${{ steps.db_env.outputs.DB_IDENTITY_PASSWORD }}
            DB_WEBMODELER_NAME: ${{ steps.db_env.outputs.DB_WEBMODELER_NAME }}
            DB_WEBMODELER_USERNAME: ${{ steps.db_env.outputs.DB_WEBMODELER_USERNAME }}
            DB_WEBMODELER_PASSWORD: ${{ steps.db_env.outputs.DB_WEBMODELER_PASSWORD }}
            RESOURCE_GROUP_NAME: ${{ steps.db_env.outputs.RESOURCE_GROUP_NAME }}
            CLUSTER_NAME: ${{ steps.db_env.outputs.CLUSTER_NAME }}

        needs:
            - triage
        if: needs.triage.outputs.should_skip == 'false'
        runs-on: ubuntu-latest
        env:
            TF_MODULES_PATH: ./.action-tf-modules/azure-kubernetes-aks-single-region-create/
            CAMUNDA_NAMESPACE: camunda
        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
              with:
                  ref: ${{ github.ref }}
                  fetch-depth: 0

            - name: Install asdf tools with cache
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@76b69ba0940210c33bd7c271917d2ec1da6a5ca1 # main

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@7709c609789c5e27b757a85817483caadbb5939a # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AZURE_CLIENT_ID;
                      secret/data/products/infrastructure-experience/ci/common AZURE_TENANT_ID;
                      secret/data/products/infrastructure-experience/ci/common AZURE_SUBSCRIPTION_ID;

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Azure Login with OIDC
              uses: azure/login@cb79c773a3cfa27f31f25eb3f677781210c9ce3d # v1.6.1
              with:
                  client-id: ${{ steps.secrets.outputs.AZURE_CLIENT_ID }}
                  tenant-id: ${{ steps.secrets.outputs.AZURE_TENANT_ID }}
                  subscription-id: ${{ steps.secrets.outputs.AZURE_SUBSCRIPTION_ID }}

            - name: Set current Camunda version
              id: camunda-version
              run: |
                  set -euo pipefail
                  CAMUNDA_VERSION=$(cat .camunda-version)
                  echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"

            - name: Generate resource names
              id: generate_names
              shell: bash
              run: |
                  set -euo pipefail

                  PREFIX="${INPUT_RESOURCE_PREFIX:-}"
                  CLUSTER="${INPUT_CLUSTER_NAME:-}"
                  RG="${INPUT_RESOURCE_GROUP_NAME:-}"

                  if [[ -z "$PREFIX" ]]; then
                    PR_NUMBER=0
                    if [[ "${GITHUB_EVENT_NAME}" == "pull_request" ]]; then
                      # for pull_request events, pull in the PR number
                      PR_NUMBER="$(jq --raw-output .pull_request.number < "$GITHUB_EVENT_PATH")"
                    fi
                    SFX="$(head /dev/urandom | tr -dc 'a-z0-9' | head -c5)"
                    PREFIX="camunda-${PR_NUMBER}-${SFX}"
                  fi

                  if [[ -z "$CLUSTER" ]]; then
                    CLUSTER="${PREFIX}-aks"
                  fi
                  if [[ -z "$RG" ]]; then
                    RG="${PREFIX}-rg"
                  fi

                  {
                    echo "RESOURCE_PREFIX=${PREFIX}";
                    echo "CLUSTER_NAME=${CLUSTER}";
                    echo "RESOURCE_GROUP_NAME=${RG}"
                  } >>"$GITHUB_ENV"


            - name: Export S3_BACKEND_BUCKET
              id: s3_prefix
              run: |
                  set -euo pipefail
                  echo "S3_BACKEND_BUCKET_PREFIX=azure/kubernetes/aks-single-region/" | tee -a "$GITHUB_OUTPUT"

            - name: Create K8S cluster and login
              uses: ./.github/actions/azure-kubernetes-aks-single-region-create
              id: create_cluster
              # Do not interrupt tests; otherwise, the Terraform state may become inconsistent.
              if: always() && success()
              with:
                  resource-prefix: ${{ env.RESOURCE_PREFIX }}
                  cluster-name: ${{ env.CLUSTER_NAME }}
                  resource-group-name: ${{ env.RESOURCE_GROUP_NAME }}
                  s3-backend-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  s3-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  s3-bucket-key-prefix: ${{ steps.s3_prefix.outputs.S3_BACKEND_BUCKET_PREFIX }}${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}/
                  tf-modules-revision: ${{ github.ref }}
                  tf-modules-path: ${{ env.TF_MODULES_PATH }}
                  ref-arch: aks-single-region

            - name: Source env from Terraform
              id: db_env
              working-directory: ${{ env.TF_MODULES_PATH }}/azure/kubernetes/aks-single-region/
              run: |
                  set -euo pipefail

                  # Source the shell script that exports your DB-related variables
                  # shellcheck disable=SC2086
                  source $GITHUB_WORKSPACE/azure/kubernetes/aks-single-region/procedure/vars-create-db.sh

                  {
                    echo "POSTGRES_FQDN=$POSTGRES_FQDN"
                    echo "POSTGRES_PORT=$POSTGRES_PORT"
                    echo "POSTGRES_ADMIN_USERNAME=$POSTGRES_ADMIN_USERNAME"
                    echo "POSTGRES_ADMIN_PASSWORD=$POSTGRES_ADMIN_PASSWORD"
                    echo "DB_KEYCLOAK_NAME=$DB_KEYCLOAK_NAME"
                    echo "DB_KEYCLOAK_USERNAME=$DB_KEYCLOAK_USERNAME"
                    echo "DB_KEYCLOAK_PASSWORD=$DB_KEYCLOAK_PASSWORD"
                    echo "DB_IDENTITY_NAME=$DB_IDENTITY_NAME"
                    echo "DB_IDENTITY_USERNAME=$DB_IDENTITY_USERNAME"
                    echo "DB_IDENTITY_PASSWORD=$DB_IDENTITY_PASSWORD"
                    echo "DB_WEBMODELER_NAME=$DB_WEBMODELER_NAME"
                    echo "DB_WEBMODELER_USERNAME=$DB_WEBMODELER_USERNAME"
                    echo "DB_WEBMODELER_PASSWORD=$DB_WEBMODELER_PASSWORD"
                    echo "RESOURCE_GROUP_NAME=$RESOURCE_GROUP_NAME"
                    echo "CLUSTER_NAME=$CLUSTER_NAME"
                  } >> "$GITHUB_OUTPUT"

    integration-tests:
        name: Run integration tests
        runs-on: ubuntu-latest
        needs:
            - prepare-clusters
        env:
            CAMUNDA_NAMESPACE: camunda   # This namespace is hard-coded in the documentation
            # https://github.com/camunda/camunda-platform-helm/blob/test/integration/scenarios/chart-full-setup/Taskfile.yaml#L12C15-L12C32
            TEST_CLUSTER_TYPE: kubernetes
            MAIL_OVERWRITE: admin@camunda.ie
            POSTGRES_FQDN: ${{ needs.prepare-clusters.outputs.POSTGRES_FQDN }}
            DB_HOST: ${{ needs.prepare-clusters.outputs.POSTGRES_FQDN }}
            POSTGRES_PORT: ${{ needs.prepare-clusters.outputs.POSTGRES_PORT }}
            POSTGRES_ADMIN_USERNAME: ${{ needs.prepare-clusters.outputs.POSTGRES_ADMIN_USERNAME }}
            POSTGRES_ADMIN_PASSWORD: ${{ needs.prepare-clusters.outputs.POSTGRES_ADMIN_PASSWORD }}
            DB_KEYCLOAK_NAME: ${{ needs.prepare-clusters.outputs.DB_KEYCLOAK_NAME }}
            DB_KEYCLOAK_USERNAME: ${{ needs.prepare-clusters.outputs.DB_KEYCLOAK_USERNAME }}
            DB_KEYCLOAK_PASSWORD: ${{ needs.prepare-clusters.outputs.DB_KEYCLOAK_PASSWORD }}
            DB_IDENTITY_NAME: ${{ needs.prepare-clusters.outputs.DB_IDENTITY_NAME }}
            DB_IDENTITY_USERNAME: ${{ needs.prepare-clusters.outputs.DB_IDENTITY_USERNAME }}
            DB_IDENTITY_PASSWORD: ${{ needs.prepare-clusters.outputs.DB_IDENTITY_PASSWORD }}
            DB_WEBMODELER_NAME: ${{ needs.prepare-clusters.outputs.DB_WEBMODELER_NAME }}
            DB_WEBMODELER_USERNAME: ${{ needs.prepare-clusters.outputs.DB_WEBMODELER_USERNAME }}
            DB_WEBMODELER_PASSWORD: ${{ needs.prepare-clusters.outputs.DB_WEBMODELER_PASSWORD }}
            RESOURCE_GROUP_NAME: ${{ needs.prepare-clusters.outputs.RESOURCE_GROUP_NAME }}
            CLUSTER_NAME: ${{ needs.prepare-clusters.outputs.CLUSTER_NAME }}

        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

            - name: Install asdf tools with cache for the project
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@76b69ba0940210c33bd7c271917d2ec1da6a5ca1 # 1.3.7

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@7709c609789c5e27b757a85817483caadbb5939a # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_USER;
                      secret/data/products/infrastructure-experience/ci/common DOCKERHUB_PASSWORD;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_ID;
                      secret/data/products/infrastructure-experience/ci/common CI_CAMUNDA_USER_TEST_CLIENT_SECRET;
                      secret/data/products/infrastructure-experience/ci/common AZURE_CLIENT_ID;
                      secret/data/products/infrastructure-experience/ci/common AZURE_TENANT_ID;
                      secret/data/products/infrastructure-experience/ci/common AZURE_SUBSCRIPTION_ID;

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Azure Login with OIDC
              uses: azure/login@cb79c773a3cfa27f31f25eb3f677781210c9ce3d # v1.6.1
              with:
                  client-id: ${{ steps.secrets.outputs.AZURE_CLIENT_ID }}
                  tenant-id: ${{ steps.secrets.outputs.AZURE_TENANT_ID }}
                  subscription-id: ${{ steps.secrets.outputs.AZURE_SUBSCRIPTION_ID }}

            - name: üîê Login into the cluster
              timeout-minutes: 10
              run: |
                  set -euo pipefail
                  az aks get-credentials --resource-group "${{ env.RESOURCE_GROUP_NAME }}" --name "${{ env.CLUSTER_NAME }}" --overwrite-existing

                  kubectl config current-context
                  kubectl get nodes

            - name: üå± Register chart setup environment values
              timeout-minutes: 3
              run: |
                  set -euo pipefail

                  # the chart env should be loaded by the client at the very first step of his installation
                  source .github/scripts/gha-functions.sh
                  export_new_env_vars generic/kubernetes/single-region/procedure/chart-env.sh


            - name: üèóÔ∏è Prepare a fresh namespace for the tests
              run: |
                  set -euo pipefail

                  # Delete the namespace to ensure a fresh start
                  if kubectl get namespace "$CAMUNDA_NAMESPACE" &>/dev/null; then
                  kubectl delete namespace "$CAMUNDA_NAMESPACE" --wait
                  while kubectl get namespace "$CAMUNDA_NAMESPACE" &>/dev/null; do
                      echo "Namespace $CAMUNDA_NAMESPACE still being deleted, waiting..."
                      sleep 5
                  done
                  fi

                  # Create the namespace
                  kubectl create namespace "$CAMUNDA_NAMESPACE"

            - name: üßπ Clean up databases
              run: |
                  set -euo pipefail

                  "${GITHUB_WORKSPACE}/azure/kubernetes/aks-single-region/procedure/create-setup-db-secret.sh"

                  # Reset PostgreSQL
                  kubectl apply -f ./azure/kubernetes/aks-single-region/manifests/reset-postgres.yml --namespace "$CAMUNDA_NAMESPACE"

                  # Wait for the job to complete
                  while true; do
                      STATUS=$(kubectl get job reset-azure-postgres --namespace "$CAMUNDA_NAMESPACE" -o jsonpath='{.status.succeeded}')

                      if [[ "$STATUS" == "1" ]]; then
                          echo "Job completed successfully."
                          break
                      fi

                      echo "Waiting for job to complete..."
                      sleep 5
                  done
                  echo "[DEBUG] Job log:"
                  kubectl logs job/reset-azure-postgres --namespace "$CAMUNDA_NAMESPACE"
                  kubectl delete job reset-azure-postgres --namespace "$CAMUNDA_NAMESPACE"

                  kubectl delete secret setup-db-secret --namespace "$CAMUNDA_NAMESPACE"

            - name: üõ†Ô∏è Assemble deployment values
              timeout-minutes: 10
              run: |
                  set -o errexit
                  set -euxo pipefail

                  echo "Construct the values.yml file"

                  cp -f azure/kubernetes/aks-single-region/helm-values/values-no-domain.yml ./values.yml

                  if [ "$WEBMODELER_ENABLED" == "true" ]; then
                    echo "Enabling WebModeler"
                    yq -i '.webModeler.enabled = true' values.yml
                  fi

                  if [ "$CONSOLE_ENABLED" == "true" ]; then
                    echo "Enabling Console"
                    yq -i '.console.enabled = true' values.yml
                  fi

                  # Add integration tests values
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    for file in registry.yml identity.yml; do
                      yq ". *+ load(\"generic/kubernetes/single-region/tests/helm-values/$file\")" values.yml > values-result.yml
                      cat values-result.yml && mv values-result.yml values.yml
                    done
                  fi

                  ./generic/kubernetes/single-region/procedure/assemble-envsubst-values.sh

            - name: üõ†Ô∏è Configure the PSQL database and associated access
              timeout-minutes: 10
              run: |
                  set -euo pipefail
                  ./azure/kubernetes/aks-single-region/procedure/check-env-variables.sh
                  ./azure/kubernetes/aks-single-region/procedure/create-setup-db-secret.sh

                  kubectl apply -f ./azure/kubernetes/aks-single-region/manifests/setup-postgres-create-db.yml --namespace "$CAMUNDA_NAMESPACE"

                  # Wait for the job to complete
                  while true; do
                      STATUS=$(kubectl get job create-setup-user-db --namespace "$CAMUNDA_NAMESPACE" -o jsonpath='{.status.succeeded}')

                      if [[ "$STATUS" == "1" ]]; then
                          echo "Job completed successfully."
                          break
                      fi

                      echo "Waiting for job to complete..."
                      sleep 5
                  done

                  echo "[DEBUG] Job log:"
                  kubectl logs job/create-setup-user-db --namespace "$CAMUNDA_NAMESPACE"

                  kubectl delete job create-setup-user-db --namespace "$CAMUNDA_NAMESPACE"
                  kubectl delete secret setup-db-secret --namespace "$CAMUNDA_NAMESPACE"

            - name: üí™ Execute DB secret creation scripts
              timeout-minutes: 10
              run: |
                  set -euo pipefail
                  ./azure/kubernetes/aks-single-region/procedure/create-external-db-secrets.sh

            - name: üèÅ Install Camunda 8 using the generic/kubernetes helm chart procedure
              timeout-minutes: 10
              run: |
                  set -euo pipefail

                  source generic/kubernetes/single-region/procedure/chart-env.sh
                  source generic/kubernetes/single-region/procedure/generate-passwords.sh

                  ./generic/kubernetes/single-region/procedure/create-identity-secret.sh

                  # Generate tests objects
                  if [ "$TESTS_ENABLED" == "true" ]; then
                    # Create the pull secrets described in generic/kubernetes/single-region/tests/helm-values/registry.yml
                    kubectl create secret docker-registry index-docker-io \
                        --docker-server=index.docker.io \
                        --docker-username="${{ steps.secrets.outputs.DOCKERHUB_USER }}" \
                        --docker-password="${{ steps.secrets.outputs.DOCKERHUB_PASSWORD }}" \
                        --namespace="$CAMUNDA_NAMESPACE"

                    kubectl create secret generic identity-secret-for-components-integration \
                        --from-literal=identity-admin-client-id="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_ID }}" \
                        --from-literal=identity-admin-client-secret="${{ steps.secrets.outputs.CI_CAMUNDA_USER_TEST_CLIENT_SECRET }}" \
                        --namespace="$CAMUNDA_NAMESPACE"
                  fi

                  ./generic/kubernetes/single-region/procedure/install-chart.sh

            - name: üëÄ‚è≥ Wait for the deployment to be healthy using generic/kubernetes/single-region
              timeout-minutes: 10
              run: |
                  set -euo pipefail
                  ./generic/kubernetes/single-region/procedure/check-deployment-ready.sh

            - name: üìê Set current Camunda version
              id: camunda-version
              run: |
                  set -euo pipefail
                  CAMUNDA_VERSION=$(cat .camunda-version)
                  echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"

            - name: üß™ Run Helm Chart tests
              if: env.TESTS_ENABLED == 'true'
              timeout-minutes: 60
              uses: ./.github/actions/internal-camunda-chart-tests
              with:
                  secrets: ${{ toJSON(secrets) }}
                  camunda-version: ${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}
                  webmodeler-enabled: ${{ env.WEBMODELER_ENABLED }}
                  console-enabled: ${{ env.CONSOLE_ENABLED }}
                  elasticsearch-enabled: 'true'
                  tests-camunda-helm-chart-repo-ref: ${{ env.TESTS_CAMUNDA_HELM_CHART_REPO_REF }}
                  tests-camunda-helm-chart-repo-path: ${{ env.TESTS_CAMUNDA_HELM_CHART_REPO_PATH }}
                  test-namespace: ${{ env.CAMUNDA_NAMESPACE }}
                  test-cluster-type: ${{ env.TEST_CLUSTER_TYPE }}
                  test-release-name: ${{ env.CAMUNDA_RELEASE_NAME }}

            - name: üî¨üö® Get failed Pods info
              if: failure()
              run: |
                  set -euo pipefail

                  kubectl -n "$CAMUNDA_NAMESPACE" get po
                  kubectl -n "$CAMUNDA_NAMESPACE" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "$CAMUNDA_NAMESPACE" describe po "$pod_name";
                    kubectl -n "$CAMUNDA_NAMESPACE" logs "$pod_name";
                  done

            - name: üßπ Cleanup Namespace
              if: always()
              run: |
                  set -euo pipefail

                  helm uninstall camunda --namespace "$CAMUNDA_NAMESPACE" --wait
                  kubectl delete namespace "$CAMUNDA_NAMESPACE" --wait

    cleanup-clusters:
        name: Cleanup AKS clusters
        if: always()
        needs:
            - prepare-clusters
            - triage
            - integration-tests
        runs-on: ubuntu-latest
        env:
            CLUSTER_NAME: ${{ needs.prepare-clusters.outputs.CLUSTER_NAME }}
            RESOURCE_GROUP_NAME: ${{ needs.prepare-clusters.outputs.RESOURCE_GROUP_NAME }}

        steps:
            - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
              if: env.CLEANUP_CLUSTERS == 'true'
              with:
                  fetch-depth: 0

            - name: Install asdf tools with cache
              if: env.CLEANUP_CLUSTERS == 'true'
              uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@76b69ba0940210c33bd7c271917d2ec1da6a5ca1 # 1.3.7

            - name: Import Secrets
              id: secrets
              uses: hashicorp/vault-action@7709c609789c5e27b757a85817483caadbb5939a # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  exportEnv: false
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AZURE_CLIENT_ID;
                      secret/data/products/infrastructure-experience/ci/common AZURE_TENANT_ID;
                      secret/data/products/infrastructure-experience/ci/common AZURE_SUBSCRIPTION_ID;

            - name: Azure Login with OIDC
              uses: azure/login@cb79c773a3cfa27f31f25eb3f677781210c9ce3d # v1.6.1
              with:
                  client-id: ${{ steps.secrets.outputs.AZURE_CLIENT_ID }}
                  tenant-id: ${{ steps.secrets.outputs.AZURE_TENANT_ID }}
                  subscription-id: ${{ steps.secrets.outputs.AZURE_SUBSCRIPTION_ID }}

            - name: Configure AWS CLI
              uses: ./.github/actions/aws-configure-cli
              with:
                  vault-addr: ${{ secrets.VAULT_ADDR }}
                  vault-role-id: ${{ secrets.VAULT_ROLE_ID }}
                  vault-secret-id: ${{ secrets.VAULT_SECRET_ID }}
                  aws-profile: ${{ env.AWS_PROFILE }}
                  aws-region: ${{ env.AWS_REGION }}

            - name: Set current Camunda version
              if: env.CLEANUP_CLUSTERS == 'true'
              id: camunda-version
              run: |
                  set -euo pipefail
                  CAMUNDA_VERSION=$(cat .camunda-version)
                  echo "CAMUNDA_VERSION=$CAMUNDA_VERSION" | tee -a "$GITHUB_OUTPUT"

            - name: Export S3_BACKEND_BUCKET based on matrix
              if: env.CLEANUP_CLUSTERS == 'true'
              id: s3_prefix
              run: |
                  set -euo pipefail
                  echo "S3_BACKEND_BUCKET_PREFIX=azure/kubernetes/aks-single-region/" | tee -a "$GITHUB_OUTPUT"

            - name: Delete on-demand AKS Cluster
              uses: ./.github/actions/azure-kubernetes-aks-single-region-cleanup
              if: always() && env.CLEANUP_CLUSTERS == 'true'
              timeout-minutes: 125
              with:
                  tf-bucket: ${{ env.S3_BACKEND_BUCKET }}
                  tf-bucket-region: ${{ env.S3_BUCKET_REGION }}
                  max-age-hours-cluster: 0
                  target: ${{ env.CLUSTER_NAME }}
                  tf-bucket-key-prefix: ${{ steps.s3_prefix.outputs.S3_BACKEND_BUCKET_PREFIX }}${{ steps.camunda-version.outputs.CAMUNDA_VERSION }}/
