---
name: Deploy AWS ROSA HCP Single Region Cluster

description: |
    This GitHub Action automates the deployment of the aws/openshift/rosa-hcp-single-region reference architecture cluster using Terraform.
    This action will also install oc, awscli, rosa cli.
    The kube context will be set on the created cluster.

inputs:
    rh-token:
        description: Red Hat Hybrid Cloud Console Token
        required: true
    cluster-name:
        description: Name of the ROSA cluster to deploy
        required: true
    admin-password:
        description: Admin password for the ROSA cluster
        required: true
    admin-username:
        description: Admin username for the ROSA cluster
        default: kube-admin
        required: true
    aws-region:
        description: AWS region where the ROSA cluster will be deployed
        required: true
    availability-zones:
        description: Comma separated list of availability zones (letters only, e.g., a,b,c)
        required: true
        default: a,b,c
    rosa-cli-version:
        description: Version of the ROSA CLI to use
        required: true
        default: latest
    openshift-version:
        description: Version of the OpenShift to install
        required: true
        # renovate: datasource=custom.rosa-camunda depName=red-hat-openshift versioning=semver
        default: 4.18.5
    replicas:
        description: Number of replicas for the ROSA cluster (empty will fallback on default value of the module)
        default: ''
    private-vpc:
        description: The VPC within which the cluster resides will only have private subnets, meaning that it cannot be accessed at all from the public
            Internet (empty will fallback on default value of the module)
        default: ''
    s3-backend-bucket:
        description: Name of the S3 bucket to store Terraform state
        required: true
    s3-bucket-region:
        description: Region of the bucket containing the resources states, if not set, will fallback on aws-region
    s3-bucket-key-prefix:
        description: Key prefix of the bucket containing the resources states. It must contain a / at the end e.g 'my-prefix/'.
        default: ''
    tf-modules-revision:
        description: Git revision of the tf modules to use
        default: main
        required: true
    tf-modules-path:
        description: Path where the tf rosa modules will be cloned
        default: ./.action-tf-modules/aws-openshift-rosa-hcp-single-region-create/
        required: true
    login:
        description: Authenticate the current kube context on the created cluster
        default: 'true'
        required: true
    cleanup-tf-modules-path:
        description: Whether to clean up the tf modules path
        default: 'false'
    tags:
        description: Tags to apply to the cluster and related resources, in JSON format
        default: '{}'

    vpn-enabled:
        description: Enable VPN setup (recommended when private_vpc is true), this will also configure the current runner to use it
        default: 'false'
    vpn-s3-bucket-name:
        description: The name of the S3 bucket to store VPN certificates and config (required if vpn-enabled is true)
        default: ''
    vpn-s3-bucket-key:
        description: The key (path) in the S3 bucket where store VPN certificates and config (default will use module value)
        default: ''
    vpn-s3-bucket-region:
        description: The AWS region for the S3 bucket (required if vpn-enabled is true)
        default: ''

outputs:
    openshift-server-api:
        description: The server API URL of the deployed ROSA cluster
        value: ${{ steps.cluster_info.outputs.cluster_api }}

    openshift-cluster-id:
        description: The ID of the deployed ROSA cluster
        value: ${{ steps.apply.outputs.cluster_id }}

    terraform-state-url:
        description: URL of the Terraform state file in the S3 bucket
        value: ${{ steps.set-terraform-variables.outputs.terraform-state-url }}

    vpn-client-configs-s3-urls:
        description: Map of S3 URLs for client configs
        value: ${{ steps.apply.outputs.vpn_client_configs_s3_urls || '' }}
    vpn-client-config-file:
        description: Config file used by the VPN
        value: ${{ steps.vpn.outputs.vpn_client_config_file || '' }}
    vpn-endpoint:
        description: Endpoint of the VPN to access the created cluster
        value: ${{ steps.apply.outputs.vpn_endpoint || '' }}


runs:
    using: composite
    steps:
        - name: Validate inputs
          shell: bash
          run: |
              if [[ "${{ inputs.vpn-enabled }}" == "true" ]]; then
                if [[ -z "${{ inputs.vpn-s3-bucket-name }}" || -z "${{ inputs.vpn-s3-bucket-region }}" ]]; then
                  echo "❌ ERROR: vpn-s3-bucket-name and vpn-s3-bucket-region are required when vpn-enabled is true"
                  exit 1
                fi
              fi

        - name: Checkout Repository
          uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
          with:
              repository: camunda/camunda-deployment-references
              ref: ${{ inputs.tf-modules-revision }}
              path: ${{ inputs.tf-modules-path }}
              fetch-depth: 0

        - name: Install asdf tools with cache for the project
          uses: camunda/infraex-common-config/./.github/actions/asdf-install-tooling@791f01a12a6b0c44f16a1dce9c9791de34ec4767 # 1.3.8

        # TODO: when available on asdf, migrate this to it
        - name: Install ROSA CLI
          shell: bash
          working-directory: /tmp
          run: |
              set -euo pipefail

              curl -LO "https://mirror.openshift.com/pub/openshift-v4/clients/rosa/${{ inputs.rosa-cli-version }}/rosa-linux.tar.gz"
              tar -xvf rosa-linux.tar.gz
              sudo mv rosa /usr/local/bin/rosa
              chmod +x /usr/local/bin/rosa
              rm -f rosa-linux.tar.gz
              rosa version

        - name: Install CLI tools from OpenShift Mirror
          uses: redhat-actions/openshift-tools-installer@144527c7d98999f2652264c048c7a9bd103f8a82 # v1
          with:
              oc: ${{ inputs.openshift-version }}

        - name: Set AWS Region and associated AZs
          id: construct_azs
          shell: bash
          run: |
              set -euo pipefail

              echo "AWS_REGION=${{ inputs.aws-region }}" | tee -a "$GITHUB_ENV"

              IFS=',' read -r -a az_letters <<< "${{ inputs.availability-zones }}"
              FULL_AZS=()
              for az in "${az_letters[@]}"; do
                FULL_AZS+=("\"${AWS_REGION}${az}\"")  # Add double quotes around each AZ
              done

              # Join the AZs with commas
              FULL_AZS_STRING=$(IFS=,; echo "${FULL_AZS[*]}")
              echo "FULL_AZS=$FULL_AZS_STRING" | tee -a "$GITHUB_ENV"


        - name: Login to Red Hat Hybrid Cloud Console
          shell: bash
          run: |
              set -euo pipefail

              rosa login --token="${{ inputs.rh-token }}"
              rosa whoami

        - name: Verify and enable HCP ROSA on AWS Marketplace
          shell: bash
          run: |
              set -euo pipefail

              rosa verify quota
              rosa verify permissions
              rosa create account-roles --mode auto

        - name: Set Terraform variables
          id: set-terraform-variables
          shell: bash
          run: |
              set -euo pipefail

              export TFSTATE_BUCKET="${{ inputs.s3-backend-bucket }}"
              export TFSTATE_KEY="${{ inputs.s3-bucket-key-prefix }}tfstate-${{ inputs.cluster-name }}/${{ inputs.cluster-name }}.tfstate"

              if [ -z "${{ inputs.s3-bucket-region }}" ]; then
                export TFSTATE_REGION="$AWS_REGION"
              else
                export TFSTATE_REGION="${{ inputs.s3-bucket-region }}"
              fi

              echo "TFSTATE_BUCKET=${TFSTATE_BUCKET}" >> "$GITHUB_OUTPUT"
              echo "TFSTATE_REGION=${TFSTATE_REGION}" >> "$GITHUB_OUTPUT"
              echo "TFSTATE_KEY=${TFSTATE_KEY}" >> "$GITHUB_OUTPUT"

              terraform_state_url="s3://${TFSTATE_BUCKET}/${TFSTATE_KEY}"
              echo "terraform-state-url=${terraform_state_url}" >> "$GITHUB_OUTPUT"

        - name: Check if S3 bucket exists
          id: create-s3-bucket
          shell: bash
          run: |
              set -euo pipefail

              if aws s3api head-bucket --bucket ${{ inputs.s3-backend-bucket }} --region ${{ steps.set-terraform-variables.outputs.TFSTATE_REGION }} 2>/dev/null; then
                echo "Bucket already exists"
              else
                echo "Bucket does not exist, creating..."
                aws s3api create-bucket --bucket ${{ inputs.s3-backend-bucket }} \
                    --region ${{ steps.set-terraform-variables.outputs.TFSTATE_REGION }} \
                    --create-bucket-configuration LocationConstraint=${{ steps.set-terraform-variables.outputs.TFSTATE_REGION }}
              fi

              aws s3api put-public-access-block --bucket ${{ inputs.s3-backend-bucket }} \
                --region ${{ steps.set-terraform-variables.outputs.TFSTATE_REGION }} \
                --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"

        - name: Terraform Init
          id: init
          working-directory: ${{ inputs.tf-modules-path }}/aws/openshift/rosa-hcp-single-region/
          env:
              RHCS_TOKEN: ${{ inputs.rh-token }}
          shell: bash
          run: |
              set -euo pipefail

              terraform version

              terraform init \
                -backend-config="bucket=${{ steps.set-terraform-variables.outputs.TFSTATE_BUCKET }}" \
                -backend-config="key=${{ steps.set-terraform-variables.outputs.TFSTATE_KEY }}" \
                -backend-config="region=${{ steps.set-terraform-variables.outputs.TFSTATE_REGION }}"

              terraform validate -no-color

        - name: Terraform Plan
          id: plan
          working-directory: ${{ inputs.tf-modules-path }}/aws/openshift/rosa-hcp-single-region/
          env:
              RHCS_TOKEN: ${{ inputs.rh-token }}
          shell: bash
          run: |
              set -euo pipefail

              echo "Adapting the files with input values"
              pwd
              ls

              extra_vars=""

              # protect sensitive values
              echo "::add-mask::${{ inputs.admin-password }}"

              # We use sed instead of -var because the module presented to the user
              # uses locals for simplicity. Locals cannot be overwritten with the CLI.
              sed -i -e 's/\(rosa_cluster_name\s*=\s*"\)[^"]*\("\)/\1${{ inputs.cluster-name }}\2/' \
                     -e 's/\(rosa_admin_password\s*=\s*"\)[^"]*\("\)/\1${{ inputs.admin-password }}\2/' \
                     -e 's/\(rosa_admin_username\s*=\s*"\)[^"]*\("\)/\1${{ inputs.admin-username }}\2/' \
                     -e 's/\(openshift_version\s*=\s*"\)[^"]*\("\)/\1${{ inputs.openshift-version }}\2/' \
                     -e "s/\(rosa_cluster_zones\s*=\s*\)[^]]*\]/\1[$FULL_AZS]/" \
                     cluster.tf

              if [ -n "${{ inputs.replicas }}" ]; then
                sed -i -e 's/\(replicas\s*=\s*\)[0-9]\+/\1${{ inputs.replicas }}/' cluster.tf
              else
                echo "No replicas value provided, skipping replica modification."
              fi

              if [ -n "${{ inputs.private-vpc }}" ]; then
                sed -i -E "s/(^\s*rosa_private_cluster\s*=\s*)[a-z]+/\1${{ inputs.private-vpc }}/" cluster.tf
              else
                echo "No private-vpc value provided, skipping private modification."
              fi

              if [[ "${{ inputs.vpn-enabled }}" == "true" ]]; then
                sed -i -E \
                  -e "s/(^\s*s3_bucket_name\s*=\s*\")[^\"]*(\")/\1${{ inputs.vpn-s3-bucket-name }}\2/" \
                  vpn.tf

                if [ -n "${{ inputs.vpn-s3-bucket-key }}" ]; then
                  bucket_key="${{ inputs.vpn-s3-bucket-key }}"
                  sed -i -E "s|(^\s*s3_ca_directory\s*=\s*).*|\1\"${bucket_key}\"|" vpn.tf
                fi

                extra_vars="-var=s3_bucket_region=${{ inputs.vpn-s3-bucket-region }}"

                echo "Displaying templated vpn.tf file:"
                cat vpn.tf
              else
                echo "VPN disabled, skipping S3 bucket modifications"
              fi


              echo "Displaying templated cluster.tf file:"
              cat cluster.tf

              raw_tags='${{ inputs.tags }}'
              escaped_tags=$(echo "$raw_tags" | jq -r 'to_entries | map("\"" + .key + "\"=\"" + .value + "\"") | join(", ")' | sed 's/.*/{&}/')

              terraform plan -no-color -var="default_tags=$escaped_tags" $extra_vars -out rosa.plan

        - name: Terraform Apply
          id: apply
          working-directory: ${{ inputs.tf-modules-path }}/aws/openshift/rosa-hcp-single-region/
          env:
              RHCS_TOKEN: ${{ inputs.rh-token }}
          shell: bash
          run: |
              set -euo pipefail

              terraform apply -no-color rosa.plan

              export cluster_id="$(terraform output -raw cluster_id)"
              echo "cluster_id=$cluster_id" >> "$GITHUB_OUTPUT"

              # Extract VPN outputs only if VPN is enabled and module applied
              if [[ "${{ inputs.vpn-enabled }}" == "true" ]]; then
                echo "vpn_client_configs_s3_urls=$(terraform output -json vpn_client_configs_s3_urls)" | tee -a "$GITHUB_OUTPUT"
                echo "vpn_endpoint=$(terraform output -json vpn_endpoint)" | tee -a "$GITHUB_OUTPUT"
              fi

# TODO: handle s3 bucket cleanup when in the desctruction

        - name: Retrieve cluster information
          id: cluster_info
          shell: bash
          run: |
              set -euo pipefail

              rosa describe cluster --output=json -c "${{ steps.apply.outputs.cluster_id }}"
              export cluster_api=$(rosa describe cluster --output=json -c "${{ steps.apply.outputs.cluster_id }}" | jq -r '.api.url')
              echo "cluster_api=$cluster_api"
              echo "cluster_api=$cluster_api" >> "$GITHUB_OUTPUT"

        - name: Configure VPN Client if enabled
          id: vpn
          shell: bash
          if: inputs.vpn-enabled == 'true'
          run: |
              sudo apt update
              sudo apt install -y openvpn openvpn-systemd-resolved

              client_name="my-client" # the client name is hard-coded in the reference
              config_url=$(echo '${{ steps.apply.outputs.vpn_client_configs_s3_urls }}' | jq -r --arg client "$client_name" '.[$client]')

              # Create a secure directory inside the GitHub workspace
              mkdir -p "$GITHUB_WORKSPACE/.vpn"
              vpn_client_config_file="$GITHUB_WORKSPACE/.vpn/client.ovpn"
              echo "vpn_client_config_file=$vpn_client_config_file" | tee -a "$GITHUB_OUTPUT"
              AWS_REGION=${{ inputs.vpn-s3-bucket-region }} aws s3 cp "$config_url"

        - name: Connect to VPN
          if: inputs.vpn-enabled == 'true'
          uses: kota65535/github-openvpn-connect-action@cd2ed8a90cc7b060dc4e001143e811b5f7ea0af5 # v3.1.0
          with:
              config_file: ${{ steps.apply.vpn.vpn_client_config_file }}
              echo_config: 'false'

        - name: Login and generate kubeconfig
          # we need to retry due as the cluster has just been created and the OIDC provider may not be available yet
          uses: nick-fields/retry@ce71cc2ab81d554ebbe88c79ab5975992d79ba08 # v3
          id: kube_config
          if: inputs.login == 'true'
          with:
              timeout_minutes: 10
              max_attempts: 40
              shell: bash
              retry_wait_seconds: 15
              command: |
                  : # see https://github.com/nick-fields/retry/issues/133
                  set -o errexit
                  set -o pipefail

                  # protect sensitive values
                  echo "::add-mask::${{ inputs.admin-password }}"

                  # Check if the user is already a cluster-admin
                  if ! rosa list users --cluster="${{ inputs.cluster-name }}" | grep -q "${{ inputs.admin-username }}"; then
                    rosa grant user cluster-admin --cluster="${{ inputs.cluster-name }}" --user="${{ inputs.admin-username }}"
                  else
                    echo "✅ User '${{ inputs.admin-username }}' is already a cluster-admin on '${{ inputs.cluster-name }}'."
                  fi

                  oc login --username "${{ inputs.admin-username }}" --password "${{ inputs.admin-password }}" "${{ steps.cluster_info.outputs.cluster_api }}"
                  oc whoami

                  kubectl config rename-context $(oc config current-context) "${{ inputs.cluster-name }}"
                  kubectl config use "${{ inputs.cluster-name }}"

        - name: Clean up cloned modules
          if: always() && inputs.cleanup-tf-modules-path == 'true'
          shell: bash
          run: |
              set -euo pipefail

              rm -rf "${{ inputs.tf-modules-path }}"
